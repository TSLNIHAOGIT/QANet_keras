{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! -*- coding: utf-8 -*-\n",
    "# from keras.engine.topology import Layer\n",
    "from tensorflow.keras.layers import Layer \n",
    "import tensorflow as tf\n",
    "\n",
    "class BatchSlice(Layer):\n",
    "    def __init__(self, dim=2, **kwargs):\n",
    "        self.dim = dim\n",
    "        super(BatchSlice, self).__init__(**kwargs)\n",
    "    '''\n",
    "    build(input_shape): 这是你定义权重的地方。这个方法必须设 self.built = True，可以通过调用 super([Layer], self).build() 完成\n",
    "    '''\n",
    "    def build(self, input_shape):\n",
    "        super(BatchSlice, self).build(input_shape)\n",
    "    '''\n",
    "    call(x): 这里是编写层的功能逻辑的地方。你只需要关注传入 call 的第一个参数：输入张量，除非你希望你的层支持masking。\n",
    "    '''\n",
    "    def call(self, x, mask=None):\n",
    "        x, length = x # [bs, len, dim]\n",
    "        length = tf.cast(tf.reduce_max(length), tf.int32)\n",
    "        st = [0] * self.dim\n",
    "        ed = [-1] * self.dim\n",
    "        ed[1] = length\n",
    "        x = tf.slice(x, st, ed)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "from tensorflow.keras.regularizers import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "class context2query_attention(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, c_maxlen, q_maxlen, dropout, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.c_maxlen = c_maxlen\n",
    "        self.q_maxlen = q_maxlen\n",
    "        self.dropout = dropout\n",
    "        super(context2query_attention, self).__init__(**kwargs)\n",
    "\n",
    "    '''\n",
    "    build(input_shape): 这是你定义权重的地方。这个方法必须设 self.built = True，可以通过调用 super([Layer], self).build() 完成\n",
    "    '''\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape: [(None, ?, 128), (None, ?, 128)]\n",
    "        init = VarianceScaling(scale=1.0, mode='fan_avg', distribution='uniform')\n",
    "        self.W0 = self.add_weight(name='W0',\n",
    "                                  shape=(input_shape[0][-1], 1),\n",
    "                                  initializer=init,\n",
    "                                  regularizer=l2(3e-7),\n",
    "                                  trainable=True)\n",
    "        self.W1 = self.add_weight(name='W1',\n",
    "                                  shape=(input_shape[1][-1], 1),\n",
    "                                  initializer=init,\n",
    "                                  regularizer=l2(3e-7),\n",
    "                                  trainable=True)\n",
    "        self.W2 = self.add_weight(name='W2',\n",
    "                                  shape=(1, 1, input_shape[0][-1]),\n",
    "                                  initializer=init,\n",
    "                                  regularizer=l2(3e-7),\n",
    "                                  trainable=True)\n",
    "        self.bias = self.add_weight(name='linear_bias',\n",
    "                                    shape=([1]),\n",
    "                                    initializer='zero',\n",
    "                                    regularizer=l2(3e-7),\n",
    "                                    trainable=True)\n",
    "        super(context2query_attention, self).build(input_shape)\n",
    "\n",
    "    def mask_logits(self, inputs, mask, mask_value=-1e30):\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        return inputs + mask_value * (1 - mask)\n",
    "    \n",
    "    '''\n",
    "    call(x): 这里是编写层的功能逻辑的地方。你只需要关注传入 call 的第一个参数：输入张量，除非你希望你的层支持masking。\n",
    "    '''\n",
    "\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        x_cont, x_ques, c_mask, q_mask = x\n",
    "        # get similarity matrix S\n",
    "        subres0 = K.tile(K.dot(x_cont, self.W0), [1, 1, self.q_maxlen])\n",
    "        subres1 = K.tile(K.permute_dimensions(K.dot(x_ques, self.W1), pattern=(0, 2, 1)), [1, self.c_maxlen, 1])\n",
    "        subres2 = K.batch_dot(x_cont * self.W2, K.permute_dimensions(x_ques, pattern=(0, 2, 1)))\n",
    "        S = subres0 + subres1 + subres2\n",
    "        S += self.bias\n",
    "        q_mask = tf.expand_dims(q_mask, 1)\n",
    "        S_ = tf.nn.softmax(self.mask_logits(S, q_mask))\n",
    "        c_mask = tf.expand_dims(c_mask, 2)\n",
    "        S_T = K.permute_dimensions(tf.nn.softmax(self.mask_logits(S, c_mask), axis=1), (0, 2, 1))\n",
    "        c2q = tf.matmul(S_, x_ques)\n",
    "        q2c = tf.matmul(tf.matmul(S_, S_T), x_cont)\n",
    "        result = K.concatenate([x_cont, c2q, x_cont * c2q, x_cont * q2c], axis=-1)\n",
    "        #result Tensor(\"context2query_attention/concat_2:0\", shape=(None, None, 512), dtype=float32)\n",
    "        print('result',result)\n",
    "        return result\n",
    "\n",
    "    '''\n",
    "    compute_output_shape(input_shape): 如果你的层更改了输入张量的形状，你应该在这里定义形状变化的逻辑，这让Keras能够自动推断各层的形状\n",
    "    '''\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "from tensorflow.keras.regularizers import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "class DepthwiseConv1D(Layer):\n",
    "\n",
    "    def __init__(self, kernel_size, filter, **kwargs):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.filter = filter\n",
    "        super(DepthwiseConv1D, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        init_relu = VarianceScaling(scale=2.0, mode='fan_in', distribution='normal')\n",
    "        self.depthwise_w = self.add_weight(\"depthwise_filter\",\n",
    "                                           shape=(self.kernel_size, 1, input_shape[-1], 1),\n",
    "                                           initializer=init_relu,\n",
    "                                           regularizer=l2(3e-7),\n",
    "                                           trainable=True)\n",
    "        self.pointwise_w = self.add_weight(\"pointwise_filter\",\n",
    "                                           (1, 1, input_shape[-1], self.filter),\n",
    "                                           initializer=init_relu,\n",
    "                                           regularizer=l2(3e-7),\n",
    "                                           trainable=True)\n",
    "        self.bias = self.add_weight(\"bias\",\n",
    "                                    input_shape[-1],\n",
    "                                    regularizer=l2(3e-7),\n",
    "                                    initializer=tf.zeros_initializer())\n",
    "        super(DepthwiseConv1D, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        x = K.expand_dims(x, axis=2)\n",
    "        x = tf.nn.separable_conv2d(x,\n",
    "                                   self.depthwise_w,\n",
    "                                   self.pointwise_w,\n",
    "                                   strides=(1, 1, 1, 1),\n",
    "                                   padding=\"SAME\")\n",
    "        x += self.bias\n",
    "        x = K.relu(x)\n",
    "        outputs = K.squeeze(x, axis=2)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.training import slot_creator\n",
    "import tensorflow.keras.backend as K\n",
    "# from tensorflow.keras.backend import moving_averages\n",
    "from tensorflow.keras.backend import moving_average_update\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class ExponentialMovingAverage():\n",
    "    def __init__(self, model, decay, weights_list=None, temp_model='temp_model.h5',\n",
    "                 name='ExponentialMovingAverage', type='cpu'):\n",
    "        # EMA for keras, the example can be seen in https://github.com/ewrfcas/QANet_keras/blob/master/train_QANet.py\n",
    "        # init before training, but after the model init.\n",
    "        self.model = model\n",
    "        self.scope_name = name\n",
    "        self.temp_model = temp_model\n",
    "        self.type = type\n",
    "        self.decay = decay\n",
    "        self._averages = {}\n",
    "\n",
    "        if weights_list is None:\n",
    "            weights_list = self.model.trainable_weights\n",
    "\n",
    "        if self.type == 'gpu':\n",
    "            self.sess = K.get_session()\n",
    "            for weight in weights_list:\n",
    "                if weight.dtype.base_dtype not in [tf.float16, tf.float32,\n",
    "                                                   tf.float64]:\n",
    "                    raise TypeError(\"The variables must be half, float, or double: %s\" %\n",
    "                                    weight.name)\n",
    "                if weight in self._averages:\n",
    "                    raise ValueError(\"Moving average already computed for: %s\" % weight.name)\n",
    "\n",
    "                # For variables: to lower communication bandwidth across devices we keep\n",
    "                # the moving averages on the same device as the variables. For other\n",
    "                # tensors, we rely on the existing device allocation mechanism.\n",
    "                with ops.init_scope():\n",
    "                    if isinstance(weight, tf.Variable):\n",
    "                        avg = slot_creator.create_slot(weight,\n",
    "                                                       weight.initialized_value(),\n",
    "                                                       self.scope_name,\n",
    "                                                       colocate_with_primary=True)\n",
    "                        # NOTE(mrry): We only add `tf.Variable` objects to the\n",
    "                        # `MOVING_AVERAGE_VARIABLES` collection.\n",
    "                        ops.add_to_collection(ops.GraphKeys.MOVING_AVERAGE_VARIABLES, weight)\n",
    "                    else:\n",
    "                        avg = slot_creator.create_zeros_slot(weight,\n",
    "                                                             self.scope_name,\n",
    "                                                             colocate_with_primary=(weight.op.type in [\"Variable\",\n",
    "                                                                                                       \"VariableV2\",\n",
    "                                                                                                       \"VarHandleOp\"]))\n",
    "                self._averages[weight] = avg\n",
    "\n",
    "            with tf.name_scope(self.scope_name):\n",
    "                decay = ops.convert_to_tensor(decay, name=\"decay\")\n",
    "                self.updates = []\n",
    "                for var in weights_list:\n",
    "                    self.updates.append(\n",
    "                        moving_averages.assign_moving_average(self._averages[var], var, decay, zero_debias=False))\n",
    "\n",
    "                self.assigns = []\n",
    "                for weight in weights_list:\n",
    "                    self.assigns.append(tf.assign(weight, self._averages[weight]))\n",
    "\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        elif self.type == 'cpu':\n",
    "            print('CPU EMA getting weights...')\n",
    "            for weight in tqdm(weights_list):\n",
    "                self._averages[weight.name] = K.get_value(weight)\n",
    "\n",
    "    def average_update(self):\n",
    "        # run in the end of each batch\n",
    "        if self.type == 'gpu':\n",
    "            self.sess.run(self.updates)\n",
    "        elif self.type == 'cpu':\n",
    "            for weight in self.model.trainable_weights:\n",
    "                old_val = self._averages[weight.name]\n",
    "                self._averages[weight.name] = self.decay * old_val + (1.0 - self.decay) * K.get_value(weight)\n",
    "\n",
    "    def assign_shadow_weights(self, backup=True):\n",
    "        # run while you need to assign shadow weights (at end of each epoch or the total training)\n",
    "        if backup:\n",
    "            self.model.save_weights(self.temp_model)\n",
    "\n",
    "        if self.type == 'gpu':\n",
    "            self.sess.run(self.assigns)\n",
    "        elif self.type == 'cpu':\n",
    "            print('CPU EMA assigning weights...')\n",
    "            for weight in tqdm(self.model.trainable_weights):\n",
    "                K.set_value(weight, self._averages[weight.name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! -*- coding: utf-8 -*-\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "\n",
    "def shape_list(x):\n",
    "  \"\"\"Return list of dims, statically where possible.\"\"\"\n",
    "  x = tf.convert_to_tensor(x)\n",
    "\n",
    "  # If unknown rank, return dynamic shape\n",
    "  if x.get_shape().dims is None:\n",
    "    return tf.shape(x)\n",
    "\n",
    "  static = x.get_shape().as_list()\n",
    "  shape = tf.shape(x)\n",
    "\n",
    "  ret = []\n",
    "  for i in range(len(static)):\n",
    "    dim = static[i]\n",
    "    if dim is None:\n",
    "      dim = shape[i]\n",
    "    ret.append(dim)\n",
    "  return ret\n",
    "\n",
    "class LabelPadding(Layer):\n",
    "    def __init__(self, max_len, **kwargs):\n",
    "        self.max_len = max_len\n",
    "        super(LabelPadding, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(LabelPadding, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None, training=None):\n",
    "        tensor_shape = shape_list(x) # [bs, len]\n",
    "        zero_paddings = tf.zeros((tensor_shape[0], self.max_len - tensor_shape[1]))\n",
    "        x = tf.concat([x, zero_paddings], axis=-1)\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! -*- coding: utf-8 -*-\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class LayerDropout(Layer):\n",
    "    def __init__(self, dropout = 0.1, **kwargs):\n",
    "        self.dropout = dropout\n",
    "        super(LayerDropout, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(LayerDropout, self).build(input_shape)\n",
    "    #0.2 should be self.dropout，但这里总是出错，就先改成0.2\n",
    "    def call(self, x, mask=None, training=None):\n",
    "        x, residual = x\n",
    "        pred = tf.random.uniform([]) < self.dropout\n",
    "        #print('self.dropout',self.dropout)\n",
    "        x_train = tf.cond(pred, lambda: residual, lambda: tf.nn.dropout(x, 1.0 -0.2 ) + residual)\n",
    "        x_test = x + residual\n",
    "        return K.in_train_phase(x_train, x_test, training=training)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! -*- coding: utf-8 -*-\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.scale = self.add_weight(name='layer_norm_scale',\n",
    "                                    shape=(input_shape[-1]),\n",
    "                                    initializer=tf.ones_initializer(),\n",
    "                                    trainable=True)\n",
    "        self.bias = self.add_weight(name='layer_norm_bias',\n",
    "                                    shape=(input_shape[-1]),\n",
    "                                    initializer=tf.zeros_initializer(),\n",
    "                                    trainable=True)\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None, training=None):\n",
    "        mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "        variance = tf.reduce_mean(tf.square(x - mean), axis=-1, keepdims=True)\n",
    "        norm_x = (x - mean) * tf.math.rsqrt(variance + K.epsilon())\n",
    "        return norm_x * self.scale + self.bias\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! -*- coding: utf-8 -*-\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, units, num_heads, dropout=0.1, bias=True, **kwargs):\n",
    "        self.units = units\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(name='bias',\n",
    "                                     shape=([1]),\n",
    "                                     initializer='zero')\n",
    "        super(MultiHeadAttention, self).build(input_shape)\n",
    "\n",
    "    def split_last_dimension(self, x, n):\n",
    "        old_shape = x.get_shape().dims\n",
    "        last = old_shape[-1]\n",
    "        new_shape = old_shape[:-1] + [n] + [last // n if last else None]\n",
    "        ret = tf.reshape(x, tf.concat([tf.shape(x)[:-1], [n, -1]], 0))\n",
    "        ret.set_shape(new_shape)\n",
    "        return tf.transpose(ret, [0, 2, 1, 3])\n",
    "\n",
    "    def mask_logits(self, inputs, mask, mask_value=-1e30):\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        return inputs + mask_value * (1 - mask)\n",
    "\n",
    "    def dot_product_attention(self, x, mask=None, dropout=0.1, training=None):\n",
    "        q, k, v = x\n",
    "        logits = tf.matmul(q, k, transpose_b=True)  # [bs, 8, len, len]\n",
    "        if self.bias:\n",
    "            logits += self.b\n",
    "        if mask is not None:  # [bs, len]\n",
    "            mask = tf.expand_dims(mask, axis=1)\n",
    "            mask = tf.expand_dims(mask, axis=1)  # [bs,1,1,len]\n",
    "            logits = self.mask_logits(logits, mask)\n",
    "        weights = tf.nn.softmax(logits, name=\"attention_weights\")\n",
    "        weights = K.in_train_phase(K.dropout(weights, dropout), weights, training=training)\n",
    "        x = tf.matmul(weights, v)\n",
    "        return x\n",
    "\n",
    "    def combine_last_two_dimensions(self, x):\n",
    "        old_shape = x.get_shape().dims\n",
    "        a, b = old_shape[-2:]\n",
    "        new_shape = old_shape[:-2] + [a * b if a and b else None]\n",
    "        ret = tf.reshape(x, tf.concat([tf.shape(x)[:-2], [-1]], 0))\n",
    "        ret.set_shape(new_shape)\n",
    "        return ret\n",
    "\n",
    "    def call(self, x, mask=None, training=None):\n",
    "        memory, query, seq_mask = x\n",
    "        Q = self.split_last_dimension(query, self.num_heads)\n",
    "        memory = tf.split(memory, 2, axis=2)\n",
    "        K = self.split_last_dimension(memory[0], self.num_heads)\n",
    "        V = self.split_last_dimension(memory[1], self.num_heads)\n",
    "\n",
    "        key_depth_per_head = self.units // self.num_heads\n",
    "        Q *= (key_depth_per_head ** -0.5)\n",
    "        x = self.dot_product_attention([Q, K, V], seq_mask, dropout=self.dropout, training=training)\n",
    "        x = self.combine_last_two_dimensions(tf.transpose(x, [0, 2, 1, 3]))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.MultiHeadAttention"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "\n",
    "class Position_Embedding(Layer):\n",
    "    def __init__(self, min_timescale=1.0, max_timescale=1.0e4, **kwargs):\n",
    "        self.min_timescale = min_timescale\n",
    "        self.max_timescale = max_timescale\n",
    "        super(Position_Embedding, self).__init__(**kwargs)\n",
    "\n",
    "    def get_timing_signal_1d(self, length, channels):\n",
    "        position=tf.cast(tf.range(length),dtype=tf.float32)\n",
    "        num_timescales = channels // 2\n",
    "        log_timescale_increment = (math.log(float(self.max_timescale) / float(self.min_timescale)) / (tf.cast(num_timescales,dtype=tf.float32) - 1))\n",
    "        inv_timescales = self.min_timescale * tf.exp(tf.cast(tf.range(num_timescales),dtype=tf.float32) * -log_timescale_increment)\n",
    "        scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0)\n",
    "        signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "        signal = tf.pad(signal, [[0, 0], [0, tf.math.mod(channels, 2)]])\n",
    "        signal = tf.reshape(signal, [1, length, channels])\n",
    "        return signal\n",
    "\n",
    "    def add_timing_signal_1d(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        channels = tf.shape(x)[2]\n",
    "        signal = self.get_timing_signal_1d(length, channels)\n",
    "        return x + signal\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return self.add_timing_signal_1d(x)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Position_Embedding"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PositionEmbedding=Position_Embedding\n",
    "PositionEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! -*- coding: utf-8 -*-\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.regularizers import *\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class QAoutputBlock(Layer):\n",
    "    def __init__(self, ans_limit=30, **kwargs):\n",
    "        self.ans_limit = ans_limit\n",
    "        super(QAoutputBlock, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(QAoutputBlock, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        x1 ,x2 = x\n",
    "        outer = tf.matmul(tf.expand_dims(x1, axis=2), tf.expand_dims(x2, axis=1))\n",
    "        outer = tf.linalg.band_part(outer, 0, self.ans_limit)\n",
    "        output1 = tf.reshape(tf.cast(tf.argmax(tf.reduce_max(outer, axis=2), axis=1), tf.float32),(-1,1))\n",
    "        output2 = tf.reshape(tf.cast(tf.argmax(tf.reduce_max(outer, axis=1), axis=1), tf.float32),(-1,1))\n",
    "\n",
    "        return [output1, output2]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [(input_shape[0][0],1), (input_shape[0][0],1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.models import *\n",
    "# from layers.context2query_attention import context2query_attention\n",
    "\n",
    "# from layers.multihead_attention import Attention as MultiHeadAttention\n",
    "# from layers.position_embedding import Position_Embedding as PositionEmbedding\n",
    "# from layers.layer_norm import LayerNormalization\n",
    "# from layers.layer_dropout import LayerDropout\n",
    "# from layers.QAoutputBlock import QAoutputBlock\n",
    "# from layers.BatchSlice import BatchSlice\n",
    "# from layers.DepthwiseConv1D import DepthwiseConv1D\n",
    "# from layers.LabelPadding import LabelPadding\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "regularizer = l2(3e-7)\n",
    "init = VarianceScaling(scale=1.0, mode='fan_avg', distribution='uniform')\n",
    "init_relu = VarianceScaling(scale=2.0, mode='fan_in', distribution='normal')\n",
    "\n",
    "\n",
    "def mask_logits(inputs, mask, mask_value=-1e30):\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    return inputs + mask_value * (1 - mask)\n",
    "\n",
    "\n",
    "def highway(highway_layers, x, num_layers=2, dropout=0.1):\n",
    "    # reduce dim\n",
    "    x = highway_layers[0](x)\n",
    "    for i in range(num_layers):\n",
    "        T = highway_layers[i * 2 + 1](x)\n",
    "        H = highway_layers[i * 2 + 2](x)\n",
    "        H = Dropout(dropout)(H)\n",
    "        x = Lambda(lambda v: v[0] * v[1] + v[2] * (1 - v[1]))([H, T, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(conv_layers, x, num_conv=4, dropout=0.1, l=1., L=1.):\n",
    "    for i in range(num_conv):\n",
    "        residual = x\n",
    "        x = LayerNormalization()(x)\n",
    "        if i % 2 == 0:\n",
    "            x = Dropout(dropout)(x)\n",
    "        x = conv_layers[i](x)\n",
    "        x = LayerDropout(dropout * (l / L))([x, residual])\n",
    "    return x\n",
    "\n",
    "\n",
    "def attention_block(attention_layer, x, seq_mask, dropout=0.1, l=1., L=1.):\n",
    "    residual = x\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x1 = attention_layer[0](x)\n",
    "    x2 = attention_layer[1](x)\n",
    "    x = attention_layer[2]([x1, x2, seq_mask])\n",
    "    x = LayerDropout(dropout * (l / L))([x, residual])\n",
    "    return x\n",
    "\n",
    "\n",
    "def feed_forward_block(FeedForward_layers, x, dropout=0.1, l=1., L=1.):\n",
    "    residual = x\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = FeedForward_layers[0](x)\n",
    "    x = FeedForward_layers[1](x)\n",
    "    x = LayerDropout(dropout * (l / L))([x, residual])\n",
    "    return x\n",
    "\n",
    "\n",
    "def QANet(config, word_mat=None, char_mat=None, cove_model=None):\n",
    "    # parameters\n",
    "    word_dim = config['word_dim']\n",
    "    char_dim = config['char_dim']\n",
    "    cont_limit = config['cont_limit']\n",
    "    char_limit = config['char_limit']\n",
    "    ans_limit = config['ans_limit']\n",
    "    filters = config['filters']\n",
    "    num_head = config['num_head']\n",
    "    dropout = config['dropout']\n",
    "\n",
    "    # Input Embedding Layer\n",
    "    #`Input()` is used to instantiate a Keras tensor.S\n",
    "    contw_input_ = Input((None,))\n",
    "    quesw_input_ = Input((None,))\n",
    "    contc_input_ = Input((None, char_limit))\n",
    "    quesc_input_ = Input((None, char_limit))\n",
    "\n",
    "    # get mask\n",
    "    c_mask = Lambda(lambda x: tf.cast(x, tf.bool))(contw_input_)  # [bs, c_len]\n",
    "    q_mask = Lambda(lambda x: tf.cast(x, tf.bool))(quesw_input_)\n",
    "    cont_len = Lambda(lambda x: tf.expand_dims(tf.reduce_sum(tf.cast(x, tf.int32), axis=1), axis=1))(c_mask)\n",
    "    ques_len = Lambda(lambda x: tf.expand_dims(tf.reduce_sum(tf.cast(x, tf.int32), axis=1), axis=1))(q_mask)\n",
    "\n",
    "    # slice\n",
    "    contw_input = BatchSlice(dim=2)([contw_input_, cont_len])\n",
    "    quesw_input = BatchSlice(dim=2)([quesw_input_, ques_len])\n",
    "    contc_input = BatchSlice(dim=3)([contc_input_, cont_len])\n",
    "    quesc_input = BatchSlice(dim=3)([quesc_input_, ques_len])\n",
    "    c_mask = BatchSlice(dim=2)([c_mask, cont_len])\n",
    "    q_mask = BatchSlice(dim=2)([q_mask, ques_len])\n",
    "    c_maxlen = tf.cast(tf.reduce_max(cont_len), tf.int32)\n",
    "    q_maxlen = tf.cast(tf.reduce_max(ques_len), tf.int32)\n",
    "\n",
    "    # embedding word\n",
    "    WordEmbedding = Embedding(word_mat.shape[0], word_dim, weights=[word_mat], trainable=False, name='word_embedding')\n",
    "    xw_cont = WordEmbedding(contw_input)\n",
    "    xw_ques = WordEmbedding(quesw_input)\n",
    "\n",
    "    # cove\n",
    "    if cove_model is not None:\n",
    "        x_cont_cove = cove_model(xw_cont)\n",
    "        x_ques_cove = cove_model(xw_ques)\n",
    "        xw_cont = Concatenate()([xw_cont, x_cont_cove])\n",
    "        xw_ques = Concatenate()([xw_ques, x_ques_cove])\n",
    "\n",
    "    # embedding char\n",
    "    CharEmbedding = Embedding(char_mat.shape[0], char_dim, weights=[char_mat], name='char_embedding')\n",
    "    xc_cont = CharEmbedding(contc_input)\n",
    "    xc_ques = CharEmbedding(quesc_input)\n",
    "    char_conv = Conv1D(filters, 5,\n",
    "                       activation='relu',\n",
    "                       kernel_initializer=init_relu,\n",
    "                       kernel_regularizer=regularizer,\n",
    "                       name='char_conv')\n",
    "    xc_cont = Lambda(lambda x: tf.reshape(x, (-1, char_limit, char_dim)))(xc_cont)\n",
    "    xc_ques = Lambda(lambda x: tf.reshape(x, (-1, char_limit, char_dim)))(xc_ques)\n",
    "    xc_cont = char_conv(xc_cont)\n",
    "    xc_ques = char_conv(xc_ques)\n",
    "    xc_cont = GlobalMaxPooling1D()(xc_cont)\n",
    "    xc_ques = GlobalMaxPooling1D()(xc_ques)\n",
    "    xc_cont = Lambda(lambda x: tf.reshape(x, (-1, c_maxlen, filters)))(xc_cont)\n",
    "    xc_ques = Lambda(lambda x: tf.reshape(x, (-1, q_maxlen, filters)))(xc_ques)\n",
    "\n",
    "    # highwayNet\n",
    "    x_cont = Concatenate()([xw_cont, xc_cont])\n",
    "    x_ques = Concatenate()([xw_ques, xc_ques])\n",
    "\n",
    "    # highway shared layers\n",
    "    highway_layers = [Conv1D(filters, 1,\n",
    "                             kernel_initializer=init,\n",
    "                             kernel_regularizer=regularizer,\n",
    "                             name='highway_input_projection')]\n",
    "    for i in range(2):\n",
    "        highway_layers.append(Conv1D(filters, 1,\n",
    "                                     kernel_initializer=init,\n",
    "                                     kernel_regularizer=regularizer,\n",
    "                                     activation='sigmoid',\n",
    "                                     name='highway' + str(i) + '_gate'))\n",
    "        highway_layers.append(Conv1D(filters, 1,\n",
    "                                     kernel_initializer=init,\n",
    "                                     kernel_regularizer=regularizer,\n",
    "                                     activation='linear',\n",
    "                                     name='highway' + str(i) + '_linear'))\n",
    "    x_cont = highway(highway_layers, x_cont, num_layers=2, dropout=dropout)\n",
    "    x_ques = highway(highway_layers, x_ques, num_layers=2, dropout=dropout)\n",
    "\n",
    "    # build shared layers\n",
    "    # shared convs\n",
    "    Encoder_DepthwiseConv1 = []\n",
    "    for i in range(4):\n",
    "        Encoder_DepthwiseConv1.append(DepthwiseConv1D(7, filters))\n",
    "\n",
    "    # shared attention\n",
    "    Encoder_SelfAttention1 = [Conv1D(2 * filters, 1,\n",
    "                                     kernel_initializer=init,\n",
    "                                     kernel_regularizer=regularizer),\n",
    "                              Conv1D(filters, 1,\n",
    "                                     kernel_initializer=init,\n",
    "                                     kernel_regularizer=regularizer),\n",
    "                              MultiHeadAttention(filters, num_head, dropout=dropout, bias=False)]\n",
    "    # shared feed-forward\n",
    "    Encoder_FeedForward1 = []\n",
    "    Encoder_FeedForward1.append(Conv1D(filters, 1,\n",
    "                                       kernel_initializer=init,\n",
    "                                       kernel_regularizer=regularizer,\n",
    "                                       activation='relu'))\n",
    "    Encoder_FeedForward1.append(Conv1D(filters, 1,\n",
    "                                       kernel_initializer=init,\n",
    "                                       kernel_regularizer=regularizer,\n",
    "                                       activation='linear'))\n",
    "\n",
    "    # Context Embedding Encoder Layer\n",
    "    x_cont = PositionEmbedding()(x_cont)\n",
    "    x_cont = conv_block(Encoder_DepthwiseConv1, x_cont, 4, dropout)\n",
    "    x_cont = attention_block(Encoder_SelfAttention1, x_cont, c_mask, dropout)\n",
    "    x_cont = feed_forward_block(Encoder_FeedForward1, x_cont, dropout)\n",
    "\n",
    "    # Question Embedding Encoder Layer\n",
    "    x_ques = PositionEmbedding()(x_ques)\n",
    "    x_ques = conv_block(Encoder_DepthwiseConv1, x_ques, 4, dropout)\n",
    "    x_ques = attention_block(Encoder_SelfAttention1, x_ques, q_mask, dropout)\n",
    "    x_ques = feed_forward_block(Encoder_FeedForward1, x_ques, dropout)\n",
    "    \n",
    "    print('x_cont={}\\n  x_ques={}\\n  c_mask={}\\n  q_mask={}\\n'.format(x_cont, x_ques, c_mask, q_mask))\n",
    "\n",
    "    # Context_to_Query_Attention_Layer\n",
    "    ##512, c_maxlen, q_maxlen, dropout初始化该层的类，输入为[x_cont, x_ques, c_mask, q_mask]\n",
    "    x = context2query_attention(512, c_maxlen, q_maxlen, dropout)([x_cont, x_ques, c_mask, q_mask])\n",
    "    \n",
    "    print('Context_to_Query_Attention_Layer x',x)\n",
    "    x = Conv1D(filters, 1,\n",
    "               kernel_initializer=init,\n",
    "               kernel_regularizer=regularizer,\n",
    "               activation='linear')(x)\n",
    "\n",
    "    print('conv1d x',x)\n",
    "    # Model_Encoder_Layer\n",
    "    # shared layers\n",
    "    Encoder_DepthwiseConv2 = []\n",
    "    Encoder_SelfAttention2 = []\n",
    "    Encoder_FeedForward2 = []\n",
    "    for i in range(7):\n",
    "        DepthwiseConv_share_2_temp = []\n",
    "        for i in range(2):\n",
    "            DepthwiseConv_share_2_temp.append(DepthwiseConv1D(5, filters))\n",
    "\n",
    "        Encoder_DepthwiseConv2.append(DepthwiseConv_share_2_temp)\n",
    "        Encoder_SelfAttention2.append([Conv1D(2 * filters, 1,\n",
    "                                              kernel_initializer=init,\n",
    "                                              kernel_regularizer=regularizer),\n",
    "                                       Conv1D(filters, 1,\n",
    "                                              kernel_initializer=init,\n",
    "                                              kernel_regularizer=regularizer),\n",
    "                                       MultiHeadAttention(filters, num_head, dropout=dropout, bias=False)])\n",
    "        Encoder_FeedForward2.append([Conv1D(filters, 1,\n",
    "                                            kernel_initializer=init,\n",
    "                                            kernel_regularizer=regularizer,\n",
    "                                            activation='relu'),\n",
    "                                     Conv1D(filters, 1,\n",
    "                                            kernel_initializer=init,\n",
    "                                            kernel_regularizer=regularizer,\n",
    "                                            activation='linear')])\n",
    "\n",
    "    outputs = [x]\n",
    "    for i in range(3):\n",
    "        x = outputs[-1]\n",
    "        for j in range(7):\n",
    "            x = PositionEmbedding()(x)\n",
    "            x = conv_block(Encoder_DepthwiseConv2[j], x, 2, dropout, l=j, L=7)\n",
    "            x = attention_block(Encoder_SelfAttention2[j], x, c_mask, dropout, l=j, L=7)\n",
    "            x = feed_forward_block(Encoder_FeedForward2[j], x, dropout, l=j, L=7)\n",
    "        outputs.append(x)\n",
    "     \n",
    "    print('outputs',outputs)\n",
    "    # Output_Layer\n",
    "    x_start = Concatenate()([outputs[1], outputs[2]])\n",
    "    print('output_layer x_start',x_start)\n",
    "    '''\n",
    "    keras.layers.Conv1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "    \n",
    "    Input shape:\n",
    "      3D tensor with shape: `(batch_size, time_steps, input_dim)`\n",
    "\n",
    "  Output shape:\n",
    "      3D tensor with shape: `(batch_size, new_steps, filters)`\n",
    "      `steps` value might have changed due to padding or strides.\n",
    "\n",
    "这也可以解释，为什么在Keras中使用Conv1D可以进行自然语言处理，因为在自然语言处理中，我们假设一个序列是600个单词，每个单词的词向量是300维，那么一个序列输入到网络中就是（600,300），当我使用Conv1D进行卷积的时候，实际上就完成了直接在序列上的卷积，卷积的时候实际是以（3,300）进行卷积，又因为每一行都是一个词向量，因此使用Conv1D（kernel_size=3）也就相当于使用神经网络进行了n_gram=3的特征提取了。这也是为什么使用卷积神经网络处理文本会非常快速有效的内涵。\n",
    "\n",
    "Conv1D（kernel_size=3）实际就是Conv2D（kernel_size=（3,300）），当然必须把输入也reshape成（600,300,1），即可在多行上进行Conv2D卷积。\n",
    "所以这里的kernel_size=1，是conv2d的（1，词向量维度）\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    x_start = Conv1D(1, 1,\n",
    "                     kernel_initializer=init,\n",
    "                     kernel_regularizer=regularizer,\n",
    "                     activation='linear')(x_start)\n",
    "    print('conv1D x_start',x_start)\n",
    "    \n",
    "    #从tensor中删除所有大小是1的维度\n",
    "    x_start = Lambda(lambda x: tf.squeeze(x, axis=-1))(x_start)\n",
    "    print('squeeze x_start',x_start)\n",
    "    \n",
    "    \n",
    "    ## mask_logits输出维度与输入维度一样\n",
    "    x_start = Lambda(lambda x: mask_logits(x[0], x[1]))([x_start, c_mask])\n",
    "    print('mask_logits x_start',x_start)\n",
    "    \n",
    "    ##输出的x_start是已经经过了softmax计算之后的值\n",
    "    \n",
    "    '''\n",
    "    softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)\n",
    "    Returns:\n",
    "    A `Tensor`. Has the same type and shape as `logits`.\n",
    "    '''\n",
    "    x_start = Lambda(lambda x: K.softmax(x), name='start')(x_start)  # [bs, len]\n",
    "    print('x_start softmax',x_start,)\n",
    "\n",
    "    x_end = Concatenate()([outputs[1], outputs[3]])\n",
    "    x_end = Conv1D(1, 1,\n",
    "                   kernel_initializer=init,\n",
    "                   kernel_regularizer=regularizer,\n",
    "                   activation='linear')(x_end)\n",
    "    x_end = Lambda(lambda x: tf.squeeze(x, axis=-1))(x_end)\n",
    "    x_end = Lambda(lambda x: mask_logits(x[0], x[1]))([x_end, c_mask])\n",
    "    x_end = Lambda(lambda x: K.softmax(x), name='end')(x_end)  # [bs, len]\n",
    "\n",
    "    x_start_fin, x_end_fin = QAoutputBlock(ans_limit, name='qa_output')([x_start, x_end])\n",
    "\n",
    "    # if use model.fit, the output shape must be padded to the max length\n",
    "    x_start = LabelPadding(cont_limit, name='start_pos')(x_start)\n",
    "    x_end = LabelPadding(cont_limit, name='end_pos')(x_end)\n",
    "    print('x_start  x_start_fin x_end x_end_fin ',x_start,x_start_fin,x_end,x_end_fin)\n",
    "    return Model(inputs=[contw_input_, quesw_input_, contc_input_, quesc_input_],\n",
    "                 outputs=[x_start, x_end, x_start_fin, x_end_fin])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1012 07:36:02.857562 140566226282240 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "W1012 07:36:04.123937 140566226282240 nn_ops.py:4283] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1012 07:36:04.226406 140566226282240 nn_ops.py:4283] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1012 07:36:04.332454 140566226282240 nn_ops.py:4283] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1012 07:36:04.426476 140566226282240 nn_ops.py:4283] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1012 07:36:05.389502 140566226282240 nn_ops.py:4283] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_cont=Tensor(\"layer_dropout_5/cond/Identity:0\", shape=(None, None, 128), dtype=float32)\n",
      "  x_ques=Tensor(\"layer_dropout_11/cond/Identity:0\", shape=(None, None, 128), dtype=float32)\n",
      "  c_mask=Tensor(\"batch_slice_4/Slice:0\", shape=(None, None), dtype=bool)\n",
      "  q_mask=Tensor(\"batch_slice_5/Slice:0\", shape=(None, None), dtype=bool)\n",
      "\n",
      "result Tensor(\"context2query_attention/concat_2:0\", shape=(None, None, 512), dtype=float32)\n",
      "Context_to_Query_Attention_Layer x Tensor(\"context2query_attention/concat_2:0\", shape=(None, None, 512), dtype=float32)\n",
      "conv1d x Tensor(\"conv1d_4_1/BiasAdd:0\", shape=(None, None, 128), dtype=float32)\n",
      "outputs [<tf.Tensor 'conv1d_4_1/BiasAdd:0' shape=(None, None, 128) dtype=float32>, <tf.Tensor 'layer_dropout_39/cond/Identity:0' shape=(None, None, 128) dtype=float32>, <tf.Tensor 'layer_dropout_67/cond/Identity:0' shape=(None, None, 128) dtype=float32>, <tf.Tensor 'layer_dropout_95/cond/Identity:0' shape=(None, None, 128) dtype=float32>]\n",
      "output_layer x_start Tensor(\"concatenate_2/concat:0\", shape=(None, None, 256), dtype=float32)\n",
      "conv1D x_start Tensor(\"conv1d_33/BiasAdd:0\", shape=(None, None, 1), dtype=float32)\n",
      "squeeze x_start Tensor(\"lambda_12/Squeeze:0\", shape=(None, None), dtype=float32)\n",
      "mask_logits x_start Tensor(\"lambda_13/add:0\", shape=(None, None), dtype=float32)\n",
      "x_start softmax Tensor(\"start/Softmax:0\", shape=(None, None), dtype=float32)\n",
      "x_start  x_start_fin x_end x_end_fin  Tensor(\"start_pos/concat:0\", shape=(None, None), dtype=float32) Tensor(\"qa_output/Reshape:0\", shape=(None, 1), dtype=float32) Tensor(\"end_pos/concat:0\", shape=(None, None), dtype=float32) Tensor(\"qa_output/Reshape_1:0\", shape=(None, 1), dtype=float32)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, None)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None)         0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1)            0           lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, None, 16)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 1)            0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, None, 16)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_slice_2 (BatchSlice)      (None, None, None)   0           input_3[0][0]                    \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_slice_3 (BatchSlice)      (None, None, None)   0           input_4[0][0]                    \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (Embedding)      (None, None, None, 6 78912       batch_slice_2[0][0]              \n",
      "                                                                 batch_slice_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 16, 64)       0           char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 16, 64)       0           char_embedding[1][0]             \n",
      "__________________________________________________________________________________________________\n",
      "char_conv (Conv1D)              (None, 12, 128)      41088       lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_slice (BatchSlice)        (None, None)         0           input_1[0][0]                    \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 128)          0           char_conv[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_slice_1 (BatchSlice)      (None, None)         0           input_2[0][0]                    \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           char_conv[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, None, 300)    3000000     batch_slice[0][0]                \n",
      "                                                                 batch_slice_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, None, 128)    0           global_max_pooling1d[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, None, 128)    0           global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, 428)    0           word_embedding[0][0]             \n",
      "                                                                 lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 428)    0           word_embedding[1][0]             \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "highway_input_projection (Conv1 (None, None, 128)    54912       concatenate[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "highway0_linear (Conv1D)        (None, None, 128)    16512       highway_input_projection[0][0]   \n",
      "                                                                 highway_input_projection[1][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 128)    0           highway0_linear[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "highway0_gate (Conv1D)          (None, None, 128)    16512       highway_input_projection[0][0]   \n",
      "                                                                 highway_input_projection[1][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 128)    0           highway0_linear[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, None, 128)    0           dropout[0][0]                    \n",
      "                                                                 highway0_gate[0][0]              \n",
      "                                                                 highway_input_projection[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, None, 128)    0           dropout_2[0][0]                  \n",
      "                                                                 highway0_gate[1][0]              \n",
      "                                                                 highway_input_projection[1][0]   \n",
      "__________________________________________________________________________________________________\n",
      "highway1_linear (Conv1D)        (None, None, 128)    16512       lambda_8[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 128)    0           highway1_linear[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "highway1_gate (Conv1D)          (None, None, 128)    16512       lambda_8[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, None, 128)    0           highway1_linear[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, None, 128)    0           dropout_1[0][0]                  \n",
      "                                                                 highway1_gate[0][0]              \n",
      "                                                                 lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, None, 128)    0           dropout_3[0][0]                  \n",
      "                                                                 highway1_gate[1][0]              \n",
      "                                                                 lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding (Position_E (None, None, 128)    0           lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_1 (Position (None, None, 128)    0           lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, None, 128)    256         position__embedding[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (None, None, 128)    256         position__embedding_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, None, 128)    0           layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, None, 128)    0           layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv1d (DepthwiseConv (None, None, 128)    17408       dropout_4[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout (LayerDropout)    (None, None, 128)    0           depthwise_conv1d[0][0]           \n",
      "                                                                 position__embedding[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_6 (LayerDropout)  (None, None, 128)    0           depthwise_conv1d[1][0]           \n",
      "                                                                 position__embedding_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, None, 128)    256         layer_dropout[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (None, None, 128)    256         layer_dropout_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv1d_1 (DepthwiseCo (None, None, 128)    17408       layer_normalization_1[0][0]      \n",
      "                                                                 layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_1 (LayerDropout)  (None, None, 128)    0           depthwise_conv1d_1[0][0]         \n",
      "                                                                 layer_dropout[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_7 (LayerDropout)  (None, None, 128)    0           depthwise_conv1d_1[1][0]         \n",
      "                                                                 layer_dropout_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, None, 128)    256         layer_dropout_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_8 (LayerNor (None, None, 128)    256         layer_dropout_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, None, 128)    0           layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, None, 128)    0           layer_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv1d_2 (DepthwiseCo (None, None, 128)    17408       dropout_5[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_2 (LayerDropout)  (None, None, 128)    0           depthwise_conv1d_2[0][0]         \n",
      "                                                                 layer_dropout_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_8 (LayerDropout)  (None, None, 128)    0           depthwise_conv1d_2[1][0]         \n",
      "                                                                 layer_dropout_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, None, 128)    256         layer_dropout_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_9 (LayerNor (None, None, 128)    256         layer_dropout_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv1d_3 (DepthwiseCo (None, None, 128)    17408       layer_normalization_3[0][0]      \n",
      "                                                                 layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_3 (LayerDropout)  (None, None, 128)    0           depthwise_conv1d_3[0][0]         \n",
      "                                                                 layer_dropout_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_9 (LayerDropout)  (None, None, 128)    0           depthwise_conv1d_3[1][0]         \n",
      "                                                                 layer_dropout_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (None, None, 128)    256         layer_dropout_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_10 (LayerNo (None, None, 128)    256         layer_dropout_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, None, 128)    0           layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, None, 128)    0           layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, None, 256)    33024       dropout_6[0][0]                  \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 128)    16512       dropout_6[0][0]                  \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_slice_4 (BatchSlice)      (None, None)         0           lambda[0][0]                     \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_slice_5 (BatchSlice)      (None, None)         0           lambda_1[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention (MultiHead (None, None, 128)    0           conv1d[0][0]                     \n",
      "                                                                 conv1d_1[0][0]                   \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "                                                                 conv1d[1][0]                     \n",
      "                                                                 conv1d_1[1][0]                   \n",
      "                                                                 batch_slice_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_4 (LayerDropout)  (None, None, 128)    0           multi_head_attention[0][0]       \n",
      "                                                                 layer_dropout_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_10 (LayerDropout) (None, None, 128)    0           multi_head_attention[1][0]       \n",
      "                                                                 layer_dropout_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, None, 128)    256         layer_dropout_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_11 (LayerNo (None, None, 128)    256         layer_dropout_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, None, 128)    0           layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, None, 128)    0           layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 128)    16512       dropout_7[0][0]                  \n",
      "                                                                 dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 128)    16512       conv1d_2[0][0]                   \n",
      "                                                                 conv1d_2[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_5 (LayerDropout)  (None, None, 128)    0           conv1d_3[0][0]                   \n",
      "                                                                 layer_dropout_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_11 (LayerDropout) (None, None, 128)    0           conv1d_3[1][0]                   \n",
      "                                                                 layer_dropout_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "context2query_attention (contex (None, None, 512)    385         layer_dropout_5[0][0]            \n",
      "                                                                 layer_dropout_11[0][0]           \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "                                                                 batch_slice_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 128)    65664       context2query_attention[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_2 (Position (None, None, 128)    0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_12 (LayerNo (None, None, 128)    256         position__embedding_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, None, 128)    0           layer_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv1d_4 (DepthwiseCo (None, None, 128)    17152       dropout_12[0][0]                 \n",
      "                                                                 dropout_33[0][0]                 \n",
      "                                                                 dropout_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_12 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_4[0][0]         \n",
      "                                                                 position__embedding_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_13 (LayerNo (None, None, 128)    256         layer_dropout_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv1d_5 (DepthwiseCo (None, None, 128)    17152       layer_normalization_13[0][0]     \n",
      "                                                                 layer_normalization_41[0][0]     \n",
      "                                                                 layer_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_13 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_5[0][0]         \n",
      "                                                                 layer_dropout_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_14 (LayerNo (None, None, 128)    256         layer_dropout_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, None, 128)    0           layer_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 256)    33024       dropout_13[0][0]                 \n",
      "                                                                 dropout_34[0][0]                 \n",
      "                                                                 dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 128)    16512       dropout_13[0][0]                 \n",
      "                                                                 dropout_34[0][0]                 \n",
      "                                                                 dropout_55[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_1 (MultiHe (None, None, 128)    0           conv1d_5[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "                                                                 conv1d_5[1][0]                   \n",
      "                                                                 conv1d_6[1][0]                   \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "                                                                 conv1d_5[2][0]                   \n",
      "                                                                 conv1d_6[2][0]                   \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_14 (LayerDropout) (None, None, 128)    0           multi_head_attention_1[0][0]     \n",
      "                                                                 layer_dropout_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_15 (LayerNo (None, None, 128)    256         layer_dropout_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, None, 128)    0           layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, None, 128)    16512       dropout_14[0][0]                 \n",
      "                                                                 dropout_35[0][0]                 \n",
      "                                                                 dropout_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, None, 128)    16512       conv1d_7[0][0]                   \n",
      "                                                                 conv1d_7[1][0]                   \n",
      "                                                                 conv1d_7[2][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_15 (LayerDropout) (None, None, 128)    0           conv1d_8[0][0]                   \n",
      "                                                                 layer_dropout_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_3 (Position (None, None, 128)    0           layer_dropout_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_16 (LayerNo (None, None, 128)    256         position__embedding_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, None, 128)    0           layer_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv1d_6 (DepthwiseCo (None, None, 128)    17152       dropout_15[0][0]                 \n",
      "                                                                 dropout_36[0][0]                 \n",
      "                                                                 dropout_57[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_16 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_6[0][0]         \n",
      "                                                                 position__embedding_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_17 (LayerNo (None, None, 128)    256         layer_dropout_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv1d_7 (DepthwiseCo (None, None, 128)    17152       layer_normalization_17[0][0]     \n",
      "                                                                 layer_normalization_45[0][0]     \n",
      "                                                                 layer_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_17 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_7[0][0]         \n",
      "                                                                 layer_dropout_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_18 (LayerNo (None, None, 128)    256         layer_dropout_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, None, 128)    0           layer_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, None, 256)    33024       dropout_16[0][0]                 \n",
      "                                                                 dropout_37[0][0]                 \n",
      "                                                                 dropout_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, None, 128)    16512       dropout_16[0][0]                 \n",
      "                                                                 dropout_37[0][0]                 \n",
      "                                                                 dropout_58[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_2 (MultiHe (None, None, 128)    0           conv1d_9[0][0]                   \n",
      "                                                                 conv1d_10[0][0]                  \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "                                                                 conv1d_9[1][0]                   \n",
      "                                                                 conv1d_10[1][0]                  \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "                                                                 conv1d_9[2][0]                   \n",
      "                                                                 conv1d_10[2][0]                  \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_18 (LayerDropout) (None, None, 128)    0           multi_head_attention_2[0][0]     \n",
      "                                                                 layer_dropout_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_19 (LayerNo (None, None, 128)    256         layer_dropout_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, None, 128)    0           layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, None, 128)    16512       dropout_17[0][0]                 \n",
      "                                                                 dropout_38[0][0]                 \n",
      "                                                                 dropout_59[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, None, 128)    16512       conv1d_11[0][0]                  \n",
      "                                                                 conv1d_11[1][0]                  \n",
      "                                                                 conv1d_11[2][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_19 (LayerDropout) (None, None, 128)    0           conv1d_12[0][0]                  \n",
      "                                                                 layer_dropout_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_4 (Position (None, None, 128)    0           layer_dropout_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_20 (LayerNo (None, None, 128)    256         position__embedding_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, None, 128)    0           layer_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv1d_8 (DepthwiseCo (None, None, 128)    17152       dropout_18[0][0]                 \n",
      "                                                                 dropout_39[0][0]                 \n",
      "                                                                 dropout_60[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_20 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_8[0][0]         \n",
      "                                                                 position__embedding_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_21 (LayerNo (None, None, 128)    256         layer_dropout_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv1d_9 (DepthwiseCo (None, None, 128)    17152       layer_normalization_21[0][0]     \n",
      "                                                                 layer_normalization_49[0][0]     \n",
      "                                                                 layer_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_21 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_9[0][0]         \n",
      "                                                                 layer_dropout_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_22 (LayerNo (None, None, 128)    256         layer_dropout_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, None, 128)    0           layer_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, None, 256)    33024       dropout_19[0][0]                 \n",
      "                                                                 dropout_40[0][0]                 \n",
      "                                                                 dropout_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, None, 128)    16512       dropout_19[0][0]                 \n",
      "                                                                 dropout_40[0][0]                 \n",
      "                                                                 dropout_61[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_3 (MultiHe (None, None, 128)    0           conv1d_13[0][0]                  \n",
      "                                                                 conv1d_14[0][0]                  \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "                                                                 conv1d_13[1][0]                  \n",
      "                                                                 conv1d_14[1][0]                  \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "                                                                 conv1d_13[2][0]                  \n",
      "                                                                 conv1d_14[2][0]                  \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_22 (LayerDropout) (None, None, 128)    0           multi_head_attention_3[0][0]     \n",
      "                                                                 layer_dropout_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_23 (LayerNo (None, None, 128)    256         layer_dropout_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, None, 128)    0           layer_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, None, 128)    16512       dropout_20[0][0]                 \n",
      "                                                                 dropout_41[0][0]                 \n",
      "                                                                 dropout_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, None, 128)    16512       conv1d_15[0][0]                  \n",
      "                                                                 conv1d_15[1][0]                  \n",
      "                                                                 conv1d_15[2][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_23 (LayerDropout) (None, None, 128)    0           conv1d_16[0][0]                  \n",
      "                                                                 layer_dropout_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_5 (Position (None, None, 128)    0           layer_dropout_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_24 (LayerNo (None, None, 128)    256         position__embedding_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, None, 128)    0           layer_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv1d_10 (DepthwiseC (None, None, 128)    17152       dropout_21[0][0]                 \n",
      "                                                                 dropout_42[0][0]                 \n",
      "                                                                 dropout_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_24 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_10[0][0]        \n",
      "                                                                 position__embedding_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_25 (LayerNo (None, None, 128)    256         layer_dropout_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv1d_11 (DepthwiseC (None, None, 128)    17152       layer_normalization_25[0][0]     \n",
      "                                                                 layer_normalization_53[0][0]     \n",
      "                                                                 layer_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_25 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_11[0][0]        \n",
      "                                                                 layer_dropout_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_26 (LayerNo (None, None, 128)    256         layer_dropout_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, None, 128)    0           layer_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, None, 256)    33024       dropout_22[0][0]                 \n",
      "                                                                 dropout_43[0][0]                 \n",
      "                                                                 dropout_64[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, None, 128)    16512       dropout_22[0][0]                 \n",
      "                                                                 dropout_43[0][0]                 \n",
      "                                                                 dropout_64[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_4 (MultiHe (None, None, 128)    0           conv1d_17[0][0]                  \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "                                                                 conv1d_17[1][0]                  \n",
      "                                                                 conv1d_18[1][0]                  \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "                                                                 conv1d_17[2][0]                  \n",
      "                                                                 conv1d_18[2][0]                  \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_26 (LayerDropout) (None, None, 128)    0           multi_head_attention_4[0][0]     \n",
      "                                                                 layer_dropout_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_27 (LayerNo (None, None, 128)    256         layer_dropout_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, None, 128)    0           layer_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, None, 128)    16512       dropout_23[0][0]                 \n",
      "                                                                 dropout_44[0][0]                 \n",
      "                                                                 dropout_65[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, None, 128)    16512       conv1d_19[0][0]                  \n",
      "                                                                 conv1d_19[1][0]                  \n",
      "                                                                 conv1d_19[2][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_27 (LayerDropout) (None, None, 128)    0           conv1d_20[0][0]                  \n",
      "                                                                 layer_dropout_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_6 (Position (None, None, 128)    0           layer_dropout_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_28 (LayerNo (None, None, 128)    256         position__embedding_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, None, 128)    0           layer_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv1d_12 (DepthwiseC (None, None, 128)    17152       dropout_24[0][0]                 \n",
      "                                                                 dropout_45[0][0]                 \n",
      "                                                                 dropout_66[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_28 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_12[0][0]        \n",
      "                                                                 position__embedding_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_29 (LayerNo (None, None, 128)    256         layer_dropout_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv1d_13 (DepthwiseC (None, None, 128)    17152       layer_normalization_29[0][0]     \n",
      "                                                                 layer_normalization_57[0][0]     \n",
      "                                                                 layer_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_29 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_13[0][0]        \n",
      "                                                                 layer_dropout_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_30 (LayerNo (None, None, 128)    256         layer_dropout_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, None, 128)    0           layer_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, None, 256)    33024       dropout_25[0][0]                 \n",
      "                                                                 dropout_46[0][0]                 \n",
      "                                                                 dropout_67[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, None, 128)    16512       dropout_25[0][0]                 \n",
      "                                                                 dropout_46[0][0]                 \n",
      "                                                                 dropout_67[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_5 (MultiHe (None, None, 128)    0           conv1d_21[0][0]                  \n",
      "                                                                 conv1d_22[0][0]                  \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "                                                                 conv1d_21[1][0]                  \n",
      "                                                                 conv1d_22[1][0]                  \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "                                                                 conv1d_21[2][0]                  \n",
      "                                                                 conv1d_22[2][0]                  \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_30 (LayerDropout) (None, None, 128)    0           multi_head_attention_5[0][0]     \n",
      "                                                                 layer_dropout_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_31 (LayerNo (None, None, 128)    256         layer_dropout_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, None, 128)    0           layer_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, None, 128)    16512       dropout_26[0][0]                 \n",
      "                                                                 dropout_47[0][0]                 \n",
      "                                                                 dropout_68[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, None, 128)    16512       conv1d_23[0][0]                  \n",
      "                                                                 conv1d_23[1][0]                  \n",
      "                                                                 conv1d_23[2][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_31 (LayerDropout) (None, None, 128)    0           conv1d_24[0][0]                  \n",
      "                                                                 layer_dropout_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_7 (Position (None, None, 128)    0           layer_dropout_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_32 (LayerNo (None, None, 128)    256         position__embedding_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, None, 128)    0           layer_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv1d_14 (DepthwiseC (None, None, 128)    17152       dropout_27[0][0]                 \n",
      "                                                                 dropout_48[0][0]                 \n",
      "                                                                 dropout_69[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_32 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_14[0][0]        \n",
      "                                                                 position__embedding_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_33 (LayerNo (None, None, 128)    256         layer_dropout_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv1d_15 (DepthwiseC (None, None, 128)    17152       layer_normalization_33[0][0]     \n",
      "                                                                 layer_normalization_61[0][0]     \n",
      "                                                                 layer_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_33 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_15[0][0]        \n",
      "                                                                 layer_dropout_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_34 (LayerNo (None, None, 128)    256         layer_dropout_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, None, 128)    0           layer_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, None, 256)    33024       dropout_28[0][0]                 \n",
      "                                                                 dropout_49[0][0]                 \n",
      "                                                                 dropout_70[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, None, 128)    16512       dropout_28[0][0]                 \n",
      "                                                                 dropout_49[0][0]                 \n",
      "                                                                 dropout_70[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_6 (MultiHe (None, None, 128)    0           conv1d_25[0][0]                  \n",
      "                                                                 conv1d_26[0][0]                  \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "                                                                 conv1d_25[1][0]                  \n",
      "                                                                 conv1d_26[1][0]                  \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "                                                                 conv1d_25[2][0]                  \n",
      "                                                                 conv1d_26[2][0]                  \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_34 (LayerDropout) (None, None, 128)    0           multi_head_attention_6[0][0]     \n",
      "                                                                 layer_dropout_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_35 (LayerNo (None, None, 128)    256         layer_dropout_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, None, 128)    0           layer_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, None, 128)    16512       dropout_29[0][0]                 \n",
      "                                                                 dropout_50[0][0]                 \n",
      "                                                                 dropout_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, None, 128)    16512       conv1d_27[0][0]                  \n",
      "                                                                 conv1d_27[1][0]                  \n",
      "                                                                 conv1d_27[2][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_35 (LayerDropout) (None, None, 128)    0           conv1d_28[0][0]                  \n",
      "                                                                 layer_dropout_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_8 (Position (None, None, 128)    0           layer_dropout_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_36 (LayerNo (None, None, 128)    256         position__embedding_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, None, 128)    0           layer_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv1d_16 (DepthwiseC (None, None, 128)    17152       dropout_30[0][0]                 \n",
      "                                                                 dropout_51[0][0]                 \n",
      "                                                                 dropout_72[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_36 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_16[0][0]        \n",
      "                                                                 position__embedding_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_37 (LayerNo (None, None, 128)    256         layer_dropout_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "depthwise_conv1d_17 (DepthwiseC (None, None, 128)    17152       layer_normalization_37[0][0]     \n",
      "                                                                 layer_normalization_65[0][0]     \n",
      "                                                                 layer_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_37 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_17[0][0]        \n",
      "                                                                 layer_dropout_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_38 (LayerNo (None, None, 128)    256         layer_dropout_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, None, 128)    0           layer_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, None, 256)    33024       dropout_31[0][0]                 \n",
      "                                                                 dropout_52[0][0]                 \n",
      "                                                                 dropout_73[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, None, 128)    16512       dropout_31[0][0]                 \n",
      "                                                                 dropout_52[0][0]                 \n",
      "                                                                 dropout_73[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_7 (MultiHe (None, None, 128)    0           conv1d_29[0][0]                  \n",
      "                                                                 conv1d_30[0][0]                  \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "                                                                 conv1d_29[1][0]                  \n",
      "                                                                 conv1d_30[1][0]                  \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "                                                                 conv1d_29[2][0]                  \n",
      "                                                                 conv1d_30[2][0]                  \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_38 (LayerDropout) (None, None, 128)    0           multi_head_attention_7[0][0]     \n",
      "                                                                 layer_dropout_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_39 (LayerNo (None, None, 128)    256         layer_dropout_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, None, 128)    0           layer_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, None, 128)    16512       dropout_32[0][0]                 \n",
      "                                                                 dropout_53[0][0]                 \n",
      "                                                                 dropout_74[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, None, 128)    16512       conv1d_31[0][0]                  \n",
      "                                                                 conv1d_31[1][0]                  \n",
      "                                                                 conv1d_31[2][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_39 (LayerDropout) (None, None, 128)    0           conv1d_32[0][0]                  \n",
      "                                                                 layer_dropout_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_9 (Position (None, None, 128)    0           layer_dropout_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_40 (LayerNo (None, None, 128)    256         position__embedding_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, None, 128)    0           layer_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_40 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_4[1][0]         \n",
      "                                                                 position__embedding_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_41 (LayerNo (None, None, 128)    256         layer_dropout_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_41 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_5[1][0]         \n",
      "                                                                 layer_dropout_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_42 (LayerNo (None, None, 128)    256         layer_dropout_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, None, 128)    0           layer_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_42 (LayerDropout) (None, None, 128)    0           multi_head_attention_1[1][0]     \n",
      "                                                                 layer_dropout_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_43 (LayerNo (None, None, 128)    256         layer_dropout_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, None, 128)    0           layer_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_43 (LayerDropout) (None, None, 128)    0           conv1d_8[1][0]                   \n",
      "                                                                 layer_dropout_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_10 (Positio (None, None, 128)    0           layer_dropout_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_44 (LayerNo (None, None, 128)    256         position__embedding_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, None, 128)    0           layer_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_44 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_6[1][0]         \n",
      "                                                                 position__embedding_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_45 (LayerNo (None, None, 128)    256         layer_dropout_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_45 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_7[1][0]         \n",
      "                                                                 layer_dropout_44[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_46 (LayerNo (None, None, 128)    256         layer_dropout_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, None, 128)    0           layer_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_46 (LayerDropout) (None, None, 128)    0           multi_head_attention_2[1][0]     \n",
      "                                                                 layer_dropout_45[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_47 (LayerNo (None, None, 128)    256         layer_dropout_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, None, 128)    0           layer_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_47 (LayerDropout) (None, None, 128)    0           conv1d_12[1][0]                  \n",
      "                                                                 layer_dropout_46[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_11 (Positio (None, None, 128)    0           layer_dropout_47[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_48 (LayerNo (None, None, 128)    256         position__embedding_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, None, 128)    0           layer_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_48 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_8[1][0]         \n",
      "                                                                 position__embedding_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_49 (LayerNo (None, None, 128)    256         layer_dropout_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_49 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_9[1][0]         \n",
      "                                                                 layer_dropout_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_50 (LayerNo (None, None, 128)    256         layer_dropout_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, None, 128)    0           layer_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_50 (LayerDropout) (None, None, 128)    0           multi_head_attention_3[1][0]     \n",
      "                                                                 layer_dropout_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_51 (LayerNo (None, None, 128)    256         layer_dropout_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, None, 128)    0           layer_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_51 (LayerDropout) (None, None, 128)    0           conv1d_16[1][0]                  \n",
      "                                                                 layer_dropout_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_12 (Positio (None, None, 128)    0           layer_dropout_51[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_52 (LayerNo (None, None, 128)    256         position__embedding_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, None, 128)    0           layer_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_52 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_10[1][0]        \n",
      "                                                                 position__embedding_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_53 (LayerNo (None, None, 128)    256         layer_dropout_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_53 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_11[1][0]        \n",
      "                                                                 layer_dropout_52[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_54 (LayerNo (None, None, 128)    256         layer_dropout_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, None, 128)    0           layer_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_54 (LayerDropout) (None, None, 128)    0           multi_head_attention_4[1][0]     \n",
      "                                                                 layer_dropout_53[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_55 (LayerNo (None, None, 128)    256         layer_dropout_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, None, 128)    0           layer_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_55 (LayerDropout) (None, None, 128)    0           conv1d_20[1][0]                  \n",
      "                                                                 layer_dropout_54[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_13 (Positio (None, None, 128)    0           layer_dropout_55[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_56 (LayerNo (None, None, 128)    256         position__embedding_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, None, 128)    0           layer_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_56 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_12[1][0]        \n",
      "                                                                 position__embedding_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_57 (LayerNo (None, None, 128)    256         layer_dropout_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_57 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_13[1][0]        \n",
      "                                                                 layer_dropout_56[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_58 (LayerNo (None, None, 128)    256         layer_dropout_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, None, 128)    0           layer_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_58 (LayerDropout) (None, None, 128)    0           multi_head_attention_5[1][0]     \n",
      "                                                                 layer_dropout_57[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_59 (LayerNo (None, None, 128)    256         layer_dropout_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, None, 128)    0           layer_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_59 (LayerDropout) (None, None, 128)    0           conv1d_24[1][0]                  \n",
      "                                                                 layer_dropout_58[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_14 (Positio (None, None, 128)    0           layer_dropout_59[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_60 (LayerNo (None, None, 128)    256         position__embedding_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, None, 128)    0           layer_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_60 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_14[1][0]        \n",
      "                                                                 position__embedding_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_61 (LayerNo (None, None, 128)    256         layer_dropout_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_61 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_15[1][0]        \n",
      "                                                                 layer_dropout_60[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_62 (LayerNo (None, None, 128)    256         layer_dropout_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, None, 128)    0           layer_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_62 (LayerDropout) (None, None, 128)    0           multi_head_attention_6[1][0]     \n",
      "                                                                 layer_dropout_61[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_63 (LayerNo (None, None, 128)    256         layer_dropout_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, None, 128)    0           layer_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_63 (LayerDropout) (None, None, 128)    0           conv1d_28[1][0]                  \n",
      "                                                                 layer_dropout_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_15 (Positio (None, None, 128)    0           layer_dropout_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_64 (LayerNo (None, None, 128)    256         position__embedding_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, None, 128)    0           layer_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_64 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_16[1][0]        \n",
      "                                                                 position__embedding_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_65 (LayerNo (None, None, 128)    256         layer_dropout_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_65 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_17[1][0]        \n",
      "                                                                 layer_dropout_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_66 (LayerNo (None, None, 128)    256         layer_dropout_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, None, 128)    0           layer_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_66 (LayerDropout) (None, None, 128)    0           multi_head_attention_7[1][0]     \n",
      "                                                                 layer_dropout_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_67 (LayerNo (None, None, 128)    256         layer_dropout_66[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, None, 128)    0           layer_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_67 (LayerDropout) (None, None, 128)    0           conv1d_32[1][0]                  \n",
      "                                                                 layer_dropout_66[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_16 (Positio (None, None, 128)    0           layer_dropout_67[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_68 (LayerNo (None, None, 128)    256         position__embedding_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, None, 128)    0           layer_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_68 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_4[2][0]         \n",
      "                                                                 position__embedding_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_69 (LayerNo (None, None, 128)    256         layer_dropout_68[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_69 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_5[2][0]         \n",
      "                                                                 layer_dropout_68[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_70 (LayerNo (None, None, 128)    256         layer_dropout_69[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, None, 128)    0           layer_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_70 (LayerDropout) (None, None, 128)    0           multi_head_attention_1[2][0]     \n",
      "                                                                 layer_dropout_69[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_71 (LayerNo (None, None, 128)    256         layer_dropout_70[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, None, 128)    0           layer_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_71 (LayerDropout) (None, None, 128)    0           conv1d_8[2][0]                   \n",
      "                                                                 layer_dropout_70[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_17 (Positio (None, None, 128)    0           layer_dropout_71[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_72 (LayerNo (None, None, 128)    256         position__embedding_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)            (None, None, 128)    0           layer_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_72 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_6[2][0]         \n",
      "                                                                 position__embedding_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_73 (LayerNo (None, None, 128)    256         layer_dropout_72[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_73 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_7[2][0]         \n",
      "                                                                 layer_dropout_72[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_74 (LayerNo (None, None, 128)    256         layer_dropout_73[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)            (None, None, 128)    0           layer_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_74 (LayerDropout) (None, None, 128)    0           multi_head_attention_2[2][0]     \n",
      "                                                                 layer_dropout_73[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_75 (LayerNo (None, None, 128)    256         layer_dropout_74[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)            (None, None, 128)    0           layer_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_75 (LayerDropout) (None, None, 128)    0           conv1d_12[2][0]                  \n",
      "                                                                 layer_dropout_74[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_18 (Positio (None, None, 128)    0           layer_dropout_75[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_76 (LayerNo (None, None, 128)    256         position__embedding_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)            (None, None, 128)    0           layer_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_76 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_8[2][0]         \n",
      "                                                                 position__embedding_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_77 (LayerNo (None, None, 128)    256         layer_dropout_76[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_77 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_9[2][0]         \n",
      "                                                                 layer_dropout_76[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_78 (LayerNo (None, None, 128)    256         layer_dropout_77[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)            (None, None, 128)    0           layer_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_78 (LayerDropout) (None, None, 128)    0           multi_head_attention_3[2][0]     \n",
      "                                                                 layer_dropout_77[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_79 (LayerNo (None, None, 128)    256         layer_dropout_78[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, None, 128)    0           layer_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_79 (LayerDropout) (None, None, 128)    0           conv1d_16[2][0]                  \n",
      "                                                                 layer_dropout_78[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_19 (Positio (None, None, 128)    0           layer_dropout_79[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_80 (LayerNo (None, None, 128)    256         position__embedding_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, None, 128)    0           layer_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_80 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_10[2][0]        \n",
      "                                                                 position__embedding_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_81 (LayerNo (None, None, 128)    256         layer_dropout_80[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_81 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_11[2][0]        \n",
      "                                                                 layer_dropout_80[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_82 (LayerNo (None, None, 128)    256         layer_dropout_81[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_64 (Dropout)            (None, None, 128)    0           layer_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_82 (LayerDropout) (None, None, 128)    0           multi_head_attention_4[2][0]     \n",
      "                                                                 layer_dropout_81[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_83 (LayerNo (None, None, 128)    256         layer_dropout_82[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)            (None, None, 128)    0           layer_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_83 (LayerDropout) (None, None, 128)    0           conv1d_20[2][0]                  \n",
      "                                                                 layer_dropout_82[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_20 (Positio (None, None, 128)    0           layer_dropout_83[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_84 (LayerNo (None, None, 128)    256         position__embedding_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)            (None, None, 128)    0           layer_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_84 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_12[2][0]        \n",
      "                                                                 position__embedding_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_85 (LayerNo (None, None, 128)    256         layer_dropout_84[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_85 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_13[2][0]        \n",
      "                                                                 layer_dropout_84[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_86 (LayerNo (None, None, 128)    256         layer_dropout_85[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_67 (Dropout)            (None, None, 128)    0           layer_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_86 (LayerDropout) (None, None, 128)    0           multi_head_attention_5[2][0]     \n",
      "                                                                 layer_dropout_85[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_87 (LayerNo (None, None, 128)    256         layer_dropout_86[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)            (None, None, 128)    0           layer_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_87 (LayerDropout) (None, None, 128)    0           conv1d_24[2][0]                  \n",
      "                                                                 layer_dropout_86[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_21 (Positio (None, None, 128)    0           layer_dropout_87[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_88 (LayerNo (None, None, 128)    256         position__embedding_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_69 (Dropout)            (None, None, 128)    0           layer_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_88 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_14[2][0]        \n",
      "                                                                 position__embedding_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_89 (LayerNo (None, None, 128)    256         layer_dropout_88[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_89 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_15[2][0]        \n",
      "                                                                 layer_dropout_88[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_90 (LayerNo (None, None, 128)    256         layer_dropout_89[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_70 (Dropout)            (None, None, 128)    0           layer_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_90 (LayerDropout) (None, None, 128)    0           multi_head_attention_6[2][0]     \n",
      "                                                                 layer_dropout_89[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_91 (LayerNo (None, None, 128)    256         layer_dropout_90[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_71 (Dropout)            (None, None, 128)    0           layer_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_91 (LayerDropout) (None, None, 128)    0           conv1d_28[2][0]                  \n",
      "                                                                 layer_dropout_90[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_22 (Positio (None, None, 128)    0           layer_dropout_91[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_92 (LayerNo (None, None, 128)    256         position__embedding_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_72 (Dropout)            (None, None, 128)    0           layer_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_92 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_16[2][0]        \n",
      "                                                                 position__embedding_22[0][0]     \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_normalization_93 (LayerNo (None, None, 128)    256         layer_dropout_92[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_93 (LayerDropout) (None, None, 128)    0           depthwise_conv1d_17[2][0]        \n",
      "                                                                 layer_dropout_92[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_94 (LayerNo (None, None, 128)    256         layer_dropout_93[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, None, 128)    0           layer_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_94 (LayerDropout) (None, None, 128)    0           multi_head_attention_7[2][0]     \n",
      "                                                                 layer_dropout_93[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_95 (LayerNo (None, None, 128)    256         layer_dropout_94[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)            (None, None, 128)    0           layer_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_dropout_95 (LayerDropout) (None, None, 128)    0           conv1d_32[2][0]                  \n",
      "                                                                 layer_dropout_94[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 256)    0           layer_dropout_39[0][0]           \n",
      "                                                                 layer_dropout_67[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 256)    0           layer_dropout_39[0][0]           \n",
      "                                                                 layer_dropout_95[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, None, 1)      257         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, None, 1)      257         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, None)         0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, None)         0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, None)         0           lambda_12[0][0]                  \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, None)         0           lambda_14[0][0]                  \n",
      "                                                                 batch_slice_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "start (Lambda)                  (None, None)         0           lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "end (Lambda)                    (None, None)         0           lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "start_pos (LabelPadding)        (None, None)         0           start[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "end_pos (LabelPadding)          (None, None)         0           end[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "qa_output (QAoutputBlock)       [(None, 1), (None, 1 0           start[0][0]                      \n",
      "                                                                 end[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 4,302,339\n",
      "Trainable params: 1,302,339\n",
      "Non-trainable params: 3,000,000\n",
      "__________________________________________________________________________________________________\n",
      "Train on 300 samples\n",
      "300/300 [==============================] - 137s 455ms/sample - loss: 6385.2245 - start_pos_loss: 3192.4221 - end_pos_loss: 3194.9270 - qa_output_loss: 216.1546 - qa_output_1_loss: 230.7336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd727c9ae48>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.initializers import *\n",
    "# from QANet_keras import QANet\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#关闭eager模式\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "tf.keras.backend.set_learning_phase(1)  # training\n",
    "\n",
    "#就代表生成10000行 300列的浮点数，浮点数都是从0-1中随机。\n",
    "##模拟的是预训练的词向量，生成QAnet模型，然后训练时对于某个词，该词存在该向量中，就取出该词向量进行使用，不存在就使用随机的词向量\n",
    "##所以模型中一定有embedding模块\n",
    "\n",
    "embedding_matrix = np.random.random((10000, 300))\n",
    "embedding_matrix_char = np.random.random((1233, 64))\n",
    "config = {\n",
    "    'word_dim': 300,\n",
    "    'char_dim': 64,\n",
    "    'cont_limit': 400,\n",
    "    'ques_limit': 50,\n",
    "    'char_limit': 16,\n",
    "    'ans_limit': 30,\n",
    "    'char_input_size': 1233,\n",
    "    'filters': 128,\n",
    "    'num_head': 8,\n",
    "    'dropout': 0.5,\n",
    "    'batch_size': 16,\n",
    "    'epoch': 25,\n",
    "    'ema_decay': 0.9999,\n",
    "    'learning_rate': 1e-3,\n",
    "    'path': 'QA001',\n",
    "    'use_cove': True\n",
    "}\n",
    "model = QANet(config, word_mat=embedding_matrix, char_mat=embedding_matrix_char)\n",
    "model.summary()\n",
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.8, beta_2=0.999, epsilon=1e-7)\n",
    "\n",
    "##损失函数有4个应该是模型有4个输出，对应4个label,计算每个输出的损失函数，加权求和后最为最终的损失函数，权重即为loss_weights\n",
    "model.compile(optimizer=optimizer, loss=['categorical_crossentropy', 'categorical_crossentropy', 'mae', 'mae'],\n",
    "              loss_weights=[1, 1, 0, 0])\n",
    "\n",
    "# load data\n",
    "char_dim = 200\n",
    "cont_limit = 400\n",
    "ques_limit = 50\n",
    "char_limit = 16\n",
    "#生成维度为（300，cont_limit）,大小在0-10000之间的随机整数##上下文长度最大400个词，每个词的维度是300d（感觉不对，应该是有300个上下文）\n",
    "context_word = np.random.randint(0, 10000, (300, cont_limit))\n",
    "question_word = np.random.randint(0, 10000, (300, ques_limit))\n",
    "\n",
    "##最多400个词，每个词最多16个字符，字符维度也是300维度\n",
    "context_char = np.random.randint(0, 96, (300, cont_limit, char_limit))\n",
    "question_char = np.random.randint(0, 96, (300, ques_limit, char_limit))\n",
    "\n",
    "start_label = np.random.randint(0, 2, (300, cont_limit))\n",
    "end_label = np.random.randint(0, 2, (300, cont_limit))\n",
    "start_label_fin = np.argmax(start_label, axis=-1)\n",
    "end_label_fin = np.argmax(end_label, axis=-1)\n",
    "'''\n",
    "fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, \n",
    "validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, \n",
    "sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1)\n",
    "\n",
    "即\n",
    "x=[context_word, question_word, context_char, question_char],\n",
    "y=[start_label, end_label, start_label_fin, end_label_fin]\n",
    "\n",
    "Model(inputs=[contw_input_, quesw_input_, contc_input_, quesc_input_],\n",
    "                 outputs=[x_start, x_end, x_start_fin, x_end_fin])\n",
    "                 \n",
    "Model根据输入经过网络得到输出，输出和对应的label求出损失函数，损失函数加权后作为最终的损失函数，优化器使得最终的损失函数最小\n",
    "'''\n",
    "model.fit([context_word, question_word, context_char, question_char],\n",
    "          [start_label, end_label, start_label_fin, end_label_fin], batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
