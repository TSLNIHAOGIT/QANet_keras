{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qanet_fit_demp\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.initializers import *\n",
    "# from QANet_keras import QANet\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#关闭eager模式\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# tf.keras.backend.set_learning_phase(1)  # training\n",
    "\n",
    "#就代表生成10000行 300列的浮点数，浮点数都是从0-1中随机。\n",
    "##模拟的是预训练的词向量，生成QAnet模型，然后训练时对于某个词，该词存在该向量中，就取出该词向量进行使用，不存在就使用随机的词向量\n",
    "##所以模型中一定有embedding模块\n",
    "\n",
    "##假设总共10000个单词，每个单词300维度 \n",
    "embedding_matrix = np.random.random((10000, 300))\n",
    "##总共1233个字符，每个字符64维度\n",
    "embedding_matrix_char = np.random.random((1233, 64))\n",
    "config = {\n",
    "    'word_dim': 300,\n",
    "    'char_dim': 64,\n",
    "    'cont_limit': 400,\n",
    "    'ques_limit': 50,\n",
    "    'char_limit': 16,\n",
    "    'ans_limit': 30,\n",
    "    'char_input_size': 1233,\n",
    "    'filters': 128,\n",
    "    'num_head': 8,\n",
    "    'dropout': 0.5,\n",
    "    'batch_size': 16,\n",
    "    'epoch': 25,\n",
    "    'ema_decay': 0.9999,\n",
    "    'learning_rate': 1e-3,\n",
    "    'path': 'QA001',\n",
    "    'use_cove': True\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qanet_keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "# tf.keras.initializers.VarianceScaling\n",
    "# from layers.context2query_attention import context2query_attention\n",
    "\n",
    "# from layers.multihead_attention import Attention as MultiHeadAttention\n",
    "# from layers.position_embedding import Position_Embedding as PositionEmbedding\n",
    "# from layers.layer_norm import LayerNormalization\n",
    "# from layers.layer_dropout import LayerDropout\n",
    "# from layers.QAoutputBlock import QAoutputBlock\n",
    "# from layers.BatchSlice import BatchSlice\n",
    "# from layers.DepthwiseConv1D import DepthwiseConv1D\n",
    "# from layers.LabelPadding import LabelPadding\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "# parameters\n",
    "word_dim = config['word_dim']\n",
    "char_dim = config['char_dim']\n",
    "cont_limit = config['cont_limit']\n",
    "char_limit = config['char_limit']\n",
    "ans_limit = config['ans_limit']\n",
    "filters = config['filters']\n",
    "num_head = config['num_head']\n",
    "dropout = config['dropout']\n",
    "\n",
    "# Input Embedding Layer\n",
    "#`Input()` is used to instantiate a Keras tensor.S\n",
    "'''\n",
    "for example:\n",
    "inp = Input(shape=(maxlen,))\n",
    "计算得到\n",
    "inp.shape=(None, maxlen)\n",
    "shape: A shape tuple (integer), not including the batch size. For instance, \n",
    "shape=(32,)indicates that the expected input will be batches of 32-dimensional vectors.\n",
    "也就是说，不看batch_size的话，其实我们输入的是一个100维度的向量，正好对应了上面的maxlen对吧。\n",
    "\n",
    "那我们embedding层的作用，是将正整数下标转换为具有固定大小的向量。\n",
    "\n",
    "如[[4], [20]]->[[0.25,0.1], [0.6,-0.2]]。一定要注意到一个下标对应一个向量。\n",
    "\n",
    "'''\n",
    "#相当于词向量的维度是输入的None,完整的维度是(None,None)=（glove词向量中总的单词数，每个单词的维度）\n",
    "#第一个None是默认的词的个数（batch_size），第二None是自己输入的词向量的维度\n",
    "####这个不是用于词向量的，是为了获得mask矩阵\n",
    "\n",
    "\n",
    "contw_input_ = Input((None,))## [bs, c_len]=[上下文总数,每个上下文单词数]\n",
    "quesw_input_ = Input((None,))\n",
    "\n",
    "#（None,char_limit）可能代表（单词个数，每个单词的字符数），总体应该是（上下文总数，每个上下文单词数，每个单词字符数）\n",
    "contc_input_ = Input((None, char_limit))\n",
    "quesc_input_ = Input((None, char_limit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input test Tensor(\"input_23:0\", shape=(None, 111, 222), dtype=float32)\n",
      "contw_input_ : Tensor(\"input_17:0\", shape=(None, None), dtype=float32)\n",
      "contc_input_: Tensor(\"input_19:0\", shape=(None, None, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('input test',Input((111,222)))\n",
    "print('contw_input_ :',contw_input_ )\n",
    "print('contc_input_:',contc_input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mask\n",
    "c_mask = Lambda(lambda x: tf.cast(x, tf.bool))(contw_input_)  # [bs, c_len]\n",
    "q_mask = Lambda(lambda x: tf.cast(x, tf.bool))(quesw_input_)\n",
    "#reduce_sum默认是对所有元素求和\n",
    "\n",
    "##axis=1是对行求和,就是求每个context的长度\n",
    "cont_len = Lambda(lambda x: tf.expand_dims(tf.reduce_sum(tf.cast(x, tf.int32), axis=1), axis=1))(c_mask)\n",
    "ques_len = Lambda(lambda x: tf.expand_dims(tf.reduce_sum(tf.cast(x, tf.int32), axis=1), axis=1))(q_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.reduce_sum(tf.cast(x, tf.int32): tf.Tensor([4 3], shape=(2,), dtype=int32)\n",
      "c_mask Tensor(\"lambda_4/Identity:0\", shape=(None, None), dtype=bool)\n",
      "ques_len Tensor(\"lambda_7/Identity:0\", shape=(None, 1), dtype=int32)\n",
      "cont_len Tensor(\"lambda_6/Identity:0\", shape=(None, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "c_mask0=[[1,1,1,1,0],[1,1,1,0,0]]\n",
    "\n",
    "print('tf.reduce_sum(tf.cast(x, tf.int32):',tf.reduce_sum(tf.cast(c_mask0, tf.int32),axis=1))\n",
    "print('c_mask',c_mask)\n",
    "print('ques_len',ques_len)\n",
    "print('cont_len',cont_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer \n",
    "import tensorflow as tf\n",
    "\n",
    "class BatchSlice(Layer):\n",
    "    def __init__(self, dim=2, **kwargs):\n",
    "        self.dim = dim\n",
    "        super(BatchSlice, self).__init__(**kwargs)\n",
    "    '''\n",
    "    build(input_shape): 这是你定义权重的地方。这个方法必须设 self.built = True，可以通过调用 super([Layer], self).build() 完成\n",
    "    '''\n",
    "    def build(self, input_shape):\n",
    "        super(BatchSlice, self).build(input_shape)\n",
    "    '''\n",
    "    call(x): 这里是编写层的功能逻辑的地方。你只需要关注传入 call 的第一个参数：输入张量，除非你希望你的层支持masking。\n",
    "    '''\n",
    "    def call(self, x, mask=None):\n",
    "        x, length = x # [bs, len, dim]\n",
    "        length = tf.cast(tf.reduce_max(length), tf.int32)\n",
    "        st = [0] * self.dim\n",
    "        ed = [-1] * self.dim\n",
    "        ed[1] = length\n",
    "        x = tf.slice(x, st, ed)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice\n",
    "##contw_input_:（上下文个数，每个上下文词个数） cont_len：（所有上下文长度，1）\n",
    "contw_input = BatchSlice(dim=2)([contw_input_, cont_len])\n",
    "quesw_input = BatchSlice(dim=2)([quesw_input_, ques_len])\n",
    "contc_input = BatchSlice(dim=3)([contc_input_, cont_len])\n",
    "quesc_input = BatchSlice(dim=3)([quesc_input_, ques_len])\n",
    "c_mask = BatchSlice(dim=2)([c_mask, cont_len])\n",
    "q_mask = BatchSlice(dim=2)([q_mask, ques_len])\n",
    "c_maxlen = tf.cast(tf.reduce_max(cont_len), tf.int32)\n",
    "q_maxlen = tf.cast(tf.reduce_max(ques_len), tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[contw_input_, cont_len] [<tf.Tensor 'input_17:0' shape=(None, None) dtype=float32>, <tf.Tensor 'lambda_2/Identity:0' shape=(None, 1) dtype=int32>]\n"
     ]
    }
   ],
   "source": [
    "print('[contw_input_, cont_len]',[contw_input_, cont_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qanet_keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "# tf.keras.initializers.VarianceScaling\n",
    "# from layers.context2query_attention import context2query_attention\n",
    "\n",
    "# from layers.multihead_attention import Attention as MultiHeadAttention\n",
    "# from layers.position_embedding import Position_Embedding as PositionEmbedding\n",
    "# from layers.layer_norm import LayerNormalization\n",
    "# from layers.layer_dropout import LayerDropout\n",
    "# from layers.QAoutputBlock import QAoutputBlock\n",
    "# from layers.BatchSlice import BatchSlice\n",
    "# from layers.DepthwiseConv1D import DepthwiseConv1D\n",
    "# from layers.LabelPadding import LabelPadding\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "regularizer = l2(3e-7)\n",
    "init = VarianceScaling(scale=1.0, mode='fan_avg', distribution='uniform')\n",
    "init_relu = VarianceScaling(scale=2.0, mode='fan_in', distribution='normal')\n",
    "\n",
    "\n",
    "def mask_logits(inputs, mask, mask_value=-1e30):\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    return inputs + mask_value * (1 - mask)\n",
    "\n",
    "\n",
    "def highway(highway_layers, x, num_layers=2, dropout=0.1):\n",
    "    # reduce dim\n",
    "    x = highway_layers[0](x)\n",
    "    for i in range(num_layers):\n",
    "        T = highway_layers[i * 2 + 1](x)\n",
    "        H = highway_layers[i * 2 + 2](x)\n",
    "        H = Dropout(dropout)(H)\n",
    "        x = Lambda(lambda v: v[0] * v[1] + v[2] * (1 - v[1]))([H, T, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(conv_layers, x, num_conv=4, dropout=0.1, l=1., L=1.):\n",
    "    for i in range(num_conv):\n",
    "        residual = x\n",
    "        x = LayerNormalization()(x)\n",
    "        if i % 2 == 0:\n",
    "            x = Dropout(dropout)(x)\n",
    "        x = conv_layers[i](x)\n",
    "        x = LayerDropout(dropout * (l / L))([x, residual])\n",
    "    return x\n",
    "\n",
    "\n",
    "def attention_block(attention_layer, x, seq_mask, dropout=0.1, l=1., L=1.):\n",
    "    residual = x\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x1 = attention_layer[0](x)\n",
    "    x2 = attention_layer[1](x)\n",
    "    x = attention_layer[2]([x1, x2, seq_mask])\n",
    "    x = LayerDropout(dropout * (l / L))([x, residual])\n",
    "    return x\n",
    "\n",
    "\n",
    "def feed_forward_block(FeedForward_layers, x, dropout=0.1, l=1., L=1.):\n",
    "    residual = x\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = FeedForward_layers[0](x)\n",
    "    x = FeedForward_layers[1](x)\n",
    "    x = LayerDropout(dropout * (l / L))([x, residual])\n",
    "    return x\n",
    "\n",
    "\n",
    "def QANet(config, word_mat=None, char_mat=None, cove_model=None):\n",
    "    # parameters\n",
    "    word_dim = config['word_dim']\n",
    "    char_dim = config['char_dim']\n",
    "    cont_limit = config['cont_limit']\n",
    "    char_limit = config['char_limit']\n",
    "    ans_limit = config['ans_limit']\n",
    "    filters = config['filters']\n",
    "    num_head = config['num_head']\n",
    "    dropout = config['dropout']\n",
    "\n",
    "    # Input Embedding Layer\n",
    "    #`Input()` is used to instantiate a Keras tensor.S\n",
    "    contw_input_ = Input((None,))\n",
    "    quesw_input_ = Input((None,))\n",
    "    contc_input_ = Input((None, char_limit))\n",
    "    quesc_input_ = Input((None, char_limit))\n",
    "\n",
    "    # get mask\n",
    "    c_mask = Lambda(lambda x: tf.cast(x, tf.bool))(contw_input_)  # [bs, c_len]\n",
    "    q_mask = Lambda(lambda x: tf.cast(x, tf.bool))(quesw_input_)\n",
    "    cont_len = Lambda(lambda x: tf.expand_dims(tf.reduce_sum(tf.cast(x, tf.int32), axis=1), axis=1))(c_mask)\n",
    "    ques_len = Lambda(lambda x: tf.expand_dims(tf.reduce_sum(tf.cast(x, tf.int32), axis=1), axis=1))(q_mask)\n",
    "\n",
    "    # slice\n",
    "    contw_input = BatchSlice(dim=2)([contw_input_, cont_len])\n",
    "    quesw_input = BatchSlice(dim=2)([quesw_input_, ques_len])\n",
    "    contc_input = BatchSlice(dim=3)([contc_input_, cont_len])\n",
    "    quesc_input = BatchSlice(dim=3)([quesc_input_, ques_len])\n",
    "    c_mask = BatchSlice(dim=2)([c_mask, cont_len])\n",
    "    q_mask = BatchSlice(dim=2)([q_mask, ques_len])\n",
    "    c_maxlen = tf.cast(tf.reduce_max(cont_len), tf.int32)\n",
    "    q_maxlen = tf.cast(tf.reduce_max(ques_len), tf.int32)\n",
    "\n",
    "    # embedding word\n",
    "    WordEmbedding = Embedding(word_mat.shape[0], word_dim, weights=[word_mat], trainable=False, name='word_embedding')\n",
    "    xw_cont = WordEmbedding(contw_input)\n",
    "    xw_ques = WordEmbedding(quesw_input)\n",
    "\n",
    "    # cove\n",
    "    if cove_model is not None:\n",
    "        x_cont_cove = cove_model(xw_cont)\n",
    "        x_ques_cove = cove_model(xw_ques)\n",
    "        xw_cont = Concatenate()([xw_cont, x_cont_cove])\n",
    "        xw_ques = Concatenate()([xw_ques, x_ques_cove])\n",
    "\n",
    "    # embedding char\n",
    "    CharEmbedding = Embedding(char_mat.shape[0], char_dim, weights=[char_mat], name='char_embedding')\n",
    "    xc_cont = CharEmbedding(contc_input)\n",
    "    xc_ques = CharEmbedding(quesc_input)\n",
    "    char_conv = Conv1D(filters, 5,\n",
    "                       activation='relu',\n",
    "                       kernel_initializer=init_relu,\n",
    "                       kernel_regularizer=regularizer,\n",
    "                       name='char_conv')\n",
    "    xc_cont = Lambda(lambda x: tf.reshape(x, (-1, char_limit, char_dim)))(xc_cont)\n",
    "    xc_ques = Lambda(lambda x: tf.reshape(x, (-1, char_limit, char_dim)))(xc_ques)\n",
    "    xc_cont = char_conv(xc_cont)\n",
    "    xc_ques = char_conv(xc_ques)\n",
    "    xc_cont = GlobalMaxPooling1D()(xc_cont)\n",
    "    xc_ques = GlobalMaxPooling1D()(xc_ques)\n",
    "    xc_cont = Lambda(lambda x: tf.reshape(x, (-1, c_maxlen, filters)))(xc_cont)\n",
    "    xc_ques = Lambda(lambda x: tf.reshape(x, (-1, q_maxlen, filters)))(xc_ques)\n",
    "\n",
    "    # highwayNet\n",
    "    x_cont = Concatenate()([xw_cont, xc_cont])\n",
    "    x_ques = Concatenate()([xw_ques, xc_ques])\n",
    "\n",
    "    # highway shared layers\n",
    "    highway_layers = [Conv1D(filters, 1,\n",
    "                             kernel_initializer=init,\n",
    "                             kernel_regularizer=regularizer,\n",
    "                             name='highway_input_projection')]\n",
    "    for i in range(2):\n",
    "        highway_layers.append(Conv1D(filters, 1,\n",
    "                                     kernel_initializer=init,\n",
    "                                     kernel_regularizer=regularizer,\n",
    "                                     activation='sigmoid',\n",
    "                                     name='highway' + str(i) + '_gate'))\n",
    "        highway_layers.append(Conv1D(filters, 1,\n",
    "                                     kernel_initializer=init,\n",
    "                                     kernel_regularizer=regularizer,\n",
    "                                     activation='linear',\n",
    "                                     name='highway' + str(i) + '_linear'))\n",
    "    x_cont = highway(highway_layers, x_cont, num_layers=2, dropout=dropout)\n",
    "    x_ques = highway(highway_layers, x_ques, num_layers=2, dropout=dropout)\n",
    "\n",
    "    # build shared layers\n",
    "    # shared convs\n",
    "    Encoder_DepthwiseConv1 = []\n",
    "    for i in range(4):\n",
    "        Encoder_DepthwiseConv1.append(DepthwiseConv1D(7, filters))\n",
    "\n",
    "    # shared attention\n",
    "    Encoder_SelfAttention1 = [Conv1D(2 * filters, 1,\n",
    "                                     kernel_initializer=init,\n",
    "                                     kernel_regularizer=regularizer),\n",
    "                              Conv1D(filters, 1,\n",
    "                                     kernel_initializer=init,\n",
    "                                     kernel_regularizer=regularizer),\n",
    "                              MultiHeadAttention(filters, num_head, dropout=dropout, bias=False)]\n",
    "    # shared feed-forward\n",
    "    Encoder_FeedForward1 = []\n",
    "    Encoder_FeedForward1.append(Conv1D(filters, 1,\n",
    "                                       kernel_initializer=init,\n",
    "                                       kernel_regularizer=regularizer,\n",
    "                                       activation='relu'))\n",
    "    Encoder_FeedForward1.append(Conv1D(filters, 1,\n",
    "                                       kernel_initializer=init,\n",
    "                                       kernel_regularizer=regularizer,\n",
    "                                       activation='linear'))\n",
    "\n",
    "    # Context Embedding Encoder Layer\n",
    "    x_cont = PositionEmbedding()(x_cont)\n",
    "    x_cont = conv_block(Encoder_DepthwiseConv1, x_cont, 4, dropout)\n",
    "    x_cont = attention_block(Encoder_SelfAttention1, x_cont, c_mask, dropout)\n",
    "    x_cont = feed_forward_block(Encoder_FeedForward1, x_cont, dropout)\n",
    "\n",
    "    # Question Embedding Encoder Layer\n",
    "    x_ques = PositionEmbedding()(x_ques)\n",
    "    x_ques = conv_block(Encoder_DepthwiseConv1, x_ques, 4, dropout)\n",
    "    x_ques = attention_block(Encoder_SelfAttention1, x_ques, q_mask, dropout)\n",
    "    x_ques = feed_forward_block(Encoder_FeedForward1, x_ques, dropout)\n",
    "    \n",
    "    print('x_cont={}\\n  x_ques={}\\n  c_mask={}\\n  q_mask={}\\n'.format(x_cont, x_ques, c_mask, q_mask))\n",
    "\n",
    "    # Context_to_Query_Attention_Layer\n",
    "    ##512, c_maxlen, q_maxlen, dropout初始化该层的类，输入为[x_cont, x_ques, c_mask, q_mask]\n",
    "    x = context2query_attention(512, c_maxlen, q_maxlen, dropout)([x_cont, x_ques, c_mask, q_mask])\n",
    "    \n",
    "    print('Context_to_Query_Attention_Layer x',x)\n",
    "    x = Conv1D(filters, 1,\n",
    "               kernel_initializer=init,\n",
    "               kernel_regularizer=regularizer,\n",
    "               activation='linear')(x)\n",
    "\n",
    "    print('conv1d x',x)\n",
    "    # Model_Encoder_Layer\n",
    "    # shared layers\n",
    "    Encoder_DepthwiseConv2 = []\n",
    "    Encoder_SelfAttention2 = []\n",
    "    Encoder_FeedForward2 = []\n",
    "    for i in range(7):\n",
    "        DepthwiseConv_share_2_temp = []\n",
    "        for i in range(2):\n",
    "            DepthwiseConv_share_2_temp.append(DepthwiseConv1D(5, filters))\n",
    "\n",
    "        Encoder_DepthwiseConv2.append(DepthwiseConv_share_2_temp)\n",
    "        Encoder_SelfAttention2.append([Conv1D(2 * filters, 1,\n",
    "                                              kernel_initializer=init,\n",
    "                                              kernel_regularizer=regularizer),\n",
    "                                       Conv1D(filters, 1,\n",
    "                                              kernel_initializer=init,\n",
    "                                              kernel_regularizer=regularizer),\n",
    "                                       MultiHeadAttention(filters, num_head, dropout=dropout, bias=False)])\n",
    "        Encoder_FeedForward2.append([Conv1D(filters, 1,\n",
    "                                            kernel_initializer=init,\n",
    "                                            kernel_regularizer=regularizer,\n",
    "                                            activation='relu'),\n",
    "                                     Conv1D(filters, 1,\n",
    "                                            kernel_initializer=init,\n",
    "                                            kernel_regularizer=regularizer,\n",
    "                                            activation='linear')])\n",
    "\n",
    "    outputs = [x]\n",
    "    for i in range(3):\n",
    "        x = outputs[-1]\n",
    "        for j in range(7):\n",
    "            x = PositionEmbedding()(x)\n",
    "            x = conv_block(Encoder_DepthwiseConv2[j], x, 2, dropout, l=j, L=7)\n",
    "            x = attention_block(Encoder_SelfAttention2[j], x, c_mask, dropout, l=j, L=7)\n",
    "            x = feed_forward_block(Encoder_FeedForward2[j], x, dropout, l=j, L=7)\n",
    "        outputs.append(x)\n",
    "     \n",
    "    print('outputs',outputs)\n",
    "    # Output_Layer\n",
    "    x_start = Concatenate()([outputs[1], outputs[2]])\n",
    "    print('output_layer x_start',x_start)\n",
    "    '''\n",
    "    keras.layers.Conv1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)\n",
    "    \n",
    "    Input shape:\n",
    "      3D tensor with shape: `(batch_size, time_steps, input_dim)`\n",
    "\n",
    "  Output shape:\n",
    "      3D tensor with shape: `(batch_size, new_steps, filters)`\n",
    "      `steps` value might have changed due to padding or strides.\n",
    "\n",
    "这也可以解释，为什么在Keras中使用Conv1D可以进行自然语言处理，因为在自然语言处理中，我们假设一个序列是600个单词，每个单词的词向量是300维，那么一个序列输入到网络中就是（600,300），当我使用Conv1D进行卷积的时候，实际上就完成了直接在序列上的卷积，卷积的时候实际是以（3,300）进行卷积，又因为每一行都是一个词向量，因此使用Conv1D（kernel_size=3）也就相当于使用神经网络进行了n_gram=3的特征提取了。这也是为什么使用卷积神经网络处理文本会非常快速有效的内涵。\n",
    "\n",
    "Conv1D（kernel_size=3）实际就是Conv2D（kernel_size=（3,300）），当然必须把输入也reshape成（600,300,1），即可在多行上进行Conv2D卷积。\n",
    "所以这里的kernel_size=1，是conv2d的（1，词向量维度）\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    x_start = Conv1D(1, 1,\n",
    "                     kernel_initializer=init,\n",
    "                     kernel_regularizer=regularizer,\n",
    "                     activation='linear')(x_start)\n",
    "    print('conv1D x_start',x_start)\n",
    "    \n",
    "    #从tensor中删除所有大小是1的维度\n",
    "    x_start = Lambda(lambda x: tf.squeeze(x, axis=-1))(x_start)\n",
    "    print('squeeze x_start',x_start)\n",
    "    \n",
    "    \n",
    "    ## mask_logits输出维度与输入维度一样\n",
    "    x_start = Lambda(lambda x: mask_logits(x[0], x[1]))([x_start, c_mask])\n",
    "    print('mask_logits x_start',x_start)\n",
    "    \n",
    "    ##输出的x_start是已经经过了softmax计算之后的值\n",
    "    \n",
    "    '''\n",
    "    softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)\n",
    "    Returns:\n",
    "    A `Tensor`. Has the same type and shape as `logits`.\n",
    "    '''\n",
    "    x_start = Lambda(lambda x: K.softmax(x), name='start')(x_start)  # [bs, len]\n",
    "    print('x_start softmax',x_start,)\n",
    "\n",
    "    x_end = Concatenate()([outputs[1], outputs[3]])\n",
    "    x_end = Conv1D(1, 1,\n",
    "                   kernel_initializer=init,\n",
    "                   kernel_regularizer=regularizer,\n",
    "                   activation='linear')(x_end)\n",
    "    x_end = Lambda(lambda x: tf.squeeze(x, axis=-1))(x_end)\n",
    "    x_end = Lambda(lambda x: mask_logits(x[0], x[1]))([x_end, c_mask])\n",
    "    x_end = Lambda(lambda x: K.softmax(x), name='end')(x_end)  # [bs, len]\n",
    "\n",
    "    x_start_fin, x_end_fin = QAoutputBlock(ans_limit, name='qa_output')([x_start, x_end])\n",
    "\n",
    "    # if use model.fit, the output shape must be padded to the max length\n",
    "    x_start = LabelPadding(cont_limit, name='start_pos')(x_start)\n",
    "    x_end = LabelPadding(cont_limit, name='end_pos')(x_end)\n",
    "    print('x_start  x_start_fin x_end x_end_fin ',x_start,x_start_fin,x_end,x_end_fin)\n",
    "    return Model(inputs=[contw_input_, quesw_input_, contc_input_, quesc_input_],\n",
    "                 outputs=[x_start, x_end, x_start_fin, x_end_fin])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = QANet(config, word_mat=embedding_matrix, char_mat=embedding_matrix_char)\n",
    "model.summary()\n",
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.8, beta_2=0.999, epsilon=1e-7)\n",
    "\n",
    "##损失函数有4个应该是模型有4个输出，对应4个label,计算每个输出的损失函数，加权求和后最为最终的损失函数，权重即为loss_weights\n",
    "model.compile(optimizer=optimizer, loss=['categorical_crossentropy', 'categorical_crossentropy', 'mae', 'mae'],\n",
    "              loss_weights=[1, 1, 0, 0])\n",
    "\n",
    "# load data\n",
    "char_dim = 200\n",
    "cont_limit = 400\n",
    "ques_limit = 50\n",
    "char_limit = 16\n",
    "#生成维度为（300，cont_limit）,大小在0-10000之间的随机整数##上下文长度最大400个词，每个词的维度是300d（感觉不对，应该是有300个上下文）\n",
    "context_word = np.random.randint(0, 10000, (300, cont_limit))\n",
    "question_word = np.random.randint(0, 10000, (300, ques_limit))\n",
    "\n",
    "##最多400个词，每个词最多16个字符，字符维度也是300维度\n",
    "context_char = np.random.randint(0, 96, (300, cont_limit, char_limit))\n",
    "question_char = np.random.randint(0, 96, (300, ques_limit, char_limit))\n",
    "\n",
    "start_label = np.random.randint(0, 2, (300, cont_limit))\n",
    "end_label = np.random.randint(0, 2, (300, cont_limit))\n",
    "start_label_fin = np.argmax(start_label, axis=-1)\n",
    "end_label_fin = np.argmax(end_label, axis=-1)\n",
    "'''\n",
    "fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, \n",
    "validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, \n",
    "sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1)\n",
    "\n",
    "即\n",
    "x=[context_word, question_word, context_char, question_char],\n",
    "y=[start_label, end_label, start_label_fin, end_label_fin]\n",
    "\n",
    "Model(inputs=[contw_input_, quesw_input_, contc_input_, quesc_input_],\n",
    "                 outputs=[x_start, x_end, x_start_fin, x_end_fin])\n",
    "                 \n",
    "Model根据输入经过网络得到输出，输出和对应的label求出损失函数，损失函数加权后作为最终的损失函数，优化器使得最终的损失函数最小\n",
    "'''\n",
    "model.fit([context_word, question_word, context_char, question_char],\n",
    "          [start_label, end_label, start_label_fin, end_label_fin], batch_size=8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
