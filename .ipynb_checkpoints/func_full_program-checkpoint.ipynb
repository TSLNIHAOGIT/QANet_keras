{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qanet_fit_demp\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.initializers import *\n",
    "# from QANet_keras import QANet\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#关闭eager模式\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# tf.keras.backend.set_learning_phase(1)  # training\n",
    "\n",
    "#就代表生成10000行 300列的浮点数，浮点数都是从0-1中随机。\n",
    "##模拟的是预训练的词向量，生成QAnet模型，然后训练时对于某个词，该词存在该向量中，就取出该词向量进行使用，不存在就使用随机的词向量\n",
    "##所以模型中一定有embedding模块\n",
    "\n",
    "##假设总共10000个单词，每个单词300维度 \n",
    "embedding_matrix = np.random.random((10000, 300))\n",
    "##总共1233个字符，每个字符64维度\n",
    "embedding_matrix_char = np.random.random((1233, 64))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    'word_dim': 300,\n",
    "    'char_dim': 64,\n",
    "    'cont_limit': 400,\n",
    "    'ques_limit': 50,\n",
    "    'char_limit': 16,\n",
    "    'ans_limit': 30,\n",
    "    'char_input_size': 1233,\n",
    "    'filters': 128,\n",
    "    'num_head': 8,\n",
    "    'dropout': 0.5,\n",
    "    'batch_size': 16,\n",
    "    'epoch': 25,\n",
    "    'ema_decay': 0.9999,\n",
    "    'learning_rate': 1e-3,\n",
    "    'path': 'QA001',\n",
    "    'use_cove': True\n",
    "}\n",
    "\n",
    "\n",
    "# load data\n",
    "char_dim = 200##好像没有使用\n",
    "cont_limit = 400\n",
    "ques_limit = 50\n",
    "char_limit = 16\n",
    "\n",
    "###0-10000之间的某个词模拟所有词的的index\n",
    "##embedding词典大小总共10000个单词；转成0-9999个索引对应，300个上下文，每个上下文长度最大为为400\n",
    "context_word = np.random.randint(0, 10000, (300, cont_limit))\n",
    "question_word = np.random.randint(0, 10000, (300, ques_limit))\n",
    "context_char = np.random.randint(0, 96, (300, cont_limit, char_limit))\n",
    "question_char = np.random.randint(0, 96, (300, ques_limit, char_limit))\n",
    "start_label = np.random.randint(0, 2, (300, cont_limit))\n",
    "end_label = np.random.randint(0, 2, (300, cont_limit))\n",
    "start_label_fin = np.argmax(start_label, axis=-1)\n",
    "end_label_fin = np.argmax(end_label, axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor example:\\ninp = Input(shape=(maxlen,))\\n计算得到\\ninp.shape=(None, maxlen)\\nshape: A shape tuple (integer), not including the batch size. For instance,\\xa0\\nshape=(32,)indicates that the expected input will be batches of 32-dimensional vectors.\\n也就是说，不看batch_size的话，其实我们输入的是一个100维度的向量，正好对应了上面的maxlen对吧。\\n\\n那我们embedding层的作用，是将正整数下标转换为具有固定大小的向量。\\n\\n如[[4], [20]]->[[0.25,0.1], [0.6,-0.2]]。一定要注意到一个下标对应一个向量。\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#qanet_keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "# tf.keras.initializers.VarianceScaling\n",
    "# from layers.context2query_attention import context2query_attention\n",
    "\n",
    "# from layers.multihead_attention import Attention as MultiHeadAttention\n",
    "# from layers.position_embedding import Position_Embedding as PositionEmbedding\n",
    "# from layers.layer_norm import LayerNormalization\n",
    "# from layers.layer_dropout import LayerDropout\n",
    "# from layers.QAoutputBlock import QAoutputBlock\n",
    "# from layers.BatchSlice import BatchSlice\n",
    "# from layers.DepthwiseConv1D import DepthwiseConv1D\n",
    "# from layers.LabelPadding import LabelPadding\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "# parameters\n",
    "word_dim = config['word_dim']\n",
    "char_dim = config['char_dim']\n",
    "cont_limit = config['cont_limit']\n",
    "char_limit = config['char_limit']\n",
    "ans_limit = config['ans_limit']\n",
    "filters = config['filters']\n",
    "num_head = config['num_head']\n",
    "dropout = config['dropout']\n",
    "\n",
    "# Input Embedding Layer\n",
    "#`Input()` is used to instantiate a Keras tensor.S\n",
    "'''\n",
    "for example:\n",
    "inp = Input(shape=(maxlen,))\n",
    "计算得到\n",
    "inp.shape=(None, maxlen)\n",
    "shape: A shape tuple (integer), not including the batch size. For instance, \n",
    "shape=(32,)indicates that the expected input will be batches of 32-dimensional vectors.\n",
    "也就是说，不看batch_size的话，其实我们输入的是一个100维度的向量，正好对应了上面的maxlen对吧。\n",
    "\n",
    "那我们embedding层的作用，是将正整数下标转换为具有固定大小的向量。\n",
    "\n",
    "如[[4], [20]]->[[0.25,0.1], [0.6,-0.2]]。一定要注意到一个下标对应一个向量。\n",
    "\n",
    "'''\n",
    "#相当于词向量的维度是输入的None,完整的维度是(None,None)=（glove词向量中总的单词数，每个单词的维度）\n",
    "#第一个None是默认的词的个数（batch_size），第二None是自己输入的词向量的维度\n",
    "####这个不是用于词向量的，是为了获得mask矩阵\n",
    "\n",
    "\n",
    "# contw_input_ = Input((None,))## [bs, c_len]=[上下文总数,每个上下文单词数]\n",
    "# quesw_input_ = Input((None,))\n",
    "\n",
    "# #（None,char_limit）可能代表（单词个数，每个单词的字符数），总体应该是（上下文总数，每个上下文单词数，每个单词字符数）\n",
    "# contc_input_ = Input((None, char_limit))\n",
    "# quesc_input_ = Input((None, char_limit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('input test',Input((111,222)))\n",
    "# print('contw_input_ :',contw_input_ )\n",
    "# print('contc_input_:',contc_input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contw_input_ : (300, 400)\n",
      "contc_input_: (300, 400, 16)\n"
     ]
    }
   ],
   "source": [
    "###直接替换input层:调试看维度是可以这样使用，构建模型时还是要使用input层\n",
    "###模拟是最长的大小，没有进行0补的0\n",
    "'''\n",
    "cont_limit = 400\n",
    "ques_limit = 50\n",
    "char_limit = 16\n",
    "'''\n",
    "\n",
    "contw_input_= np.random.randint(0, 10000, (300, cont_limit))\n",
    "quesw_input_ = np.random.randint(0, 10000, (300, ques_limit))\n",
    "contc_input_  = np.random.randint(0, 96, (300, cont_limit, char_limit))\n",
    "quesc_input_ = np.random.randint(0, 96, (300, ques_limit, char_limit))\n",
    "\n",
    "print('contw_input_ :',contw_input_.shape )\n",
    "print('contc_input_:',contc_input_.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mask\n",
    "#里面都是词典对应的id,且短的已经用零补全了，因此能转为布尔值计算mask矩阵\n",
    "c_mask = Lambda(lambda x: tf.cast(x, tf.bool))(contw_input_)  # [bs, c_len]\n",
    "q_mask = Lambda(lambda x: tf.cast(x, tf.bool))(quesw_input_)\n",
    "#reduce_sum默认是对所有元素求和\n",
    "\n",
    "##axis=1是对行求和,就是求每个context的长度\n",
    "cont_len = Lambda(lambda x: tf.expand_dims(tf.reduce_sum(tf.cast(x, tf.int32), axis=1), axis=1))(c_mask)\n",
    "ques_len = Lambda(lambda x: tf.expand_dims(tf.reduce_sum(tf.cast(x, tf.int32), axis=1), axis=1))(q_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_mask (300, 400)\n",
      "ques_len (300, 1)\n",
      "cont_len (300, 1)\n"
     ]
    }
   ],
   "source": [
    "# c_mask0=[[1,1,1,1,0],[1,1,1,0,0]]\n",
    "\n",
    "# print('tf.reduce_sum(tf.cast(x, tf.int32):',tf.reduce_sum(tf.cast(c_mask0, tf.int32),axis=1))\n",
    "# print('expand_dims',tf.expand_dims(tf.reduce_sum(tf.cast(c_mask0, tf.int32),axis=1),axis=1))\n",
    "print('c_mask',c_mask.shape)\n",
    "print('ques_len',ques_len.shape)\n",
    "print('cont_len',cont_len.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer \n",
    "import tensorflow as tf\n",
    "\n",
    "class BatchSlice(Layer):\n",
    "    def __init__(self, dim=2, **kwargs):\n",
    "        self.dim = dim\n",
    "        super(BatchSlice, self).__init__(**kwargs)\n",
    "    '''\n",
    "    build(input_shape): 这是你定义权重的地方。这个方法必须设 self.built = True，可以通过调用 super([Layer], self).build() 完成\n",
    "    '''\n",
    "    def build(self, input_shape):\n",
    "        super(BatchSlice, self).build(input_shape)\n",
    "    '''\n",
    "    call(x): 这里是编写层的功能逻辑的地方。你只需要关注传入 call 的第一个参数：输入张量，除非你希望你的层支持masking。\n",
    "    '''\n",
    "    #eg input=[contw_input_, cont_len]\n",
    "    def call(self, x, mask=None):\n",
    "        \n",
    "        x, length = x # [bs, len, dim]\n",
    "        ##选取上下文长度中最长的一个，作为最终的长度\n",
    "        length = tf.cast(tf.reduce_max(length), tf.int32)\n",
    "        st = [0] * self.dim\n",
    "        ed = [-1] * self.dim\n",
    "        ed[1] = length\n",
    "        \n",
    "        #从开始位置x[st]抽取元素，各个维度对应的元素个数为ed\n",
    "        #此处 x=[batch_size,each_context_length]\n",
    "         #    st=[0,0]\n",
    "        #   ed=[-1,max_len_context]\n",
    "        \n",
    "        #即从第0个元素开始，抽取每个上下文都\n",
    "        x = tf.slice(x, st, ed)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qanet_keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "# tf.keras.initializers.VarianceScaling\n",
    "# from layers.context2query_attention import context2query_attention\n",
    "\n",
    "# from layers.multihead_attention import Attention as MultiHeadAttention\n",
    "# from layers.position_embedding import Position_Embedding as PositionEmbedding\n",
    "# from layers.layer_norm import LayerNormalization\n",
    "# from layers.layer_dropout import LayerDropout\n",
    "# from layers.QAoutputBlock import QAoutputBlock\n",
    "# from layers.BatchSlice import BatchSlice\n",
    "# from layers.DepthwiseConv1D import DepthwiseConv1D\n",
    "# from layers.LabelPadding import LabelPadding\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice\n",
    "##contw_input_:（上下文个数，每个上下文词个数） cont_len：（所有上下文长度，1）\n",
    "##暂时还不知道切片的目的是什么，理论上应该是输入的文本长度太长时，要截取一部分，太短时要补全一部分\n",
    "##但是输入时已经是一个矩阵了，意思是已经进行了截取或者补全之后的矩阵了，再次截取时只会再截取一部分数据过来，丢掉一部分数据\n",
    "\n",
    "\n",
    "##dim=2或者3都是从头开始截取，对context的长度进行截断（没有对word长度截断）\n",
    "contw_input = BatchSlice(dim=2)([contw_input_, cont_len])\n",
    "quesw_input = BatchSlice(dim=2)([quesw_input_, ques_len])\n",
    "contc_input = BatchSlice(dim=3)([contc_input_, cont_len])\n",
    "quesc_input = BatchSlice(dim=3)([quesc_input_, ques_len])\n",
    "\n",
    "##对mask进行截取构成新的mask矩阵\n",
    "c_mask = BatchSlice(dim=2)([c_mask, cont_len])\n",
    "q_mask = BatchSlice(dim=2)([q_mask, ques_len])\n",
    "\n",
    "##上下文和问题的最大长度\n",
    "c_maxlen = tf.cast(tf.reduce_max(cont_len), tf.int32)\n",
    "q_maxlen = tf.cast(tf.reduce_max(ques_len), tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[contw_input_, cont_len] [array([[7529, 2545, 4427, ..., 6192, 2989, 2766],\n",
      "       [9153, 9191, 2235, ..., 8960, 7030, 9662],\n",
      "       [5502, 6430, 1130, ..., 7490, 5800,  340],\n",
      "       ...,\n",
      "       [5975, 5497, 1172, ..., 3680, 2252,  393],\n",
      "       [3071, 7983, 4220, ..., 1393, 2928, 1723],\n",
      "       [2675, 5402, 5702, ..., 6392,  969, 3272]]), <tf.Tensor: id=9, shape=(300, 1), dtype=int32, numpy=\n",
      "array([[400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [399],\n",
      "       [399],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [399],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [399],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [399],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [399],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [398],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [399],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [399],\n",
      "       [400],\n",
      "       [400],\n",
      "       [399],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [398],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [399],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [399],\n",
      "       [399],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400],\n",
      "       [400]], dtype=int32)>]\n"
     ]
    }
   ],
   "source": [
    "print('[contw_input_, cont_len]',[contw_input_, cont_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " # embedding word\n",
    "word_mat=embedding_matrix\n",
    "char_mat=embedding_matrix_char\n",
    "\n",
    "#第一个参数词汇表大小，第二个参数是词向量维度，weights是embedding层权重初始化的一种方式：此处就是用word_mat来初始化embedding层\n",
    "WordEmbedding = Embedding(word_mat.shape[0], word_dim, weights=[word_mat], trainable=False, name='word_embedding')\n",
    "#根据词汇表索引去取相应的embedding值\n",
    "xw_cont = WordEmbedding(contw_input)\n",
    "xw_ques = WordEmbedding(quesw_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordEmbedding <tensorflow.python.keras.layers.embeddings.Embedding object at 0x7fd7841a23c8>\n",
      "xw_cont (300, 400, 300)\n",
      "xw_ques (300, 50, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nxw_cont (300, 400, 300)=(batch_size,content_length,word_embedding_size)\\nxw_ques (300, 50, 300)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('WordEmbedding',WordEmbedding)\n",
    "print('xw_cont',xw_cont.shape)\n",
    "print('xw_ques',xw_ques.shape)\n",
    "'''\n",
    "xw_cont (300, 400, 300)=(batch_size,content_length,word_embedding_size)\n",
    "xw_ques (300, 50, 300)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cove_model=None\n",
    "# cove##暂时没有看这个源代码\n",
    "if cove_model is not None:\n",
    "    x_cont_cove = cove_model(xw_cont)\n",
    "    x_ques_cove = cove_model(xw_ques)\n",
    "    xw_cont = Concatenate()([xw_cont, x_cont_cove])\n",
    "    xw_ques = Concatenate()([xw_ques, x_ques_cove])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding char\n",
    "CharEmbedding = Embedding(char_mat.shape[0], char_dim, weights=[char_mat], name='char_embedding')\n",
    "xc_cont = CharEmbedding(contc_input)\n",
    "xc_ques = CharEmbedding(quesc_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xc_cont (300, 400, 16, 64)\n",
      "xc_ques (300, 50, 16, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nxc_cont (300, 400, 16, 64)=(batch_size,content_length,word_length,char_embedding_size)\\nxc_ques (300, 50, 16, 64)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('xc_cont',xc_cont.shape)\n",
    "print('xc_ques',xc_ques.shape)\n",
    "'''\n",
    "xc_cont (300, 400, 16, 64)=(batch_size,content_length,word_length,char_embedding_size)\n",
    "xc_ques (300, 50, 16, 64)\n",
    "##字符embedding是随机初始化后进行训练的\n",
    "\n",
    "经过reshape, Conv1D(filters, 5）变为：\n",
    "#xc_context将前两个维度合并为300*400=120000个词汇（含重复的）\n",
    "#(300*400,16,64)然后送入conv1d卷积层:输出为（batch_size=120000,new_step=16-5+1,filters=128）\n",
    "\n",
    "经过对第二个维度如time_step进行最大池化，那么对所有step第一个元素取最大值，第二个元素取最大值等等\n",
    "##与bert分类时池化类似，那里是去第一个，这是最大的一个  \n",
    "##xc_cont = GlobalMaxPooling1D()(xc_cont)；变为xc_cont (120000, 128) xc_ques (15000, 128)\n",
    "\n",
    "经过reshape之后变为：xc_cont (300, 400, 128)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizer = l2(3e-7)\n",
    "init = VarianceScaling(scale=1.0, mode='fan_avg', distribution='uniform')\n",
    "init_relu = VarianceScaling(scale=2.0, mode='fan_in', distribution='normal')\n",
    "\n",
    "char_conv = Conv1D(filters, 5,\n",
    "                   activation='relu',\n",
    "                   kernel_initializer=init_relu,\n",
    "                   kernel_regularizer=regularizer,\n",
    "                   name='char_conv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xc_context将前两个维度合并为300*400=120000个词汇（含重复的）\n",
    "#(300*400,16,64)然后送入conv1d卷积层:输出为（batch_size=120000,new_step=16-5+1,filters=128）\n",
    "#一维卷积中kernl_size=5卷积核大小为5，但是移动幅度默认为1,16-5=11，\n",
    "#下面移动11词，加上第一次的总共12次；#filters卷积核个数，最终将每个卷积核的结果拼接到一起；\n",
    "xc_cont = Lambda(lambda x: tf.reshape(x, (-1, char_limit, char_dim)))(xc_cont)\n",
    "xc_ques = Lambda(lambda x: tf.reshape(x, (-1, char_limit, char_dim)))(xc_ques)\n",
    "xc_cont = char_conv(xc_cont)\n",
    "xc_ques = char_conv(xc_ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xc_cont (120000, 12, 128)\n",
      "xc_ques (15000, 12, 128)\n"
     ]
    }
   ],
   "source": [
    "print('xc_cont',xc_cont.shape)\n",
    "print('xc_ques',xc_ques.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##对第二个维度如time_step进行最大池化，那么对所有step第一个元素取最大值，第二个元素取最大值等等\n",
    "xc_cont = GlobalMaxPooling1D()(xc_cont)\n",
    "xc_ques = GlobalMaxPooling1D()(xc_ques)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 11  12  13]\n",
      " [111 122 133]], shape=(2, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x_t=[\n",
    "    [[11,0,3],[0,12,8],[9,7,13]],\n",
    "    [[111,0,3],[0,122,8],[9,7,133]],\n",
    "]\n",
    "x_t=tf.constant(x_t)\n",
    "print(GlobalMaxPooling1D()(x_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xc_cont (120000, 128)\n",
      "xc_ques (15000, 128)\n"
     ]
    }
   ],
   "source": [
    "print('xc_cont',xc_cont.shape)\n",
    "print('xc_ques',xc_ques.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape\n",
    "xc_cont = Lambda(lambda x: tf.reshape(x, (-1, c_maxlen, filters)))(xc_cont)\n",
    "xc_ques = Lambda(lambda x: tf.reshape(x, (-1, q_maxlen, filters)))(xc_ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xc_cont (300, 400, 128)\n",
      "xc_ques (300, 50, 128)\n"
     ]
    }
   ],
   "source": [
    "print('xc_cont',xc_cont.shape)\n",
    "print('xc_ques',xc_ques.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# highwayNet\n",
    "# (300,400,300) (300,400,128)\n",
    "##concatenate 默认axis=-1，即沿着最后一轴进行拼接\n",
    "x_cont = Concatenate()([xw_cont, xc_cont])\n",
    "x_ques = Concatenate()([xw_ques, xc_ques])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_cont (300, 400, 428)\n",
      "x_ques (300, 50, 428)\n"
     ]
    }
   ],
   "source": [
    "print('x_cont',x_cont.shape)\n",
    "print('x_ques',x_ques.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highway(highway_layers, x, num_layers=2, dropout=0.1):\n",
    "    # reduce dim\n",
    "    x = highway_layers[0](x)\n",
    "    for i in range(num_layers):\n",
    "        T = highway_layers[i * 2 + 1](x)\n",
    "        H = highway_layers[i * 2 + 2](x)\n",
    "        H = Dropout(dropout)(H)\n",
    "        # H*T+x*(1-T)\n",
    "        x = Lambda(lambda v: v[0] * v[1] + v[2] * (1 - v[1]))([H, T, x])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filters=128\n",
    "# highway shared layers\n",
    "highway_layers = [Conv1D(filters, 1,\n",
    "                         kernel_initializer=init,\n",
    "                         kernel_regularizer=regularizer,\n",
    "                         name='highway_input_projection')]\n",
    "for i in range(2):\n",
    "    highway_layers.append(Conv1D(filters, 1,\n",
    "                                 kernel_initializer=init,\n",
    "                                 kernel_regularizer=regularizer,\n",
    "                                 activation='sigmoid',\n",
    "                                 name='highway' + str(i) + '_gate'))\n",
    "    highway_layers.append(Conv1D(filters, 1,\n",
    "                                 kernel_initializer=init,\n",
    "                                 kernel_regularizer=regularizer,\n",
    "                                 activation='linear',\n",
    "                                 name='highway' + str(i) + '_linear'))\n",
    "x_cont = highway(highway_layers, x_cont, num_layers=2, dropout=dropout)\n",
    "x_ques = highway(highway_layers, x_ques, num_layers=2, dropout=dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_cont (300, 400, 128)\n",
      "x_ques (300, 50, 128)\n"
     ]
    }
   ],
   "source": [
    "print('x_cont',x_cont.shape)\n",
    "print('x_ques',x_ques.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseConv1D(Layer):\n",
    "\n",
    "    def __init__(self, kernel_size, filter, **kwargs):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.filter = filter\n",
    "        super(DepthwiseConv1D, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        init_relu = VarianceScaling(scale=2.0, mode='fan_in', distribution='normal')\n",
    "        self.depthwise_w = self.add_weight(\"depthwise_filter\",\n",
    "                                           shape=(self.kernel_size, 1, input_shape[-1], 1),\n",
    "                                           initializer=init_relu,\n",
    "                                           regularizer=l2(3e-7),\n",
    "                                           trainable=True)\n",
    "        self.pointwise_w = self.add_weight(\"pointwise_filter\",\n",
    "                                           (1, 1, input_shape[-1], self.filter),\n",
    "                                           initializer=init_relu,\n",
    "                                           regularizer=l2(3e-7),\n",
    "                                           trainable=True)\n",
    "        self.bias = self.add_weight(\"bias\",\n",
    "                                    input_shape[-1],\n",
    "                                    regularizer=l2(3e-7),\n",
    "                                    initializer=tf.zeros_initializer())\n",
    "        super(DepthwiseConv1D, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        x = K.expand_dims(x, axis=2)\n",
    "        x = tf.nn.separable_conv2d(x,\n",
    "                                   self.depthwise_w,\n",
    "                                   self.pointwise_w,\n",
    "                                   strides=(1, 1, 1, 1),\n",
    "                                   padding=\"SAME\")\n",
    "        x += self.bias\n",
    "        x = K.relu(x)\n",
    "        outputs = K.squeeze(x, axis=2)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, units, num_heads, dropout=0.1, bias=True, **kwargs):\n",
    "        self.units = units\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(name='bias',\n",
    "                                     shape=([1]),\n",
    "                                     initializer='zero')\n",
    "        super(MultiHeadAttention, self).build(input_shape)\n",
    "\n",
    "    def split_last_dimension(self, x, n):\n",
    "        old_shape = x.get_shape().dims\n",
    "        last = old_shape[-1]\n",
    "        new_shape = old_shape[:-1] + [n] + [last // n if last else None]\n",
    "        ret = tf.reshape(x, tf.concat([tf.shape(x)[:-1], [n, -1]], 0))\n",
    "        ret.set_shape(new_shape)\n",
    "        return tf.transpose(ret, [0, 2, 1, 3])\n",
    "\n",
    "    def mask_logits(self, inputs, mask, mask_value=-1e30):\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        return inputs + mask_value * (1 - mask)\n",
    "\n",
    "    def dot_product_attention(self, x, mask=None, dropout=0.1, training=None):\n",
    "        q, k, v = x\n",
    "        logits = tf.matmul(q, k, transpose_b=True)  # [bs, 8, len, len]\n",
    "        if self.bias:\n",
    "            logits += self.b\n",
    "        if mask is not None:  # [bs, len]\n",
    "            mask = tf.expand_dims(mask, axis=1)\n",
    "            mask = tf.expand_dims(mask, axis=1)  # [bs,1,1,len]\n",
    "            logits = self.mask_logits(logits, mask)\n",
    "        weights = tf.nn.softmax(logits, name=\"attention_weights\")\n",
    "        weights = K.in_train_phase(K.dropout(weights, dropout), weights, training=training)\n",
    "        x = tf.matmul(weights, v)\n",
    "        return x\n",
    "\n",
    "    def combine_last_two_dimensions(self, x):\n",
    "        old_shape = x.get_shape().dims\n",
    "        a, b = old_shape[-2:]\n",
    "        new_shape = old_shape[:-2] + [a * b if a and b else None]\n",
    "        ret = tf.reshape(x, tf.concat([tf.shape(x)[:-2], [-1]], 0))\n",
    "        ret.set_shape(new_shape)\n",
    "        return ret\n",
    "\n",
    "    def call(self, x, mask=None, training=None):\n",
    "        memory, query, seq_mask = x\n",
    "        Q = self.split_last_dimension(query, self.num_heads)\n",
    "        memory = tf.split(memory, 2, axis=2)\n",
    "        K = self.split_last_dimension(memory[0], self.num_heads)\n",
    "        V = self.split_last_dimension(memory[1], self.num_heads)\n",
    "\n",
    "        key_depth_per_head = self.units // self.num_heads\n",
    "        Q *= (key_depth_per_head ** -0.5)\n",
    "        x = self.dot_product_attention([Q, K, V], seq_mask, dropout=self.dropout, training=training)\n",
    "        x = self.combine_last_two_dimensions(tf.transpose(x, [0, 2, 1, 3]))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build shared layers\n",
    "# shared convs\n",
    "Encoder_DepthwiseConv1 = []\n",
    "for i in range(4):\n",
    "    Encoder_DepthwiseConv1.append(DepthwiseConv1D(7, filters))\n",
    "\n",
    "# shared attention\n",
    "Encoder_SelfAttention1 = [Conv1D(2 * filters, 1,\n",
    "                                 kernel_initializer=init,\n",
    "                                 kernel_regularizer=regularizer),\n",
    "                          Conv1D(filters, 1,\n",
    "                                 kernel_initializer=init,\n",
    "                                 kernel_regularizer=regularizer),\n",
    "                          MultiHeadAttention(filters, num_head, dropout=dropout, bias=False)]\n",
    "# shared feed-forward\n",
    "Encoder_FeedForward1 = []\n",
    "Encoder_FeedForward1.append(Conv1D(filters, 1,\n",
    "                                   kernel_initializer=init,\n",
    "                                   kernel_regularizer=regularizer,\n",
    "                                   activation='relu'))\n",
    "Encoder_FeedForward1.append(Conv1D(filters, 1,\n",
    "                                   kernel_initializer=init,\n",
    "                                   kernel_regularizer=regularizer,\n",
    "                                   activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Position_Embedding(Layer):\n",
    "    def __init__(self, min_timescale=1.0, max_timescale=1.0e4, **kwargs):\n",
    "        self.min_timescale = min_timescale\n",
    "        self.max_timescale = max_timescale\n",
    "        super(Position_Embedding, self).__init__(**kwargs)\n",
    "\n",
    "    def get_timing_signal_1d(self, length, channels):\n",
    "        position=tf.cast(tf.range(length),dtype=tf.float32)\n",
    "        num_timescales = channels // 2\n",
    "        log_timescale_increment = (math.log(float(self.max_timescale) / float(self.min_timescale)) / (tf.cast(num_timescales,dtype=tf.float32) - 1))\n",
    "        inv_timescales = self.min_timescale * tf.exp(tf.cast(tf.range(num_timescales),dtype=tf.float32) * -log_timescale_increment)\n",
    "        scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0)\n",
    "        signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n",
    "        signal = tf.pad(signal, [[0, 0], [0, tf.math.mod(channels, 2)]])\n",
    "        signal = tf.reshape(signal, [1, length, channels])\n",
    "        return signal\n",
    "\n",
    "    def add_timing_signal_1d(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        channels = tf.shape(x)[2]\n",
    "        signal = self.get_timing_signal_1d(length, channels)\n",
    "        return x + signal\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return self.add_timing_signal_1d(x)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "PositionEmbedding=Position_Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class LayerDropout(Layer):\n",
    "    def __init__(self, dropout = 0.1, **kwargs):\n",
    "        self.dropout = dropout\n",
    "        super(LayerDropout, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(LayerDropout, self).build(input_shape)\n",
    "    #0.2 should be self.dropout，但这里总是出错，就先改成0.2\n",
    "    def call(self, x, mask=None, training=None):\n",
    "        x, residual = x\n",
    "        pred = tf.random.uniform([]) < self.dropout\n",
    "        #print('self.dropout',self.dropout)\n",
    "        x_train = tf.cond(pred, lambda: residual, lambda: tf.nn.dropout(x, 1.0 -0.2 ) + residual)\n",
    "        x_test = x + residual\n",
    "        return K.in_train_phase(x_train, x_test, training=training)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(conv_layers, x, num_conv=4, dropout=0.1, l=1., L=1.):\n",
    "    for i in range(num_conv):\n",
    "        residual = x\n",
    "        x = LayerNormalization()(x)\n",
    "        if i % 2 == 0:\n",
    "            x = Dropout(dropout)(x)\n",
    "        x = conv_layers[i](x)\n",
    "        x = LayerDropout(dropout * (l / L))([x, residual])\n",
    "    return x\n",
    "def attention_block(attention_layer, x, seq_mask, dropout=0.1, l=1., L=1.):\n",
    "    residual = x\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x1 = attention_layer[0](x)\n",
    "    x2 = attention_layer[1](x)\n",
    "    x = attention_layer[2]([x1, x2, seq_mask])\n",
    "    x = LayerDropout(dropout * (l / L))([x, residual])\n",
    "    return x\n",
    "\n",
    "\n",
    "def feed_forward_block(FeedForward_layers, x, dropout=0.1, l=1., L=1.):\n",
    "    residual = x\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = FeedForward_layers[0](x)\n",
    "    x = FeedForward_layers[1](x)\n",
    "    x = LayerDropout(dropout * (l / L))([x, residual])\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1105 02:07:57.446197 140563765102336 nn_ops.py:4283] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1105 02:08:01.545173 140563765102336 nn_ops.py:4283] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1105 02:08:01.569972 140563765102336 nn_ops.py:4283] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W1105 02:08:01.687632 140563765102336 nn_ops.py:4283] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "# Context Embedding Encoder Layer\n",
    "x_cont = PositionEmbedding()(x_cont)\n",
    "x_cont = conv_block(Encoder_DepthwiseConv1, x_cont, 4, dropout)\n",
    "x_cont = attention_block(Encoder_SelfAttention1, x_cont, c_mask, dropout)\n",
    "x_cont = feed_forward_block(Encoder_FeedForward1, x_cont, dropout)\n",
    "\n",
    "# Question Embedding Encoder Layer\n",
    "x_ques = PositionEmbedding()(x_ques)\n",
    "x_ques = conv_block(Encoder_DepthwiseConv1, x_ques, 4, dropout)\n",
    "x_ques = attention_block(Encoder_SelfAttention1, x_ques, q_mask, dropout)\n",
    "x_ques = feed_forward_block(Encoder_FeedForward1, x_ques, dropout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_cont=(300, 400, 128)\n",
      "  x_ques=(300, 50, 128)\n",
      "  c_mask=(300, 400)\n",
      "  q_mask=(300, 50)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('x_cont={}\\n  x_ques={}\\n  c_mask={}\\n  q_mask={}\\n'.format(x_cont.shape, x_ques.shape, c_mask.shape, q_mask.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "from tensorflow.keras.regularizers import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "class context2query_attention(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, c_maxlen, q_maxlen, dropout, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.c_maxlen = c_maxlen\n",
    "        self.q_maxlen = q_maxlen\n",
    "        self.dropout = dropout\n",
    "        super(context2query_attention, self).__init__(**kwargs)\n",
    "\n",
    "    '''\n",
    "    build(input_shape): 这是你定义权重的地方。这个方法必须设 self.built = True，可以通过调用 super([Layer], self).build() 完成\n",
    "    '''\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape: [(None, ?, 128), (None, ?, 128)]\n",
    "        init = VarianceScaling(scale=1.0, mode='fan_avg', distribution='uniform')\n",
    "        self.W0 = self.add_weight(name='W0',\n",
    "                                  shape=(input_shape[0][-1], 1),\n",
    "                                  initializer=init,\n",
    "                                  regularizer=l2(3e-7),\n",
    "                                  trainable=True)\n",
    "        self.W1 = self.add_weight(name='W1',\n",
    "                                  shape=(input_shape[1][-1], 1),\n",
    "                                  initializer=init,\n",
    "                                  regularizer=l2(3e-7),\n",
    "                                  trainable=True)\n",
    "        self.W2 = self.add_weight(name='W2',\n",
    "                                  shape=(1, 1, input_shape[0][-1]),\n",
    "                                  initializer=init,\n",
    "                                  regularizer=l2(3e-7),\n",
    "                                  trainable=True)\n",
    "        self.bias = self.add_weight(name='linear_bias',\n",
    "                                    shape=([1]),\n",
    "                                    initializer='zero',\n",
    "                                    regularizer=l2(3e-7),\n",
    "                                    trainable=True)\n",
    "        super(context2query_attention, self).build(input_shape)\n",
    "\n",
    "    def mask_logits(self, inputs, mask, mask_value=-1e30):\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        return inputs + mask_value * (1 - mask)\n",
    "    \n",
    "    '''\n",
    "    call(x): 这里是编写层的功能逻辑的地方。你只需要关注传入 call 的第一个参数：输入张量，除非你希望你的层支持masking。\n",
    "    '''\n",
    "\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        x_cont, x_ques, c_mask, q_mask = x\n",
    "        # get similarity matrix S\n",
    "        subres0 = K.tile(K.dot(x_cont, self.W0), [1, 1, self.q_maxlen])\n",
    "        subres1 = K.tile(K.permute_dimensions(K.dot(x_ques, self.W1), pattern=(0, 2, 1)), [1, self.c_maxlen, 1])\n",
    "        subres2 = K.batch_dot(x_cont * self.W2, K.permute_dimensions(x_ques, pattern=(0, 2, 1)))\n",
    "        S = subres0 + subres1 + subres2\n",
    "        S += self.bias\n",
    "        q_mask = tf.expand_dims(q_mask, 1)\n",
    "        S_ = tf.nn.softmax(self.mask_logits(S, q_mask))\n",
    "        c_mask = tf.expand_dims(c_mask, 2)\n",
    "        S_T = K.permute_dimensions(tf.nn.softmax(self.mask_logits(S, c_mask), axis=1), (0, 2, 1))\n",
    "        c2q = tf.matmul(S_, x_ques)\n",
    "        q2c = tf.matmul(tf.matmul(S_, S_T), x_cont)\n",
    "        result = K.concatenate([x_cont, c2q, x_cont * c2q, x_cont * q2c], axis=-1)\n",
    "        #result Tensor(\"context2query_attention/concat_2:0\", shape=(None, None, 512), dtype=float32)\n",
    "        print('result',result.shape)\n",
    "        return result\n",
    "\n",
    "    '''\n",
    "    compute_output_shape(input_shape): 如果你的层更改了输入张量的形状，你应该在这里定义形状变化的逻辑，这让Keras能够自动推断各层的形状\n",
    "    '''\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result (300, 400, 512)\n",
      "Context_to_Query_Attention_Layer x (300, 400, 512)\n"
     ]
    }
   ],
   "source": [
    "# Context_to_Query_Attention_Layer\n",
    "##512, c_maxlen, q_maxlen, dropout初始化该层的类，输入为[x_cont, x_ques, c_mask, q_mask]\n",
    "x = context2query_attention(512, c_maxlen, q_maxlen, dropout)([x_cont, x_ques, c_mask, q_mask])\n",
    "\n",
    "print('Context_to_Query_Attention_Layer x',x.shape)\n",
    "x = Conv1D(filters, 1,\n",
    "           kernel_initializer=init,\n",
    "           kernel_regularizer=regularizer,\n",
    "           activation='linear')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (300, 400, 128)\n"
     ]
    }
   ],
   "source": [
    "print('x',x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_Encoder_Layer\n",
    "# shared layers\n",
    "Encoder_DepthwiseConv2 = []\n",
    "Encoder_SelfAttention2 = []\n",
    "Encoder_FeedForward2 = []\n",
    "for i in range(3):#7\n",
    "    DepthwiseConv_share_2_temp = []\n",
    "    for i in range(2):\n",
    "        DepthwiseConv_share_2_temp.append(DepthwiseConv1D(5, filters))\n",
    "\n",
    "    Encoder_DepthwiseConv2.append(DepthwiseConv_share_2_temp)\n",
    "    Encoder_SelfAttention2.append([Conv1D(2 * filters, 1,\n",
    "                                          kernel_initializer=init,\n",
    "                                          kernel_regularizer=regularizer),\n",
    "                                   Conv1D(filters, 1,\n",
    "                                          kernel_initializer=init,\n",
    "                                          kernel_regularizer=regularizer),\n",
    "                                   MultiHeadAttention(filters, num_head, dropout=dropout, bias=False)])\n",
    "    Encoder_FeedForward2.append([Conv1D(filters, 1,\n",
    "                                        kernel_initializer=init,\n",
    "                                        kernel_regularizer=regularizer,\n",
    "                                        activation='relu'),\n",
    "                                 Conv1D(filters, 1,\n",
    "                                        kernel_initializer=init,\n",
    "                                        kernel_regularizer=regularizer,\n",
    "                                        activation='linear')])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1105 02:08:02.367172 140563765102336 nn_ops.py:4283] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "outputs = [x]\n",
    "for i in range(3):#3\n",
    "    x = outputs[-1]\n",
    "    for j in range(3):#7\n",
    "        x = PositionEmbedding()(x)\n",
    "        x = conv_block(Encoder_DepthwiseConv2[j], x, 2, dropout, l=j, L=7)\n",
    "        x = attention_block(Encoder_SelfAttention2[j], x, c_mask, dropout, l=j, L=7)\n",
    "        x = feed_forward_block(Encoder_FeedForward2[j], x, dropout, l=j, L=7)\n",
    "    outputs.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(outputs) 4\n",
      "shape 0 (300, 400, 128)\n",
      "shape 1 (300, 400, 128)\n",
      "shape 2 (300, 400, 128)\n",
      "shape 3 (300, 400, 128)\n"
     ]
    }
   ],
   "source": [
    "print('len(outputs)',len(outputs))\n",
    "for index,each in enumerate(outputs):\n",
    "    print('shape',index,each.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output_Layer\n",
    "###类似于bert encoder端的输出，维度是：（batch_size,time_step,dim）\n",
    "#通过conv1D（1,1，）之后变为 x_start (None, None, 1)\n",
    "#再经过squeeze之后变为x_start (None, None)#（batch_size,time_step），每个step对应一个开始的位置\n",
    "#然后就可以计算start_softmax了；mask_logits x_start (None, None)；x_start softmax (None, None)\n",
    "\n",
    "\n",
    "x_start = Concatenate()([outputs[1], outputs[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_start (300, 400, 256)\n"
     ]
    }
   ],
   "source": [
    "print('x_start',x_start.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_logits(inputs, mask, mask_value=-1e30):\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    return inputs + mask_value * (1 - mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1D x_start (300, 400, 1)\n",
      "squeeze x_start (300, 400)\n",
      "mask_logits x_start (300, 400)\n",
      "x_start softmax (300, 400)\n"
     ]
    }
   ],
   "source": [
    "x_start = Conv1D(1, 1,\n",
    "                 kernel_initializer=init,\n",
    "                 kernel_regularizer=regularizer,\n",
    "                 activation='linear')(x_start)\n",
    "\n",
    "print('conv1D x_start',x_start.shape)\n",
    "\n",
    "#从tensor中删除所有大小是1的维度\n",
    "x_start = Lambda(lambda x: tf.squeeze(x, axis=-1))(x_start)\n",
    "\n",
    "print('squeeze x_start',x_start.shape)\n",
    "\n",
    "## mask_logits输出维度与输入维度一样\n",
    "x_start = Lambda(lambda x: mask_logits(x[0], x[1]))([x_start, c_mask])\n",
    "\n",
    "print('mask_logits x_start',x_start.shape)\n",
    "\n",
    "\n",
    "##输出的x_start是已经经过了softmax计算之后的值\n",
    "\n",
    "'''\n",
    "softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)\n",
    "Returns:\n",
    "A `Tensor`. Has the same type and shape as `logits`.\n",
    "axis: The dimension softmax would be performed on. The default is -1 which\n",
    "      indicates the last dimension.\n",
    "'''\n",
    "x_start = Lambda(lambda x: K.softmax(x), name='start')(x_start)  # [bs, len]\n",
    "\n",
    "print('x_start softmax',x_start.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_end concatte (300, 400, 256)\n",
      "x_end conv1d (300, 400, 1)\n",
      "x_end squeeze (300, 400)\n",
      "x_end logits (300, 400)\n",
      "x_end softmax (300, 400)\n"
     ]
    }
   ],
   "source": [
    "x_end = Concatenate()([outputs[1], outputs[3]])\n",
    "print('x_end concatte',x_end.shape)\n",
    "x_end = Conv1D(1, 1,\n",
    "               kernel_initializer=init,\n",
    "               kernel_regularizer=regularizer,\n",
    "               activation='linear')(x_end)\n",
    "print('x_end conv1d',x_end.shape)\n",
    "x_end = Lambda(lambda x: tf.squeeze(x, axis=-1))(x_end)\n",
    "print('x_end squeeze',x_end.shape)\n",
    "x_end = Lambda(lambda x: mask_logits(x[0], x[1]))([x_end, c_mask])\n",
    "print('x_end logits',x_end.shape)\n",
    "x_end = Lambda(lambda x: K.softmax(x), name='end')(x_end)  # [bs, len]\n",
    "print('x_end softmax',x_end.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.regularizers import *\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class QAoutputBlock(Layer):\n",
    "    def __init__(self, ans_limit=30, **kwargs):\n",
    "        self.ans_limit = ans_limit\n",
    "        super(QAoutputBlock, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(QAoutputBlock, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        x1 ,x2 = x\n",
    "        outer = tf.matmul(tf.expand_dims(x1, axis=2), tf.expand_dims(x2, axis=1))\n",
    "        #num_lower:下三角矩阵保留的副对角线数量，从主对角线开始计算，相当于下三角的带宽。取值为负数时，则全部保留。\n",
    "        #num_upper:上三角矩阵保留的副对角线数量，从主对角线开始计算，相当于上三角的带宽。取值为负数时，则全部保留。\n",
    "        #本程序中下三角不保留一行，上三角保留答案个长度\n",
    "        #开始的位置一定在结束的位置前面，所以只取上三角部分；行对应开始位置，列对应结束位置\n",
    "        ##答案的长度：从开始位置到结束位置；第一行0-ans_limit,第二行1-ans_limit+1一直到所有的情况\n",
    "        outer = tf.linalg.band_part(outer, 0, self.ans_limit)#（batch_size,start_steop,end_step)\n",
    "        \n",
    "        #列取索引，就是对应第几行，就是开始的索引位置；即对每个结束位置固定后，每个开始位置计算，取乘积最大的作为开始位置\n",
    "        \n",
    "        ##output1和output2似乎是在验证评估时使用，训练时不使用\n",
    "        output1 = tf.reshape(tf.cast(tf.argmax(tf.reduce_max(outer, axis=2), axis=1), tf.float32),(-1,1))\n",
    "        \n",
    "        ##与上面对应：结束位置索引即对每个开始位置固定后，每个结束位置计算，取乘积最大的作为结束位置\n",
    "        output2 = tf.reshape(tf.cast(tf.argmax(tf.reduce_max(outer, axis=1), axis=1), tf.float32),(-1,1))\n",
    "\n",
    "        return [output1, output2]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [(input_shape[0][0],1), (input_shape[0][0],1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "\n",
    "def shape_list(x):\n",
    "  \"\"\"Return list of dims, statically where possible.\"\"\"\n",
    "  x = tf.convert_to_tensor(x)\n",
    "\n",
    "  # If unknown rank, return dynamic shape\n",
    "  if x.get_shape().dims is None:\n",
    "    return tf.shape(x)\n",
    "\n",
    "  static = x.get_shape().as_list()\n",
    "  shape = tf.shape(x)\n",
    "\n",
    "  ret = []\n",
    "  for i in range(len(static)):\n",
    "    dim = static[i]\n",
    "    if dim is None:\n",
    "      dim = shape[i]\n",
    "    ret.append(dim)\n",
    "  return ret\n",
    "\n",
    "class LabelPadding(Layer):\n",
    "    def __init__(self, max_len, **kwargs):\n",
    "        self.max_len = max_len\n",
    "        super(LabelPadding, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(LabelPadding, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None, training=None):\n",
    "        tensor_shape = shape_list(x) # [bs, len]\n",
    "        zero_paddings = tf.zeros((tensor_shape[0], self.max_len - tensor_shape[1]))\n",
    "        x = tf.concat([x, zero_paddings], axis=-1)\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer  (300, 400, 400)\n"
     ]
    }
   ],
   "source": [
    "x1 ,x2 = [x_start, x_end]\n",
    "outer = tf.matmul(tf.expand_dims(x1, axis=2), tf.expand_dims(x2, axis=1))\n",
    "outer = tf.linalg.band_part(outer, 0,ans_limit)#（batch_size,start_steop,end_step)\n",
    "print('outer ',outer.shape)\n",
    "# #列取索引，就是对应第几行，就是开始的索引位置；即对每个结束位置固定后，每个开始位置计算，取乘积最大的作为开始位置\n",
    "\n",
    "# ##output1和output2似乎是在验证评估时使用，训练时不使用\n",
    "# output1 = tf.reshape(tf.cast(tf.argmax(tf.reduce_max(outer, axis=2), axis=1), tf.float32),(-1,1))\n",
    "\n",
    "# ##与上面对应：结束位置索引即对每个开始位置固定后，每个结束位置计算，取乘积最大的作为结束位置\n",
    "# output2 = tf.reshape(tf.cast(tf.argmax(tf.reduce_max(outer, axis=1), axis=1), tf.float32),(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 400)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_max(outer, axis=2).shape)\n",
    "print(tf.argmax(tf.reduce_max(outer, axis=2), axis=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 400)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_max(outer, axis=1).shape)\n",
    "print(tf.argmax(tf.reduce_max(outer, axis=1), axis=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_start  x_start_fin x_end x_end_fin  (300, 400) (300, 1) (300, 400) (300, 1)\n"
     ]
    }
   ],
   "source": [
    "##找到最大概率值对应的索引位置\n",
    "x_start_fin, x_end_fin = QAoutputBlock(ans_limit, name='qa_output')([x_start, x_end])\n",
    "\n",
    "# if use model.fit, the output shape must be padded to the max length\n",
    "x_start = LabelPadding(cont_limit, name='start_pos')(x_start)\n",
    "x_end = LabelPadding(cont_limit, name='end_pos')(x_end)\n",
    "print('x_start  x_start_fin x_end x_end_fin ',x_start.shape,x_start_fin.shape,x_end.shape,x_end_fin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_start_fin\n",
    "# x_end_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs__=[contw_input_, quesw_input_, contc_input_, quesc_input_]\n",
    "# inputs__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Tensor.op is meaningless when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-336da66b2b6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model=Model(inputs=[contw_input_, quesw_input_, contc_input_, quesc_input_],\n\u001b[0;32m----> 2\u001b[0;31m                  outputs=[x_start, x_end, x_start_fin, x_end_fin])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0m_keras_api_gauge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;31m# initializing _distribution_strategy here since it is possible to call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m    166\u001b[0m       \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m       \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_history'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m       \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_keras_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mcreate_keras_history\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mkeras_tensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mTensors\u001b[0m \u001b[0mfound\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcame\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0ma\u001b[0m \u001b[0mKeras\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m   \"\"\"\n\u001b[0;32m--> 184\u001b[0;31m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreated_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_keras_history_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcreated_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36m_create_keras_history_helper\u001b[0;34m(tensors, processed_ops, created_layers)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_history'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m  \u001b[0;31m# The Op that created this Tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed_ops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m       \u001b[0;31m# Recursively set `_keras_history`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m     raise AttributeError(\n\u001b[0;32m-> 1080\u001b[0;31m         \"Tensor.op is meaningless when eager execution is enabled.\")\n\u001b[0m\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Tensor.op is meaningless when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model=Model(inputs=[contw_input_, quesw_input_, contc_input_, quesc_input_],\n",
    "                 outputs=[x_start, x_end, x_start_fin, x_end_fin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.8, beta_2=0.999, epsilon=1e-7)\n",
    "\n",
    "##损失函数有4个应该是模型有4个输出，对应4个label,计算每个输出的损失函数，加权求和后最为最终的损失函数，权重即为loss_weights\n",
    "model.compile(optimizer=optimizer, loss=['categorical_crossentropy', 'categorical_crossentropy', 'mae', 'mae'],\n",
    "              loss_weights=[1, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "##假设总共10000个单词，每个单词300维度 \n",
    "embedding_matrix = np.random.random((10000, 300))\n",
    "##总共1233个字符，每个字符64维度\n",
    "embedding_matrix_char = np.random.random((1233, 64))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    'word_dim': 300,\n",
    "    'char_dim': 64,\n",
    "    'cont_limit': 400,\n",
    "    'ques_limit': 50,\n",
    "    'char_limit': 16,\n",
    "    'ans_limit': 30,\n",
    "    'char_input_size': 1233,\n",
    "    'filters': 128,\n",
    "    'num_head': 8,\n",
    "    'dropout': 0.5,\n",
    "    'batch_size': 16,\n",
    "    'epoch': 25,\n",
    "    'ema_decay': 0.9999,\n",
    "    'learning_rate': 1e-3,\n",
    "    'path': 'QA001',\n",
    "    'use_cove': True\n",
    "}\n",
    "\n",
    "\n",
    "# load data\n",
    "char_dim = 200##好像没有使用\n",
    "cont_limit = 400\n",
    "ques_limit = 50\n",
    "char_limit = 16\n",
    "\n",
    "###0-10000之间的某个词模拟所有词的的index\n",
    "##embedding词典大小总共10000个单词；转成0-9999个索引对应，300个上下文，每个上下文长度最大为为400\n",
    "context_word = np.random.randint(0, 10000, (300, cont_limit))\n",
    "question_word = np.random.randint(0, 10000, (300, ques_limit))\n",
    "context_char = np.random.randint(0, 96, (300, cont_limit, char_limit))\n",
    "question_char = np.random.randint(0, 96, (300, ques_limit, char_limit))\n",
    "start_label = np.random.randint(0, 2, (300, cont_limit))\n",
    "end_label = np.random.randint(0, 2, (300, cont_limit))\n",
    "start_label_fin = np.argmax(start_label, axis=-1)\n",
    "end_label_fin = np.argmax(end_label, axis=-1)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# load data\n",
    "char_dim = 64\n",
    "cont_limit = 400\n",
    "ques_limit = 50\n",
    "char_limit = 16\n",
    "#生成维度为（300，cont_limit）,大小在0-10000之间的随机整数##上下文长度最大400个词，每个词的维度是300d（感觉不对，应该是有300个上下文）\n",
    "context_word = np.random.randint(0, 10000, (300, cont_limit))\n",
    "question_word = np.random.randint(0, 10000, (300, ques_limit))\n",
    "\n",
    "##最多400个词，每个词最多16个字符，字符维度也是300维度\n",
    "context_char = np.random.randint(0, 96, (300, cont_limit, char_limit))\n",
    "question_char = np.random.randint(0, 96, (300, ques_limit, char_limit))\n",
    "\n",
    "start_label = np.random.randint(0, 2, (300, cont_limit))\n",
    "end_label = np.random.randint(0, 2, (300, cont_limit))\n",
    "start_label_fin = np.argmax(start_label, axis=-1)\n",
    "end_label_fin = np.argmax(end_label, axis=-1)\n",
    "'''\n",
    "fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, \n",
    "validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, \n",
    "sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1)\n",
    "\n",
    "即\n",
    "x=[context_word, question_word, context_char, question_char],\n",
    "y=[start_label, end_label, start_label_fin, end_label_fin]\n",
    "\n",
    "Model(inputs=[contw_input_, quesw_input_, contc_input_, quesc_input_],\n",
    "                 outputs=[x_start, x_end, x_start_fin, x_end_fin])\n",
    "                 \n",
    "Model根据输入经过网络得到输出，输出和对应的label求出损失函数，损失函数加权后作为最终的损失函数，优化器使得最终的损失函数最小\n",
    "'''\n",
    "model.fit([context_word, question_word, context_char, question_char],\n",
    "          [start_label, end_label, start_label_fin, end_label_fin], batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
