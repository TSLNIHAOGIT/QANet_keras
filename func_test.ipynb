{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "char_dim = 200\n",
    "cont_limit = 400##上下文长度最大400个词，每个词的维度是300d\n",
    "\n",
    "ques_limit = 50\n",
    "char_limit = 16\n",
    "\n",
    "context_word = np.random.randint(0, 10000, (300, cont_limit))\n",
    "\n",
    "start_label = np.random.randint(0, 2, (300, cont_limit))\n",
    "start_label_fin = np.argmax(start_label, axis=-1)##按照行，每行取最大值对应的索引，最终就是一维数组，大小为300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 400)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2736, 1749, 6160, ..., 5420, 8854,  776],\n",
       "       [4082, 6325, 8149, ..., 2807, 7564, 5399],\n",
       "       [9513, 2773, 1264, ..., 7053, 6454,  593],\n",
       "       ...,\n",
       "       [1063, 5727, 6204, ..., 1103, 8081, 1759],\n",
       "       [6217, 9398, 5502, ..., 2905,  822, 5792],\n",
       "       [9438, 3392, 9298, ..., 8015,  554, 5276]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(context_word.shape)\n",
    "context_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 400)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, ..., 1, 0, 0],\n",
       "       [1, 0, 1, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       ...,\n",
       "       [0, 0, 1, ..., 1, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 1, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(start_label.shape)\n",
    "start_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1,  0,  4,  2,  2,  1,  3,  0,  1,  2,  2,  2,  0,  1,  1,  2,  0,\n",
       "        2,  0,  0,  0,  0,  1,  0,  1,  2,  2,  0,  1,  1,  0,  5,  0,  0,\n",
       "        3,  0,  0,  3,  0,  0,  0,  1,  1,  1,  0,  2,  2,  0,  0,  0,  1,\n",
       "        2,  1,  0,  0,  1,  0,  0,  4,  1,  1,  0,  0,  1,  1,  1,  0,  0,\n",
       "        0,  2,  3,  0,  0,  2,  1,  2,  0,  0,  0,  1,  1,  0,  4,  0,  0,\n",
       "        0,  0,  2,  2,  0,  0,  0,  0,  2,  0,  1,  3,  1,  2,  3,  0,  0,\n",
       "        0,  2,  0,  1,  4,  2,  0,  0, 15,  4,  0,  0,  5,  1,  1,  0,  6,\n",
       "        0,  0,  1,  0,  2,  0,  1,  1,  1,  1,  2,  0,  2,  4,  3,  0,  2,\n",
       "        1,  0,  0,  0,  0,  0,  1,  0,  2,  0,  2,  1,  0,  2,  2,  0,  1,\n",
       "        0,  3,  0,  0,  6,  0,  1,  5,  0,  3,  3,  0,  0,  0,  1,  1,  1,\n",
       "        1,  0,  0,  4,  3,  0,  0,  0,  6,  1,  0,  1,  1,  0,  0,  0,  1,\n",
       "        2, 11,  0,  0,  3,  1,  3,  2,  1,  1,  1,  0,  0,  3,  1,  0,  2,\n",
       "        1,  1,  7,  1,  1,  0,  2,  0,  0,  4,  2,  0, 12,  0,  0,  0,  0,\n",
       "        0,  1,  0,  4,  1,  0,  0,  0,  1,  0,  1,  1,  2,  1,  0,  0,  1,\n",
       "        0,  2,  5,  3,  0,  0,  0,  0,  1,  0,  1,  0,  0,  0,  1,  0,  1,\n",
       "        1,  1,  2,  0,  0,  1,  6,  0,  1,  0,  0,  1,  0,  2,  0,  1,  6,\n",
       "        1,  4,  0,  1,  0,  0,  2,  0,  8,  1,  0,  0,  0,  0,  0,  2,  0,\n",
       "        0,  1,  4,  0,  3,  0,  0,  2,  2,  0,  0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(start_label_fin.shape)\n",
    "start_label_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array([[0, 1, 2],\n",
    "       [3, 4, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(data,axis=-1)#按照最后一个轴取最大值对应的索引，二维时（0列,1行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "aa=np.array([1,2,3])\n",
    "print(aa.shape)\n",
    "bb=np.array([[1,2,3]])\n",
    "print(bb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54063602, 0.59705779, 0.23673067, ..., 0.00901409, 0.94397308,\n",
       "        0.93437064],\n",
       "       [0.83807122, 0.17333524, 0.89638117, ..., 0.21455012, 0.06870714,\n",
       "        0.77759069],\n",
       "       [0.83292363, 0.16857965, 0.44754816, ..., 0.3123176 , 0.62324809,\n",
       "        0.01759297],\n",
       "       ...,\n",
       "       [0.72111465, 0.33476204, 0.40248569, ..., 0.43109959, 0.05329936,\n",
       "        0.77043341],\n",
       "       [0.44750056, 0.0377479 , 0.31506585, ..., 0.39251956, 0.73286458,\n",
       "        0.18546568],\n",
       "       [0.77641584, 0.02369054, 0.91366589, ..., 0.15751444, 0.51642877,\n",
       "        0.57348254]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.random((1000, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[\n",
    "    [[1,2,3]],\n",
    "     [[2,3,4]]\n",
    "]\n",
    "a=np.array(a)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=10, shape=(2, 3), dtype=int64, numpy=\n",
       "array([[1, 2, 3],\n",
       "       [2, 3, 4]])>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.squeeze(a,axis=1)\n",
    "##去掉维度为1的那个维度，若有多个维度为1的，可通过axis指定，默认去掉所有维度为1的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=[\n",
    "    [[1],[2]],\n",
    "    [[3],[4]]\n",
    "]\n",
    "b=np.array(b)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8, shape=(2, 2), dtype=int64, numpy=\n",
       "array([[1, 2],\n",
       "       [3, 4]])>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.squeeze(b,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=[\n",
    "    [[1],[2]],\n",
    "    [[3],[3]]\n",
    "]\n",
    "c=np.array(c)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=18, shape=(2, 2), dtype=int64, numpy=\n",
       "array([[1, 2],\n",
       "       [3, 3]])>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.squeeze(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.squeeze(c,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=23, shape=(2, 2), dtype=int64, numpy=\n",
       "array([[1, 2],\n",
       "       [3, 3]])>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.squeeze(c,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 1, 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=[\n",
    "    [\n",
    "        [\n",
    "            [[1,2]],\n",
    "            [[2,3]],\n",
    "            [[3,4]],\n",
    "        ],\n",
    "        \n",
    "        [\n",
    "            [[4,5]],\n",
    "            [[5,6]],\n",
    "            [[6,7]],\n",
    "        ]\n",
    "    ]\n",
    "]\n",
    "d=np.array(d)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=25, shape=(2, 3, 2), dtype=int64, numpy=\n",
       "array([[[1, 2],\n",
       "        [2, 3],\n",
       "        [3, 4]],\n",
       "\n",
       "       [[4, 5],\n",
       "        [5, 6],\n",
       "        [6, 7]]])>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.squeeze(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=27, shape=(2, 3, 1, 2), dtype=int64, numpy=\n",
       "array([[[[1, 2]],\n",
       "\n",
       "        [[2, 3]],\n",
       "\n",
       "        [[3, 4]]],\n",
       "\n",
       "\n",
       "       [[[4, 5]],\n",
       "\n",
       "        [[5, 6]],\n",
       "\n",
       "        [[6, 7]]]])>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.squeeze(d,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=29, shape=(1, 2, 3, 2), dtype=int64, numpy=\n",
       "array([[[[1, 2],\n",
       "         [2, 3],\n",
       "         [3, 4]],\n",
       "\n",
       "        [[4, 5],\n",
       "         [5, 6],\n",
       "         [6, 7]]]])>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.squeeze(d,axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 1\n",
      "d 4\n"
     ]
    }
   ],
   "source": [
    "a,b,c,d=[1,2,3,4]\n",
    "print('a',a)\n",
    "print('d',d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]], shape=(3, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]\n",
      " [1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]], shape=(6, 2), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=42, shape=(6, 6), dtype=float32, numpy=\n",
       "array([[1., 2., 1., 2., 1., 2.],\n",
       "       [3., 4., 3., 4., 3., 4.],\n",
       "       [5., 6., 5., 6., 5., 6.],\n",
       "       [1., 2., 1., 2., 1., 2.],\n",
       "       [3., 4., 3., 4., 3., 4.],\n",
       "       [5., 6., 5., 6., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.float32)\n",
    "print(a)\n",
    "#对输入a进行扩充，a第一个维度大小变为原来的2倍，第二个维度大小变为原来的3倍\n",
    "print(tf.tile(a,[2,1]))\n",
    "a1 = tf.tile(a, [2, 3])\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant([[[1, 2], [3, 4], [5, 6]],[[1, 2], [3, 4], [5, 6]]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.constant([[[1, 2], [3, 4], [5, 6]],[[1, 2], [3, 4], [5, 6]]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "c=np.random.randn(1,2,3,4)\n",
    "d=np.random.randn(1,2,4,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-0.57716738, -3.23812773, -1.87291973],\n",
       "         [-0.58326127, -0.37149743, -1.76212542],\n",
       "         [-4.89883118,  0.96391325, -1.67687701]],\n",
       "\n",
       "        [[ 4.30059462,  3.98749693, -0.97745195],\n",
       "         [-3.5577567 , -1.0608963 , -1.41327879],\n",
       "         [-2.11998842, -2.36538543, -0.60398722]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(c,d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[[-0.57716738, -3.23812773, -1.87291973],\n",
       "           [-2.55418118,  1.13998462, -0.37116878]]],\n",
       "\n",
       "\n",
       "         [[[-0.58326127, -0.37149743, -1.76212542],\n",
       "           [-0.15401943, -0.81604797, -1.60033706]]],\n",
       "\n",
       "\n",
       "         [[[-4.89883118,  0.96391325, -1.67687701],\n",
       "           [ 3.73058028, -2.56517434,  3.2849799 ]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 2.13108952,  6.44127828,  2.00633629],\n",
       "           [ 4.30059462,  3.98749693, -0.97745195]]],\n",
       "\n",
       "\n",
       "         [[[ 0.59077796, -3.62844897, -1.09117756],\n",
       "           [-3.5577567 , -1.0608963 , -1.41327879]]],\n",
       "\n",
       "\n",
       "         [[[-0.62996252, -2.81283388, -1.23866006],\n",
       "           [-2.11998842, -2.36538543, -0.60398722]]]]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(c,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###开始一些其它计算\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([\n",
    "    [[1,2,3,4],[2,3,4,5],[3,4,5,6]],\n",
    "    [[1,2,3,1],[2,2,2,2],[3,3,3,3]],\n",
    "])\n",
    "a = tf.constant(a, dtype=tf.float32)\n",
    "w=np.array([[1,1,1,1],[2,2,2,2]]).T\n",
    "w=tf.constant(w, dtype=tf.float32)\n",
    "ww=np.array([[[2,2,2,2]]])\n",
    "ww=tf.constant(ww, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1. 2. 3. 4.]\n",
      "  [2. 3. 4. 5.]\n",
      "  [3. 4. 5. 6.]]\n",
      "\n",
      " [[1. 2. 3. 1.]\n",
      "  [2. 2. 2. 2.]\n",
      "  [3. 3. 3. 3.]]], shape=(2, 3, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 2.]\n",
      " [1. 2.]\n",
      " [1. 2.]\n",
      " [1. 2.]], shape=(4, 2), dtype=float32)\n",
      "tf.Tensor([[[2. 2. 2. 2.]]], shape=(1, 1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(w)\n",
    "print(ww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=934, shape=(2, 3, 2), dtype=float32, numpy=\n",
       "array([[[10., 20.],\n",
       "        [14., 28.],\n",
       "        [18., 36.]],\n",
       "\n",
       "       [[ 7., 14.],\n",
       "        [ 8., 16.],\n",
       "        [12., 24.]]], dtype=float32)>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.dot(a,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 2.  4.  6.  8.]\n",
      "  [ 4.  6.  8. 10.]\n",
      "  [ 6.  8. 10. 12.]]\n",
      "\n",
      " [[ 2.  4.  6.  2.]\n",
      "  [ 4.  4.  4.  4.]\n",
      "  [ 6.  6.  6.  6.]]], shape=(2, 3, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[3. 4. 5. 6.]\n",
      "  [4. 5. 6. 7.]\n",
      "  [5. 6. 7. 8.]]\n",
      "\n",
      " [[3. 4. 5. 3.]\n",
      "  [4. 4. 4. 4.]\n",
      "  [5. 5. 5. 5.]]], shape=(2, 3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "tf.Tensor(\n",
    "[[[1. 2. 3. 4.]\n",
    "  [2. 3. 4. 5.]\n",
    "  [3. 4. 5. 6.]]\n",
    "\n",
    " [[1. 2. 3. 1.]\n",
    "  [2. 2. 2. 2.]\n",
    "  [3. 3. 3. 3.]]], shape=(2, 3, 4), dtype=float32)\n",
    "\n",
    "tf.Tensor([[[2. 2. 2. 2.]]], shape=(1, 1, 4), dtype=float32)\n",
    "'''\n",
    "# print(a)\n",
    "# print(ww)\n",
    "print(a*ww)##要求维度个数相同，[1,1,dim]这种形式时最后一维每个元素对应相乘\n",
    "print(a+ww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa= np.array([[0.4], [1.2]])\n",
    "w12=tf.constant(wa.T)\n",
    "w11=tf.constant(wa)\n",
    "xa = np.array([range(1,6), range(5,10)])\n",
    "x11=tf.constant(xa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.4]\n",
      " [1.2]], shape=(2, 1), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[1 2 3 4 5]\n",
      " [5 6 7 8 9]], shape=(2, 5), dtype=int64)\n",
      "数组相乘\n",
      " [[ 0.4  0.8  1.2  1.6  2. ]\n",
      " [ 6.   7.2  8.4  9.6 10.8]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(w11)\n",
    "print(x11)\n",
    "\n",
    "print('数组相乘\\n',wa*xa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt=[[1,2],[3,4]]\n",
    "xt=[[3,2],[5,4]]\n",
    "wt=tf.constant(wt)\n",
    "xt=tf.constant(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[3 2]\n",
      " [5 4]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 3  4]\n",
      " [15 16]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[4 4]\n",
      " [8 8]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "##tensorflow中点乘就是对应元素相乘\n",
    "print(wt)\n",
    "#矩阵和该矩阵所有元素对应相乘\n",
    "print(xt)\n",
    "print(wt*xt)##tensorflow中：行数相同，列数为1时，也是对应元素相乘;shape完全相同时也是对应各个元素相乘\n",
    "print(wt+xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor([3 5], shape=(2,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 3 10]\n",
      " [ 9 20]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[4 7]\n",
      " [6 9]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "##1维数组shape=(2,),二维矩阵：shape=(2,1)\n",
    "print(wt)\n",
    "#元素中每一行元素和改行元素对应相乘\n",
    "print(xt[:,0])\n",
    "print(wt*xt[:,0])##tensorflow中：行数相同，列数为1时，也是对应元素相乘;shape完全相同时也是对应各个元素相乘\n",
    "print(wt+xt[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[3]\n",
      " [5]], shape=(2, 1), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 3  6]\n",
      " [15 20]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[4 5]\n",
      " [8 9]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(wt)\n",
    "##矩阵中每一列和该列元素对应相乘\n",
    "print(tf.reshape(xt[:,0],(2,1)))\n",
    "print(wt*tf.reshape(xt[:,0],(2,1)))\n",
    "print(wt+tf.reshape(xt[:,0],(2,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=134, shape=(3, 2), dtype=float32, numpy=\n",
       "array([[10., 20.],\n",
       "       [14., 28.],\n",
       "       [18., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.dot(a[0],w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=55, shape=(3, 1), dtype=float32, numpy=\n",
       "array([[ 7.],\n",
       "       [ 8.],\n",
       "       [12.]], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.dot(a[1],w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=250, shape=(2, 3, 4), dtype=int64, numpy=\n",
       "array([[[0, 2, 0, 2],\n",
       "        [2, 1, 2, 2],\n",
       "        [1, 0, 1, 1]],\n",
       "\n",
       "       [[0, 0, 2, 0],\n",
       "        [2, 2, 2, 2],\n",
       "        [1, 1, 2, 2]]])>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot:只需a矩阵的最后一维dim等于b矩阵倒数第二维dim即可，对应二维情况就是第一个的列数等于第二个矩阵行数；\n",
    "\n",
    "# 也就是说点积发生在a,b矩阵最后两个维度上\n",
    "\n",
    "a1=tf.constant(np.random.randint(0,3,(2,3,4)))\n",
    "\n",
    "b1=tf.constant(np.random.randint(0,3,(6,7,4,5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0 2 0 2]\n",
      " [2 1 2 2]\n",
      " [1 0 1 1]], shape=(3, 4), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "a1\n",
    "print(a1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 0 0 2 2]\n",
      " [0 1 0 1 1]\n",
      " [2 0 0 1 2]\n",
      " [0 0 0 0 0]], shape=(4, 5), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "b1\n",
    "print(b1[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 6, 7, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[[ 0,  2,  0,  2,  2],\n",
       "          [ 2,  8,  2,  6,  2],\n",
       "          [ 4,  2,  2,  4,  2],\n",
       "          ...,\n",
       "          [ 2,  4,  2,  2,  4],\n",
       "          [ 2,  4,  6,  6,  4],\n",
       "          [ 4,  4,  4,  0,  6]],\n",
       "\n",
       "         [[ 4,  6,  4,  6,  6],\n",
       "          [ 4,  6,  2,  8,  6],\n",
       "          [ 6,  4,  2,  4,  4],\n",
       "          ...,\n",
       "          [ 4,  8,  6,  6,  6],\n",
       "          [ 4,  6,  4,  2,  6],\n",
       "          [ 0,  4,  4,  6,  4]],\n",
       "\n",
       "         [[ 6,  2,  4,  0,  8],\n",
       "          [ 4,  4,  2,  0,  0],\n",
       "          [ 4,  6,  4,  8,  4],\n",
       "          ...,\n",
       "          [ 2,  2,  4,  8,  4],\n",
       "          [ 4,  4,  2,  0,  6],\n",
       "          [ 6,  6,  8,  4,  2]],\n",
       "\n",
       "         [[ 4,  0,  4,  0,  6],\n",
       "          [ 6,  6,  6,  6,  4],\n",
       "          [ 4,  6,  0,  6,  0],\n",
       "          ...,\n",
       "          [ 4,  4,  2,  4,  8],\n",
       "          [ 0,  4,  2,  2,  6],\n",
       "          [ 6,  4,  6,  6,  6]],\n",
       "\n",
       "         [[ 4,  8,  8,  4,  8],\n",
       "          [ 0,  4,  4,  4,  4],\n",
       "          [ 4,  4,  4,  2,  4],\n",
       "          ...,\n",
       "          [ 0,  4,  4,  6,  4],\n",
       "          [ 6,  4,  4,  4,  4],\n",
       "          [ 4,  4,  8,  2,  4]],\n",
       "\n",
       "         [[ 4,  8,  4,  6,  6],\n",
       "          [ 8,  6,  6,  6,  6],\n",
       "          [ 0,  4,  2,  2,  6],\n",
       "          ...,\n",
       "          [ 2,  8,  8,  4,  4],\n",
       "          [ 0,  4,  4,  6,  2],\n",
       "          [ 4,  4,  0,  4,  4]]],\n",
       "\n",
       "\n",
       "        [[[ 8,  1,  0,  7,  9],\n",
       "          [ 4, 10,  3, 11,  6],\n",
       "          [ 6,  5,  5,  6,  5],\n",
       "          ...,\n",
       "          [ 5,  7,  5,  5,  8],\n",
       "          [ 9,  8, 10, 11,  6],\n",
       "          [12,  8, 10,  6,  4]],\n",
       "\n",
       "         [[ 4, 10,  5, 10,  5],\n",
       "          [ 6,  4,  6,  8,  6],\n",
       "          [ 8, 10,  1,  3,  4],\n",
       "          ...,\n",
       "          [ 9,  6, 10,  4,  8],\n",
       "          [10,  5,  7,  6,  6],\n",
       "          [ 4, 10, 10,  8,  6]],\n",
       "\n",
       "         [[ 6,  2,  4,  6, 12],\n",
       "          [ 4,  8,  1,  8,  8],\n",
       "          [ 6,  8,  6, 14,  4],\n",
       "          ...,\n",
       "          [ 6,  5,  6,  8, 12],\n",
       "          [ 3,  2,  2,  4,  8],\n",
       "          [ 6,  4, 14,  7,  5]],\n",
       "\n",
       "         [[ 3,  6,  8,  8,  6],\n",
       "          [11,  9,  4, 12,  8],\n",
       "          [ 5,  7,  6,  7,  4],\n",
       "          ...,\n",
       "          [ 7,  8,  7,  5, 10],\n",
       "          [ 6,  4,  5,  2,  8],\n",
       "          [ 9,  6, 11,  6,  9]],\n",
       "\n",
       "         [[ 3,  6,  8,  7,  8],\n",
       "          [ 2,  6,  8,  6,  6],\n",
       "          [ 6, 11, 10,  4,  6],\n",
       "          ...,\n",
       "          [ 6,  5, 10, 11, 12],\n",
       "          [ 6,  3,  8,  6,  6],\n",
       "          [ 6, 10, 14,  6,  6]],\n",
       "\n",
       "         [[ 4,  8,  6,  6,  9],\n",
       "          [12, 12,  9, 10, 11],\n",
       "          [ 4, 10,  7,  9,  7],\n",
       "          ...,\n",
       "          [ 6, 10,  6,  4,  6],\n",
       "          [ 8,  6,  4, 11, 10],\n",
       "          [11,  8,  6,  4, 10]]],\n",
       "\n",
       "\n",
       "        [[[ 4,  0,  0,  3,  4],\n",
       "          [ 2,  4,  1,  5,  3],\n",
       "          [ 3,  2,  2,  2,  2],\n",
       "          ...,\n",
       "          [ 2,  3,  2,  2,  4],\n",
       "          [ 4,  3,  4,  5,  2],\n",
       "          [ 6,  4,  5,  3,  1]],\n",
       "\n",
       "         [[ 1,  4,  2,  4,  2],\n",
       "          [ 3,  1,  3,  3,  2],\n",
       "          [ 3,  5,  0,  1,  1],\n",
       "          ...,\n",
       "          [ 4,  2,  4,  1,  3],\n",
       "          [ 5,  2,  3,  3,  2],\n",
       "          [ 2,  5,  5,  3,  2]],\n",
       "\n",
       "         [[ 2,  1,  1,  3,  5],\n",
       "          [ 1,  4,  0,  4,  4],\n",
       "          [ 2,  3,  2,  6,  1],\n",
       "          ...,\n",
       "          [ 3,  2,  2,  3,  6],\n",
       "          [ 1,  0,  1,  2,  3],\n",
       "          [ 2,  1,  6,  3,  2]],\n",
       "\n",
       "         [[ 1,  3,  3,  4,  2],\n",
       "          [ 5,  4,  1,  5,  4],\n",
       "          [ 2,  3,  3,  3,  2],\n",
       "          ...,\n",
       "          [ 3,  4,  3,  2,  4],\n",
       "          [ 3,  1,  2,  1,  3],\n",
       "          [ 4,  3,  5,  2,  4]],\n",
       "\n",
       "         [[ 1,  2,  3,  3,  3],\n",
       "          [ 1,  3,  4,  2,  2],\n",
       "          [ 3,  5,  4,  2,  3],\n",
       "          ...,\n",
       "          [ 3,  2,  5,  5,  6],\n",
       "          [ 2,  1,  3,  2,  3],\n",
       "          [ 2,  5,  6,  3,  3]],\n",
       "\n",
       "         [[ 1,  3,  3,  2,  4],\n",
       "          [ 5,  5,  4,  4,  5],\n",
       "          [ 2,  5,  3,  4,  3],\n",
       "          ...,\n",
       "          [ 3,  4,  2,  1,  2],\n",
       "          [ 4,  3,  1,  5,  5],\n",
       "          [ 5,  4,  3,  1,  5]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[ 4,  0,  0,  2,  4],\n",
       "          [ 0,  0,  2,  2,  2],\n",
       "          [ 2,  0,  0,  0,  0],\n",
       "          ...,\n",
       "          [ 2,  2,  2,  0,  0],\n",
       "          [ 4,  4,  2,  2,  2],\n",
       "          [ 4,  0,  2,  4,  0]],\n",
       "\n",
       "         [[ 2,  4,  2,  2,  0],\n",
       "          [ 2,  0,  4,  2,  0],\n",
       "          [ 2,  2,  0,  0,  2],\n",
       "          ...,\n",
       "          [ 2,  0,  2,  0,  0],\n",
       "          [ 2,  0,  4,  0,  2],\n",
       "          [ 4,  2,  4,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  2,  4,  4],\n",
       "          [ 2,  2,  0,  4,  4],\n",
       "          [ 4,  2,  2,  4,  0],\n",
       "          ...,\n",
       "          [ 0,  2,  2,  2,  4],\n",
       "          [ 0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  4,  2,  2]],\n",
       "\n",
       "         [[ 0,  2,  4,  4,  0],\n",
       "          [ 4,  4,  0,  4,  0],\n",
       "          [ 0,  0,  4,  2,  4],\n",
       "          ...,\n",
       "          [ 2,  4,  2,  0,  0],\n",
       "          [ 2,  0,  0,  0,  4],\n",
       "          [ 2,  0,  4,  2,  2]],\n",
       "\n",
       "         [[ 0,  0,  2,  4,  0],\n",
       "          [ 2,  2,  2,  0,  2],\n",
       "          [ 2,  4,  4,  0,  2],\n",
       "          ...,\n",
       "          [ 4,  0,  2,  4,  4],\n",
       "          [ 2,  0,  4,  0,  0],\n",
       "          [ 4,  4,  4,  0,  0]],\n",
       "\n",
       "         [[ 2,  2,  0,  0,  4],\n",
       "          [ 2,  4,  0,  4,  2],\n",
       "          [ 4,  2,  4,  4,  2],\n",
       "          ...,\n",
       "          [ 0,  0,  0,  0,  0],\n",
       "          [ 4,  2,  0,  4,  4],\n",
       "          [ 4,  0,  2,  2,  4]]],\n",
       "\n",
       "\n",
       "        [[[ 8,  2,  0,  8, 10],\n",
       "          [ 4, 12,  4, 12,  6],\n",
       "          [ 6,  6,  6,  8,  6],\n",
       "          ...,\n",
       "          [ 6,  8,  6,  6,  8],\n",
       "          [10, 10, 12, 12,  8],\n",
       "          [12,  8, 10,  6,  6]],\n",
       "\n",
       "         [[ 6, 12,  6, 12,  6],\n",
       "          [ 6,  6,  6, 10,  8],\n",
       "          [10, 10,  2,  4,  6],\n",
       "          ...,\n",
       "          [10,  8, 12,  6, 10],\n",
       "          [10,  6,  8,  6,  8],\n",
       "          [ 4, 10, 10, 10,  8]],\n",
       "\n",
       "         [[ 8,  2,  6,  6, 14],\n",
       "          [ 6,  8,  2,  8,  8],\n",
       "          [ 8, 10,  8, 16,  6],\n",
       "          ...,\n",
       "          [ 6,  6,  8, 10, 12],\n",
       "          [ 4,  4,  2,  4, 10],\n",
       "          [ 8,  6, 16,  8,  6]],\n",
       "\n",
       "         [[ 4,  6, 10,  8,  8],\n",
       "          [12, 10,  6, 14,  8],\n",
       "          [ 6,  8,  6,  8,  4],\n",
       "          ...,\n",
       "          [ 8,  8,  8,  6, 12],\n",
       "          [ 6,  6,  6,  2, 10],\n",
       "          [10,  6, 12,  8, 10]],\n",
       "\n",
       "         [[ 4,  8, 10,  8, 10],\n",
       "          [ 2,  6,  8,  8,  8],\n",
       "          [ 6, 12, 12,  4,  6],\n",
       "          ...,\n",
       "          [ 6,  6, 10, 12, 12],\n",
       "          [ 8,  4, 10,  8,  6],\n",
       "          [ 8, 10, 16,  6,  6]],\n",
       "\n",
       "         [[ 6, 10,  6,  8, 10],\n",
       "          [14, 14, 10, 12, 12],\n",
       "          [ 4, 10,  8, 10,  8],\n",
       "          ...,\n",
       "          [ 6, 12,  8,  6,  8],\n",
       "          [ 8,  6,  6, 12, 10],\n",
       "          [12,  8,  6,  6, 10]]],\n",
       "\n",
       "\n",
       "        [[[ 6,  1,  0,  5,  7],\n",
       "          [ 3,  8,  3,  9,  5],\n",
       "          [ 6,  3,  3,  4,  3],\n",
       "          ...,\n",
       "          [ 4,  6,  4,  3,  6],\n",
       "          [ 7,  7,  8,  9,  5],\n",
       "          [10,  6,  8,  5,  4]],\n",
       "\n",
       "         [[ 4,  9,  5,  8,  5],\n",
       "          [ 6,  4,  6,  8,  5],\n",
       "          [ 7,  8,  1,  3,  4],\n",
       "          ...,\n",
       "          [ 7,  6,  8,  4,  6],\n",
       "          [ 8,  5,  7,  4,  6],\n",
       "          [ 4,  8,  9,  6,  4]],\n",
       "\n",
       "         [[ 5,  2,  4,  5, 11],\n",
       "          [ 4,  7,  1,  6,  6],\n",
       "          [ 6,  7,  5, 12,  3],\n",
       "          ...,\n",
       "          [ 4,  4,  5,  8, 10],\n",
       "          [ 3,  2,  2,  2,  6],\n",
       "          [ 5,  4, 12,  6,  4]],\n",
       "\n",
       "         [[ 3,  4,  7,  6,  5],\n",
       "          [10,  9,  4, 10,  6],\n",
       "          [ 4,  6,  5,  7,  4],\n",
       "          ...,\n",
       "          [ 6,  8,  5,  4,  8],\n",
       "          [ 4,  3,  3,  2,  8],\n",
       "          [ 8,  5, 10,  6,  8]],\n",
       "\n",
       "         [[ 3,  6,  8,  7,  7],\n",
       "          [ 2,  6,  7,  4,  5],\n",
       "          [ 6,  9,  8,  3,  6],\n",
       "          ...,\n",
       "          [ 5,  4,  8, 10, 10],\n",
       "          [ 6,  3,  7,  4,  5],\n",
       "          [ 6,  9, 12,  4,  5]],\n",
       "\n",
       "         [[ 4,  8,  5,  5,  9],\n",
       "          [10, 10,  7,  9,  9],\n",
       "          [ 4,  8,  6,  7,  7],\n",
       "          ...,\n",
       "          [ 4,  8,  6,  3,  4],\n",
       "          [ 6,  6,  3, 10,  8],\n",
       "          [ 9,  6,  4,  4,  9]]]]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=K.dot(a1,b1)\n",
    "print(res.shape)\n",
    "np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 0, 2, 2],\n",
       "       [8, 1, 0, 7, 9],\n",
       "       [4, 0, 0, 3, 4]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a1[0],b1[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=340, shape=(3, 5), dtype=int64, numpy=\n",
       "array([[0, 2, 0, 2, 2],\n",
       "       [8, 1, 0, 7, 9],\n",
       "       [4, 0, 0, 3, 4]])>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.dot(a1[0],b1[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[ 6  6]\n",
      "   [11 10]\n",
      "   [12 10]\n",
      "   [ 9  9]]\n",
      "\n",
      "  [[ 1  1]\n",
      "   [ 8  7]\n",
      "   [ 7  7]\n",
      "   [ 7  6]]\n",
      "\n",
      "  [[ 6  9]\n",
      "   [ 4  6]\n",
      "   [ 8 10]\n",
      "   [ 6  6]]]\n",
      "\n",
      "\n",
      " [[[ 3  2]\n",
      "   [ 4  4]\n",
      "   [ 8  5]\n",
      "   [ 3  1]]\n",
      "\n",
      "  [[ 3  4]\n",
      "   [ 8  8]\n",
      "   [ 3  4]\n",
      "   [10  8]]\n",
      "\n",
      "  [[ 2  1]\n",
      "   [ 4  2]\n",
      "   [11  4]\n",
      "   [ 7  2]]]], shape=(2, 3, 4, 2), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[ 6  6]\n",
      " [11 10]\n",
      " [12 10]\n",
      " [ 9  9]], shape=(4, 2), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "a2=tf.constant(np.random.randint(0,3,(2,3,4,5)))\n",
    "\n",
    "b2=tf.constant(np.random.randint(0,3,(2,3,5,2)))\n",
    "'''\n",
    "其中由这样一段描述：\n",
    "\n",
    "如果a和b的dimention大于2，实际上进行的会是batch_mat_mul,此时进行叉乘的是batch中的每一个切片（slice）\n",
    "\n",
    "这就要求：\n",
    "\n",
    "a和b除了最后两个维度可以不一致，其他维度要相同(比如上面代码第一维和第二维分别都是1,2)\n",
    "a和b最后两维的维度要符合矩阵乘法的要求（比如a的(3,4)能和b的(4,6)进行矩阵乘法）\n",
    "'''\n",
    "\n",
    "\n",
    "print(tf.matmul(a2,b2))#shape=(2,3,4,2)\n",
    "print(tf.matmul(a2[0][0],b2[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[[[ 6  6]\n",
      "     [ 1  2]\n",
      "     [ 4  6]]\n",
      "\n",
      "    [[ 5  2]\n",
      "     [ 2  4]\n",
      "     [ 6  1]]]\n",
      "\n",
      "\n",
      "   [[[11 10]\n",
      "     [ 8  6]\n",
      "     [ 8 11]]\n",
      "\n",
      "    [[ 6  5]\n",
      "     [ 7  4]\n",
      "     [ 7  2]]]\n",
      "\n",
      "\n",
      "   [[[12 10]\n",
      "     [ 6  4]\n",
      "     [ 6 10]]\n",
      "\n",
      "    [[ 6  6]\n",
      "     [ 6  4]\n",
      "     [ 8  2]]]\n",
      "\n",
      "\n",
      "   [[[ 9  9]\n",
      "     [ 5  4]\n",
      "     [ 5  8]]\n",
      "\n",
      "    [[ 5  4]\n",
      "     [ 4  4]\n",
      "     [ 7  1]]]]\n",
      "\n",
      "\n",
      "\n",
      "  [[[[ 2  1]\n",
      "     [ 1  1]\n",
      "     [ 2  2]]\n",
      "\n",
      "    [[ 1  0]\n",
      "     [ 1  0]\n",
      "     [ 1  0]]]\n",
      "\n",
      "\n",
      "   [[[13 13]\n",
      "     [ 8  7]\n",
      "     [10 14]]\n",
      "\n",
      "    [[ 9  6]\n",
      "     [ 8  7]\n",
      "     [10  3]]]\n",
      "\n",
      "\n",
      "   [[[ 6  8]\n",
      "     [ 7  7]\n",
      "     [ 9 10]]\n",
      "\n",
      "    [[ 6  2]\n",
      "     [ 6  5]\n",
      "     [ 5  2]]]\n",
      "\n",
      "\n",
      "   [[[ 7  8]\n",
      "     [ 7  6]\n",
      "     [ 8 10]]\n",
      "\n",
      "    [[ 6  4]\n",
      "     [ 7  5]\n",
      "     [ 5  3]]]]\n",
      "\n",
      "\n",
      "\n",
      "  [[[[ 6  8]\n",
      "     [ 4  4]\n",
      "     [ 6  9]]\n",
      "\n",
      "    [[ 7  5]\n",
      "     [ 6  7]\n",
      "     [ 6  4]]]\n",
      "\n",
      "\n",
      "   [[[ 6  5]\n",
      "     [ 5  3]\n",
      "     [ 4  6]]\n",
      "\n",
      "    [[ 3  4]\n",
      "     [ 5  2]\n",
      "     [ 3  2]]]\n",
      "\n",
      "\n",
      "   [[[ 8 10]\n",
      "     [ 5  6]\n",
      "     [ 8 10]]\n",
      "\n",
      "    [[ 7  2]\n",
      "     [ 4  6]\n",
      "     [ 8  1]]]\n",
      "\n",
      "\n",
      "   [[[ 4  3]\n",
      "     [ 2  3]\n",
      "     [ 6  6]]\n",
      "\n",
      "    [[ 4  0]\n",
      "     [ 3  2]\n",
      "     [ 3  1]]]]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " [[[[[ 4  6]\n",
      "     [ 6  5]\n",
      "     [ 5  6]]\n",
      "\n",
      "    [[ 3  2]\n",
      "     [ 4  3]\n",
      "     [ 3  1]]]\n",
      "\n",
      "\n",
      "   [[[ 8  7]\n",
      "     [ 3  2]\n",
      "     [ 3  6]]\n",
      "\n",
      "    [[ 4  4]\n",
      "     [ 3  3]\n",
      "     [ 6  1]]]\n",
      "\n",
      "\n",
      "   [[[11 11]\n",
      "     [ 8  7]\n",
      "     [10 13]]\n",
      "\n",
      "    [[ 8  5]\n",
      "     [ 8  6]\n",
      "     [ 8  3]]]\n",
      "\n",
      "\n",
      "   [[[ 2  3]\n",
      "     [ 0  1]\n",
      "     [ 2  3]]\n",
      "\n",
      "    [[ 3  1]\n",
      "     [ 1  3]\n",
      "     [ 3  1]]]]\n",
      "\n",
      "\n",
      "\n",
      "  [[[[ 4  5]\n",
      "     [ 2  2]\n",
      "     [ 3  5]]\n",
      "\n",
      "    [[ 4  3]\n",
      "     [ 3  4]\n",
      "     [ 4  2]]]\n",
      "\n",
      "\n",
      "   [[[12 14]\n",
      "     [ 9  8]\n",
      "     [10 14]]\n",
      "\n",
      "    [[ 9  6]\n",
      "     [ 8  8]\n",
      "     [10  3]]]\n",
      "\n",
      "\n",
      "   [[[ 2  3]\n",
      "     [ 1  3]\n",
      "     [ 6  6]]\n",
      "\n",
      "    [[ 5  0]\n",
      "     [ 3  4]\n",
      "     [ 3  2]]]\n",
      "\n",
      "\n",
      "   [[[10 12]\n",
      "     [10 10]\n",
      "     [14 16]]\n",
      "\n",
      "    [[10  4]\n",
      "     [10  8]\n",
      "     [ 8  4]]]]\n",
      "\n",
      "\n",
      "\n",
      "  [[[[ 2  2]\n",
      "     [ 1  2]\n",
      "     [ 4  4]]\n",
      "\n",
      "    [[ 3  0]\n",
      "     [ 2  2]\n",
      "     [ 2  1]]]\n",
      "\n",
      "\n",
      "   [[[ 7  7]\n",
      "     [ 7  5]\n",
      "     [ 6  8]]\n",
      "\n",
      "    [[ 4  4]\n",
      "     [ 6  3]\n",
      "     [ 4  2]]]\n",
      "\n",
      "\n",
      "   [[[13 14]\n",
      "     [ 8  8]\n",
      "     [12 16]]\n",
      "\n",
      "    [[11  6]\n",
      "     [ 9  9]\n",
      "     [11  4]]]\n",
      "\n",
      "\n",
      "   [[[10  9]\n",
      "     [ 5  3]\n",
      "     [ 4  8]]\n",
      "\n",
      "    [[ 5  6]\n",
      "     [ 5  4]\n",
      "     [ 7  2]]]]]], shape=(2, 3, 4, 2, 3, 2), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(K.dot(a2,b2))#shape=(2,3,4,2,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 12, 4, 12, 8)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "w = K.variable(np.random.randint(10,size=(10,12,4,5)))\n",
    "k = K.variable(np.random.randint(10,size=(10,12,5,8)))\n",
    "# print(w)\n",
    "# print(k)\n",
    "z = K.batch_dot(w,k)\n",
    "print(z.shape) (10, 12, 4, 12, 8)\n",
    "#bahtch=10,后面的进行dot运算（12,4,5）*（12,5,8）=（12,4,12,8）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1 1]\n",
      "  [1 1]\n",
      "  [1 1]]\n",
      "\n",
      " [[1 1]\n",
      "  [1 1]\n",
      "  [1 1]]], shape=(2, 3, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[[2 2]\n",
      "  [2 2]\n",
      "  [2 2]]\n",
      "\n",
      " [[2 2]\n",
      "  [2 2]\n",
      "  [2 2]]], shape=(2, 3, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "s1=[\n",
    "    [[1],[1],[1]],\n",
    "    [[1],[1],[1]]\n",
    "]\n",
    "s1=tf.constant(s1)\n",
    "s11=K.tile(s1,(1,1,2))\n",
    "print(s11)\n",
    "s2=[\n",
    "    [[2,2]],\n",
    "    [[2,2]]\n",
    "]\n",
    "s22=K.tile(s2,(1,3,1))\n",
    "print(s22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=731, shape=(2, 3, 2), dtype=int32, numpy=\n",
       "array([[[3, 3],\n",
       "        [3, 3],\n",
       "        [3, 3]],\n",
       "\n",
       "       [[3, 3],\n",
       "        [3, 3],\n",
       "        [3, 3]]], dtype=int32)>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s11+s22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=825, shape=(1,), dtype=int32, numpy=array([5], dtype=int32)>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias=tf.constant(5,shape=([1]))\n",
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=827, shape=(2, 3, 2), dtype=int32, numpy=\n",
       "array([[[8, 8],\n",
       "        [8, 8],\n",
       "        [8, 8]],\n",
       "\n",
       "       [[8, 8],\n",
       "        [8, 8],\n",
       "        [8, 8]]], dtype=int32)>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#矩阵的每一个元素都会加上该一维的偏置\n",
    "s11+s22+bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[[1 2]\n",
      "  [3 4]]], shape=(1, 2, 2), dtype=int32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[[1 2]]\n",
      "\n",
      " [[3 4]]], shape=(2, 1, 2), dtype=int32) \n",
      "\n",
      "tf.Tensor(\n",
      "[[[1]\n",
      "  [2]]\n",
      "\n",
      " [[3]\n",
      "  [4]]], shape=(2, 2, 1), dtype=int32) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=tf.constant([[1,2],[3,4]])\n",
    "print(data,'\\n')\n",
    "print(tf.expand_dims(data, 0),'\\n')#第0维上加一个括号，增加一维度（1,2,2）\n",
    "print(tf.expand_dims(data, 1),'\\n')#第1维上加一个括号，增加一维度（2,1，2）\n",
    "print(tf.expand_dims(data, 2),'\\n')#第2维上加一个括号，增加一维度（2,2，1）已经是每一个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=847, shape=(2, 2), dtype=int32, numpy=\n",
       "array([[1, 2],\n",
       "       [3, 4]], dtype=int32)>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=855, shape=(2, 2), dtype=int32, numpy=\n",
       "array([[ 0, -1],\n",
       "       [-2, -3]], dtype=int32)>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=859, shape=(2, 2), dtype=int32, numpy=\n",
       "array([[  0,  -9],\n",
       "       [-18, -27]], dtype=int32)>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9*(1-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=897, shape=(2, 1, 7), dtype=float64, numpy=\n",
       "array([[[1., 1., 0., 0., 1., 1., 0.]],\n",
       "\n",
       "       [[1., 1., 1., 1., 1., 1., 0.]]])>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.random.randint(0,2,(2,7))\n",
    "a\n",
    "q_mask=tf.expand_dims(a,1)\n",
    "q_mask=tf.cast(q_mask,tf.double)\n",
    "q_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=904, shape=(2, 3, 7), dtype=float64, numpy=\n",
       "array([[[0.08124812, 0.47503315, 0.85238698, 0.23175964, 0.21702967,\n",
       "         0.93806323, 0.24411109],\n",
       "        [0.28786503, 0.58584822, 0.61423697, 0.74173244, 0.69138455,\n",
       "         0.14517381, 0.40864815],\n",
       "        [0.48104063, 0.91338028, 0.59380825, 0.72756885, 0.81353266,\n",
       "         0.18233884, 0.80596023]],\n",
       "\n",
       "       [[0.62325715, 0.74000245, 0.82670345, 0.5015219 , 0.98724012,\n",
       "         0.26058705, 0.74287924],\n",
       "        [0.21638141, 0.09648241, 0.43902492, 0.54732038, 0.40627565,\n",
       "         0.16509311, 0.99817473],\n",
       "        [0.83538681, 0.00468636, 0.74278758, 0.76083177, 0.83519068,\n",
       "         0.99024952, 0.62622862]]])>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=np.random.random((2,3,7))\n",
    "b.shape\n",
    "query=tf.constant(b)\n",
    "query\n",
    "'''\n",
    "array([[[1., 1., 0., 0., 1., 1., 0.]],\n",
    "\n",
    "       [[1., 1., 1., 1., 1., 1., 0.]]])>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=909, shape=(2, 3, 7), dtype=float64, numpy=\n",
       "array([[[ 8.12481161e-02,  4.75033149e-01, -1.00000000e+30,\n",
       "         -1.00000000e+30,  2.17029673e-01,  9.38063230e-01,\n",
       "         -1.00000000e+30],\n",
       "        [ 2.87865034e-01,  5.85848218e-01, -1.00000000e+30,\n",
       "         -1.00000000e+30,  6.91384549e-01,  1.45173814e-01,\n",
       "         -1.00000000e+30],\n",
       "        [ 4.81040628e-01,  9.13380278e-01, -1.00000000e+30,\n",
       "         -1.00000000e+30,  8.13532655e-01,  1.82338841e-01,\n",
       "         -1.00000000e+30]],\n",
       "\n",
       "       [[ 6.23257148e-01,  7.40002446e-01,  8.26703452e-01,\n",
       "          5.01521902e-01,  9.87240122e-01,  2.60587047e-01,\n",
       "         -1.00000000e+30],\n",
       "        [ 2.16381413e-01,  9.64824073e-02,  4.39024921e-01,\n",
       "          5.47320381e-01,  4.06275653e-01,  1.65093113e-01,\n",
       "         -1.00000000e+30],\n",
       "        [ 8.35386807e-01,  4.68636346e-03,  7.42787577e-01,\n",
       "          7.60831772e-01,  8.35190678e-01,  9.90249516e-01,\n",
       "         -1.00000000e+30]]])>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query+(-1e30)*(1-q_mask)\n",
    "##(2,13,7)  (2,1,7)  和*乘法对应元素相乘类似，这个是对应元素相加，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1018, shape=(2, 13, 7), dtype=float64, numpy=\n",
       "array([[[-5.77220136e+29, -5.39344373e+29, -3.93989925e+29,\n",
       "         -5.62971029e+29, -7.34045667e+29, -2.13850902e+29,\n",
       "         -3.81286764e+29],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00],\n",
       "        [-7.24665972e+29, -4.55407558e+29, -8.59643789e+29,\n",
       "         -9.33170317e+29, -3.39505085e+29, -9.73783035e+29,\n",
       "         -5.22979313e+29],\n",
       "        [-5.44946520e+29, -7.23897652e+29, -2.87831947e+29,\n",
       "         -1.22167872e+29, -7.56176650e+29, -6.17760891e+29,\n",
       "         -1.16573210e+29],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00],\n",
       "        [-5.95546367e+29, -8.31162309e+29, -5.12189527e+29,\n",
       "         -9.09793721e+29, -4.21982951e+29, -5.81124491e+29,\n",
       "         -7.76367643e+29],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00],\n",
       "        [-2.57224503e+29, -1.86568845e+29, -8.67236447e+29,\n",
       "         -2.93899692e+29, -8.30515506e+29, -4.01722291e+29,\n",
       "         -9.71961024e+29],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00],\n",
       "        [-6.07396947e+29, -8.32449571e+29, -8.70257214e+28,\n",
       "         -4.18056165e+29, -8.88047614e+29, -8.84626967e+28,\n",
       "         -8.81814893e+29]],\n",
       "\n",
       "       [[-5.86123838e+29, -4.25219099e+29, -1.95238734e+29,\n",
       "         -7.62619401e+29, -9.48566166e+29, -5.28126733e+29,\n",
       "         -6.24003649e+29],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00],\n",
       "        [-3.37337577e+29, -4.47691900e+29, -3.19890275e+29,\n",
       "         -2.86095535e+29, -1.59003848e+29, -2.40539098e+29,\n",
       "         -6.31951314e+29],\n",
       "        [-9.38844577e+29, -1.14262044e+29, -4.34154099e+29,\n",
       "         -5.15406873e+29, -7.21516953e+29, -6.24096337e+29,\n",
       "         -5.08393231e+29],\n",
       "        [-3.67410312e+27, -9.23669622e+29, -6.55953887e+29,\n",
       "         -5.23319514e+29, -5.78573544e+29, -1.17644796e+29,\n",
       "         -1.71990021e+29],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00],\n",
       "        [-3.71071395e+29, -2.92182122e+29, -3.51452309e+29,\n",
       "         -7.71214146e+29, -9.75800890e+29, -7.16022891e+29,\n",
       "         -7.47287157e+29],\n",
       "        [-9.82663800e+29, -7.84165423e+29, -1.93182600e+29,\n",
       "         -7.82371558e+29, -3.77141421e+29, -1.16642056e+29,\n",
       "         -8.12066214e+29],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00],\n",
       "        [-9.23924431e+29, -4.91191999e+29, -1.09450524e+29,\n",
       "         -3.70595493e+28, -2.65377938e+29, -6.73502204e+29,\n",
       "         -2.57998348e+29],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "         -0.00000000e+00]]])>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_input=query*(-1e30)*(1-q_mask)\n",
    "q_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.         0.         0.         0.         0.         1.\n",
      "   0.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.         0.         0.         0.         1.         0.\n",
      "   0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   1.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.         0.         0.         0.         1.         0.\n",
      "   0.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.         1.         0.         0.         0.         0.\n",
      "   0.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.         0.         1.         0.         0.         0.\n",
      "   0.        ]]\n",
      "\n",
      " [[0.         0.         1.         0.         0.         0.\n",
      "   0.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.         0.         0.         0.         1.         0.\n",
      "   0.        ]\n",
      "  [0.         1.         0.         0.         0.         0.\n",
      "   0.        ]\n",
      "  [1.         0.         0.         0.         0.         0.\n",
      "   0.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.         1.         0.         0.         0.         0.\n",
      "   0.        ]\n",
      "  [0.         0.         0.         0.         0.         1.\n",
      "   0.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.         0.         0.         1.         0.         0.\n",
      "   0.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]]], shape=(2, 13, 7), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[[0.         0.         0.         0.         0.         1.\n",
      "   0.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.         0.         0.         0.         1.         0.\n",
      "   0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   1.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.         0.         0.         0.         1.         0.\n",
      "   0.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.         1.         0.         0.         0.         0.\n",
      "   0.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.         0.         1.         0.         0.         0.\n",
      "   0.        ]]\n",
      "\n",
      " [[0.         0.         1.         0.         0.         0.\n",
      "   0.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.         0.         0.         0.         1.         0.\n",
      "   0.        ]\n",
      "  [0.         1.         0.         0.         0.         0.\n",
      "   0.        ]\n",
      "  [1.         0.         0.         0.         0.         0.\n",
      "   0.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.         1.         0.         0.         0.         0.\n",
      "   0.        ]\n",
      "  [0.         0.         0.         0.         0.         1.\n",
      "   0.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.         0.         0.         1.         0.         0.\n",
      "   0.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]]], shape=(2, 13, 7), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[[0.         0.         0.         0.         0.         1.\n",
      "   0.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.         0.         0.         0.         1.         0.\n",
      "   0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.\n",
      "   1.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.         0.         0.         0.         1.         0.\n",
      "   0.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.         1.         0.         0.         0.         0.\n",
      "   0.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.         0.         1.         0.         0.         0.\n",
      "   0.        ]]\n",
      "\n",
      " [[0.         0.         1.         0.         0.         0.\n",
      "   0.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.         0.         0.         0.         1.         0.\n",
      "   0.        ]\n",
      "  [0.         1.         0.         0.         0.         0.\n",
      "   0.        ]\n",
      "  [1.         0.         0.         0.         0.         0.\n",
      "   0.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.         1.         0.         0.         0.         0.\n",
      "   0.        ]\n",
      "  [0.         0.         0.         0.         0.         1.\n",
      "   0.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]\n",
      "  [0.         0.         0.         1.         0.         0.\n",
      "   0.        ]\n",
      "  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      "   0.14285714]]], shape=(2, 13, 7), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "#The default is -1 which indicates the last dimension.\n",
    "print( tf.nn.softmax(q_input))\n",
    "print( tf.nn.softmax(q_input,axis=-1))\n",
    "print( tf.nn.softmax(q_input,axis=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1110, shape=(2, 4, 3), dtype=float64, numpy=\n",
       "array([[[0.37708031, 0.42404427, 0.55021036],\n",
       "        [0.03604717, 0.56437762, 0.00955075],\n",
       "        [0.80728975, 0.25549949, 0.95188034],\n",
       "        [0.42733193, 0.79561552, 0.91994308]],\n",
       "\n",
       "       [[0.80195553, 0.08423908, 0.96791519],\n",
       "        [0.60823064, 0.21005072, 0.15458767],\n",
       "        [0.00286257, 0.31676107, 0.87643974],\n",
       "        [0.09298468, 0.54308661, 0.48699636]]])>"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dd=[\n",
    "#     [\n",
    "#       [1,1,1]   ,\n",
    "#         [2,2,2],\n",
    "#         [3,3,3],\n",
    "#         [4,4,4]\n",
    "#     ],\n",
    "#     [\n",
    "#          [1,1,1]   ,\n",
    "#         [2,2,2],\n",
    "#         [3,3,3],\n",
    "#         [4,4,4]\n",
    "        \n",
    "#     ]\n",
    "# ]\n",
    "dd=np.random.random((2,4,3))\n",
    "dd=tf.constant(dd)\n",
    "dd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1111, shape=(2, 4, 3), dtype=float64, numpy=\n",
       "array([[[0.308918  , 0.32377209, 0.36730991],\n",
       "        [0.2724833 , 0.46215843, 0.26535827],\n",
       "        [0.36610133, 0.21084429, 0.42305438],\n",
       "        [0.2449878 , 0.35406907, 0.40094312]],\n",
       "\n",
       "       [[0.37475774, 0.1828312 , 0.44241106],\n",
       "        [0.43349142, 0.29110735, 0.27540123],\n",
       "        [0.20989815, 0.28729827, 0.50280358],\n",
       "        [0.24682887, 0.38714419, 0.36602694]]])>"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(dd)##似乎dd值范围在0-1之间才能做softmax操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum([0.308918  , 0.32377209, 0.36730991])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行和 1.0\n",
      "列和 1.19249043\n"
     ]
    }
   ],
   "source": [
    "# 默认最后一维度：此时是行，行和为1,列和不为1\n",
    "print('行和',sum([0.308918 , 0.32377209, 0.36730991]))\n",
    "print('列和',sum([0.308918  ,0.2724833,0.36610133,0.2449878 ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1142, shape=(2, 4, 3), dtype=float64, numpy=\n",
       "array([[[0.23255   , 0.22494344, 0.22104187],\n",
       "        [0.16535127, 0.25883282, 0.12872683],\n",
       "        [0.35756412, 0.19005324, 0.33030686],\n",
       "        [0.24453461, 0.3261705 , 0.31992444]],\n",
       "\n",
       "       [[0.36156275, 0.20088001, 0.33622723],\n",
       "        [0.29788595, 0.2278117 , 0.14907652],\n",
       "        [0.16260804, 0.25346601, 0.3068355 ],\n",
       "        [0.17794326, 0.31784228, 0.20786075]]])>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(dd,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "列和 0.9999999999999999\n",
      "行和 0.67853531\n"
     ]
    }
   ],
   "source": [
    "print('列和',sum([0.23255,0.16535127,0.35756412,0.24453461]))\n",
    "print('行和',sum([0.23255   , 0.22494344, 0.22104187]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=989, shape=(2, 13, 1), dtype=float64, numpy=\n",
       "array([[[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.]],\n",
       "\n",
       "       [[0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]]])>"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.random.randint(0,2,(2,13))\n",
    "a\n",
    "c_mask=tf.expand_dims(a,2)\n",
    "c_mask=tf.cast(q_mask,tf.double)\n",
    "c_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=994, shape=(2, 13, 7), dtype=float64, numpy=\n",
       "array([[[0.14414024, 0.07553769, 0.26281012, 0.76483462, 0.94715607,\n",
       "         0.88912307, 0.13794227],\n",
       "        [0.75449063, 0.61372398, 0.0456633 , 0.34835367, 0.6535259 ,\n",
       "         0.40428802, 0.11426278],\n",
       "        [0.52524679, 0.00142671, 0.18282409, 0.13555104, 0.94660706,\n",
       "         0.21750855, 0.66793793],\n",
       "        [0.44959093, 0.74552392, 0.65499016, 0.07548763, 0.36425376,\n",
       "         0.33949416, 0.51383536],\n",
       "        [0.46405552, 0.30970269, 0.85808352, 0.15152728, 0.09734747,\n",
       "         0.15237416, 0.07018381],\n",
       "        [0.82344076, 0.77333663, 0.87814143, 0.76586831, 0.23241264,\n",
       "         0.68544789, 0.58820174],\n",
       "        [0.03880519, 0.05872777, 0.71441885, 0.0889644 , 0.04675575,\n",
       "         0.25039672, 0.05565358],\n",
       "        [0.21918458, 0.43129876, 0.05285348, 0.38738854, 0.11087829,\n",
       "         0.53785992, 0.32438123],\n",
       "        [0.12593844, 0.87967976, 0.4162448 , 0.39884067, 0.39818171,\n",
       "         0.60424485, 0.12112703],\n",
       "        [0.12771692, 0.0489746 , 0.7798664 , 0.67834259, 0.1394599 ,\n",
       "         0.49540566, 0.15637613],\n",
       "        [0.1796364 , 0.8910519 , 0.95884695, 0.89217567, 0.67488297,\n",
       "         0.12415049, 0.83995446],\n",
       "        [0.32843641, 0.51426574, 0.18474491, 0.24856081, 0.88138243,\n",
       "         0.14836483, 0.45430575],\n",
       "        [0.84033751, 0.08437491, 0.03393738, 0.22492647, 0.45904874,\n",
       "         0.90833628, 0.33410669]],\n",
       "\n",
       "       [[0.73626752, 0.38316424, 0.12104114, 0.36796301, 0.48866055,\n",
       "         0.45232767, 0.08470767],\n",
       "        [0.17631595, 0.2582124 , 0.20699559, 0.68953753, 0.95208879,\n",
       "         0.97716478, 0.67946473],\n",
       "        [0.08470899, 0.16810159, 0.5030419 , 0.54741094, 0.06258651,\n",
       "         0.2606233 , 0.95518857],\n",
       "        [0.41445141, 0.24961128, 0.06750267, 0.82652401, 0.61304456,\n",
       "         0.39200246, 0.44856729],\n",
       "        [0.32410941, 0.70156905, 0.74299627, 0.1098404 , 0.33513138,\n",
       "         0.2334868 , 0.26547993],\n",
       "        [0.86247414, 0.44568769, 0.55348763, 0.71195757, 0.14622433,\n",
       "         0.57867228, 0.42270036],\n",
       "        [0.35165633, 0.32599123, 0.22175   , 0.82526127, 0.18231854,\n",
       "         0.14523742, 0.18830542],\n",
       "        [0.10244034, 0.29300706, 0.44480569, 0.82885029, 0.4957129 ,\n",
       "         0.22185583, 0.78295483],\n",
       "        [0.66575783, 0.23155892, 0.9011133 , 0.49411697, 0.0257682 ,\n",
       "         0.1418148 , 0.46857595],\n",
       "        [0.73788301, 0.94712168, 0.99286088, 0.62663115, 0.37685972,\n",
       "         0.42652569, 0.79084602],\n",
       "        [0.99358578, 0.67849887, 0.69787812, 0.55053607, 0.23528687,\n",
       "         0.22415711, 0.11119817],\n",
       "        [0.4641917 , 0.76992933, 0.08217185, 0.34269304, 0.74283775,\n",
       "         0.12256779, 0.05027895],\n",
       "        [0.59399353, 0.10836448, 0.73206586, 0.51007127, 0.01739159,\n",
       "         0.59729763, 0.51201976]]])>"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "b=np.random.random((2,13,7))\n",
    "b.shape\n",
    "context=tf.constant(b)\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1000, shape=(2, 13, 7), dtype=float64, numpy=\n",
       "array([[[-1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30],\n",
       "        [ 7.54490631e-01,  6.13723977e-01,  4.56632963e-02,\n",
       "          3.48353666e-01,  6.53525898e-01,  4.04288024e-01,\n",
       "          1.14262780e-01],\n",
       "        [-1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30],\n",
       "        [-1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30],\n",
       "        [ 4.64055521e-01,  3.09702694e-01,  8.58083521e-01,\n",
       "          1.51527275e-01,  9.73474724e-02,  1.52374158e-01,\n",
       "          7.01838116e-02],\n",
       "        [-1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30],\n",
       "        [ 3.88051887e-02,  5.87277735e-02,  7.14418851e-01,\n",
       "          8.89643977e-02,  4.67557464e-02,  2.50396719e-01,\n",
       "          5.56535758e-02],\n",
       "        [-1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30],\n",
       "        [ 1.25938443e-01,  8.79679763e-01,  4.16244799e-01,\n",
       "          3.98840666e-01,  3.98181707e-01,  6.04244846e-01,\n",
       "          1.21127033e-01],\n",
       "        [ 1.27716917e-01,  4.89746008e-02,  7.79866399e-01,\n",
       "          6.78342587e-01,  1.39459903e-01,  4.95405660e-01,\n",
       "          1.56376126e-01],\n",
       "        [ 1.79636401e-01,  8.91051903e-01,  9.58846945e-01,\n",
       "          8.92175673e-01,  6.74882967e-01,  1.24150491e-01,\n",
       "          8.39954465e-01],\n",
       "        [ 3.28436412e-01,  5.14265735e-01,  1.84744912e-01,\n",
       "          2.48560814e-01,  8.81382425e-01,  1.48364827e-01,\n",
       "          4.54305747e-01],\n",
       "        [-1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30]],\n",
       "\n",
       "       [[-1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30],\n",
       "        [ 1.76315949e-01,  2.58212402e-01,  2.06995592e-01,\n",
       "          6.89537534e-01,  9.52088791e-01,  9.77164780e-01,\n",
       "          6.79464725e-01],\n",
       "        [ 8.47089866e-02,  1.68101594e-01,  5.03041905e-01,\n",
       "          5.47410938e-01,  6.25865090e-02,  2.60623302e-01,\n",
       "          9.55188572e-01],\n",
       "        [-1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30],\n",
       "        [-1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30],\n",
       "        [-1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30],\n",
       "        [ 3.51656327e-01,  3.25991231e-01,  2.21749999e-01,\n",
       "          8.25261272e-01,  1.82318540e-01,  1.45237425e-01,\n",
       "          1.88305419e-01],\n",
       "        [-1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30],\n",
       "        [-1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30],\n",
       "        [ 7.37883009e-01,  9.47121676e-01,  9.92860882e-01,\n",
       "          6.26631148e-01,  3.76859718e-01,  4.26525685e-01,\n",
       "          7.90846018e-01],\n",
       "        [ 9.93585777e-01,  6.78498875e-01,  6.97878117e-01,\n",
       "          5.50536070e-01,  2.35286874e-01,  2.24157113e-01,\n",
       "          1.11198168e-01],\n",
       "        [-1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30, -1.00000000e+30, -1.00000000e+30,\n",
       "         -1.00000000e+30],\n",
       "        [ 5.93993530e-01,  1.08364481e-01,  7.32065863e-01,\n",
       "          5.10071267e-01,  1.73915878e-02,  5.97297633e-01,\n",
       "          5.12019763e-01]]])>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context+(-1e30)*(1-c_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=989, shape=(2, 13, 1), dtype=float64, numpy=\n",
       "array([[[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.]],\n",
       "\n",
       "       [[0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.]]])>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.2345974  0.99016678 0.95038503 0.20807637 0.44327811 0.47392734\n",
      "   0.44124904]\n",
      "  [0.48196546 0.51911287 0.46693383 0.93386459 0.87218028 0.29846174\n",
      "   0.38130826]\n",
      "  [0.73619398 0.7637975  0.92156318 0.05454127 0.214883   0.59725102\n",
      "   0.26594292]\n",
      "  [0.15223345 0.09367253 0.8657996  0.73453137 0.47255408 0.85502357\n",
      "   0.36367682]\n",
      "  [0.40599337 0.70839731 0.15275264 0.79377885 0.24467067 0.00760953\n",
      "   0.97374171]\n",
      "  [0.75726946 0.28781554 0.46203929 0.69097858 0.36371387 0.31812274\n",
      "   0.97690111]\n",
      "  [0.44663725 0.99504392 0.28164576 0.34080975 0.52307026 0.811288\n",
      "   0.87350683]\n",
      "  [0.17711269 0.65596359 0.92192487 0.68777076 0.04106203 0.20217109\n",
      "   0.77373601]\n",
      "  [0.8458154  0.32186447 0.24668871 0.74743889 0.66900351 0.72569252\n",
      "   0.02872936]\n",
      "  [0.70339635 0.80493022 0.40129723 0.77347677 0.23708378 0.80376155\n",
      "   0.59911753]\n",
      "  [0.83732021 0.19098813 0.18545181 0.0158981  0.00346169 0.48796876\n",
      "   0.38073926]\n",
      "  [0.80699719 0.30041815 0.51116373 0.13431669 0.09109613 0.59340355\n",
      "   0.94496581]\n",
      "  [0.78155948 0.934585   0.85834826 0.67011797 0.66176985 0.12706776\n",
      "   0.94862216]]], shape=(1, 13, 7), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[[0.04611949 0.50021704 0.3403836  0.96546213 0.82286535 0.36938872\n",
      "   0.69369115]\n",
      "  [0.57008254 0.11770827 0.02108898 0.28998957 0.09547596 0.55883485\n",
      "   0.20014567]\n",
      "  [0.01261671 0.37396073 0.20858714 0.64812165 0.45951597 0.96764561\n",
      "   0.97459381]\n",
      "  [0.01074766 0.12664143 0.59025763 0.56242738 0.15021628 0.5033446\n",
      "   0.09289908]\n",
      "  [0.39154437 0.2280868  0.84650096 0.46942739 0.64514535 0.45458149\n",
      "   0.81874023]\n",
      "  [0.75101136 0.46010769 0.72529368 0.98679036 0.45647061 0.68155152\n",
      "   0.42391932]\n",
      "  [0.53208319 0.13796865 0.91342658 0.39940756 0.77175872 0.92357541\n",
      "   0.13253102]\n",
      "  [0.39362317 0.42371683 0.95218488 0.54612439 0.27985545 0.00938231\n",
      "   0.33741922]\n",
      "  [0.69189939 0.8850581  0.0195934  0.94490876 0.8061132  0.81666133\n",
      "   0.51404781]\n",
      "  [0.25133106 0.54822535 0.20320511 0.79584467 0.76374255 0.15845911\n",
      "   0.41881988]\n",
      "  [0.86696025 0.95476847 0.93973109 0.83573378 0.1031451  0.16092307\n",
      "   0.15905384]\n",
      "  [0.89019774 0.60267669 0.79061232 0.9216989  0.13737488 0.99339279\n",
      "   0.14715429]\n",
      "  [0.91930361 0.28355766 0.14417453 0.4581707  0.92918062 0.213815\n",
      "   0.52586712]]\n",
      "\n",
      " [[0.16867095 0.13985064 0.19748195 0.18865689 0.52872261 0.74899586\n",
      "   0.80452809]\n",
      "  [0.88024387 0.75988372 0.21060264 0.05782449 0.11227084 0.05550571\n",
      "   0.35737175]\n",
      "  [0.48211917 0.03464259 0.88369622 0.05865621 0.4372654  0.92554184\n",
      "   0.24923703]\n",
      "  [0.33409216 0.89408284 0.24601316 0.89952594 0.90015189 0.19785821\n",
      "   0.88170577]\n",
      "  [0.10314258 0.46254315 0.45692164 0.85066256 0.78599654 0.74269394\n",
      "   0.25194093]\n",
      "  [0.20780629 0.4408922  0.65776498 0.07733062 0.6316213  0.69382403\n",
      "   0.50614533]\n",
      "  [0.36665525 0.36215116 0.11044192 0.4326926  0.33990768 0.05712641\n",
      "   0.82862534]\n",
      "  [0.39392852 0.40398002 0.03171907 0.9315789  0.36330234 0.10029709\n",
      "   0.81511838]\n",
      "  [0.33451878 0.36940085 0.95781909 0.70291539 0.53957152 0.77623545\n",
      "   0.13398049]\n",
      "  [0.1887714  0.92129375 0.3855264  0.75379109 0.4795572  0.27051664\n",
      "   0.68507559]\n",
      "  [0.20088371 0.1099287  0.86312973 0.79678089 0.95590148 0.29941438\n",
      "   0.96623807]\n",
      "  [0.92888123 0.45644685 0.6950294  0.50813796 0.07635308 0.58140873\n",
      "   0.91403061]\n",
      "  [0.25034768 0.54682751 0.48241593 0.60795559 0.39798348 0.79527321\n",
      "   0.89651974]]], shape=(2, 13, 7), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "b1=np.random.random((1,13,7))\n",
    "b.shape\n",
    "b1=tf.constant(b1)\n",
    "\n",
    "\n",
    "\n",
    "c1=np.random.random((2,13,7))\n",
    "\n",
    "c1=tf.constant(c1)\n",
    "print(b1)\n",
    "print(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1007, shape=(2, 13, 7), dtype=float64, numpy=\n",
       "array([[[0.28071689, 1.49038381, 1.29076863, 1.17353851, 1.26614346,\n",
       "         0.84331606, 1.13494019],\n",
       "        [1.05204801, 0.63682114, 0.48802282, 1.22385416, 0.96765624,\n",
       "         0.85729659, 0.58145393],\n",
       "        [0.74881069, 1.13775822, 1.13015031, 0.70266292, 0.67439897,\n",
       "         1.56489663, 1.24053673],\n",
       "        [0.16298112, 0.22031395, 1.45605722, 1.29695875, 0.62277036,\n",
       "         1.35836817, 0.4565759 ],\n",
       "        [0.79753774, 0.9364841 , 0.9992536 , 1.26320623, 0.88981602,\n",
       "         0.46219102, 1.79248194],\n",
       "        [1.50828082, 0.74792323, 1.18733297, 1.67776894, 0.82018448,\n",
       "         0.99967426, 1.40082042],\n",
       "        [0.97872044, 1.13301257, 1.19507234, 0.74021732, 1.29482898,\n",
       "         1.73486341, 1.00603785],\n",
       "        [0.57073586, 1.07968042, 1.87410975, 1.23389515, 0.32091749,\n",
       "         0.2115534 , 1.11115523],\n",
       "        [1.53771479, 1.20692257, 0.26628211, 1.69234766, 1.47511671,\n",
       "         1.54235385, 0.54277716],\n",
       "        [0.95472741, 1.35315557, 0.60450233, 1.56932144, 1.00082633,\n",
       "         0.96222065, 1.01793741],\n",
       "        [1.70428045, 1.1457566 , 1.1251829 , 0.85163188, 0.1066068 ,\n",
       "         0.64889183, 0.53979309],\n",
       "        [1.69719493, 0.90309484, 1.30177605, 1.05601559, 0.22847101,\n",
       "         1.58679634, 1.0921201 ],\n",
       "        [1.70086309, 1.21814266, 1.00252279, 1.12828867, 1.59095046,\n",
       "         0.34088277, 1.47448928]],\n",
       "\n",
       "       [[0.40326835, 1.13001741, 1.14786697, 0.39673326, 0.97200072,\n",
       "         1.22292321, 1.24577713],\n",
       "        [1.36220934, 1.27899659, 0.67753647, 0.99168908, 0.98445112,\n",
       "         0.35396744, 0.73868001],\n",
       "        [1.21831316, 0.79844009, 1.8052594 , 0.11319748, 0.6521484 ,\n",
       "         1.52279286, 0.51517995],\n",
       "        [0.48632561, 0.98775537, 1.11181276, 1.63405731, 1.37270597,\n",
       "         1.05288178, 1.24538259],\n",
       "        [0.50913595, 1.17094046, 0.60967428, 1.64444141, 1.03066721,\n",
       "         0.75030346, 1.22568265],\n",
       "        [0.96507575, 0.72870773, 1.11980427, 0.7683092 , 0.99533517,\n",
       "         1.01194678, 1.48304644],\n",
       "        [0.8132925 , 1.35719508, 0.39208768, 0.77350236, 0.86297794,\n",
       "         0.86841441, 1.70213217],\n",
       "        [0.57104121, 1.05994361, 0.95364393, 1.61934967, 0.40436437,\n",
       "         0.30246818, 1.58885439],\n",
       "        [1.18033418, 0.69126532, 1.2045078 , 1.45035428, 1.20857503,\n",
       "         1.50192797, 0.16270985],\n",
       "        [0.89216775, 1.72622397, 0.78682363, 1.52726786, 0.71664098,\n",
       "         1.07427818, 1.28419313],\n",
       "        [1.03820392, 0.30091683, 1.04858154, 0.81267899, 0.95936317,\n",
       "         0.78738314, 1.34697733],\n",
       "        [1.73587842, 0.756865  , 1.20619314, 0.64245466, 0.16744921,\n",
       "         1.17481228, 1.85899642],\n",
       "        [1.03190716, 1.48141251, 1.34076419, 1.27807356, 1.05975333,\n",
       "         0.92234097, 1.8451419 ]]])>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1+c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1008, shape=(2, 13, 7), dtype=float64, numpy=\n",
       "array([[[0.09223898, 1.00043407, 0.68076721, 1.93092427, 1.6457307 ,\n",
       "         0.73877743, 1.3873823 ],\n",
       "        [1.14016509, 0.23541654, 0.04217797, 0.57997914, 0.19095191,\n",
       "         1.11766971, 0.40029134],\n",
       "        [0.02523342, 0.74792145, 0.41717427, 1.2962433 , 0.91903195,\n",
       "         1.93529122, 1.94918762],\n",
       "        [0.02149533, 0.25328285, 1.18051525, 1.12485477, 0.30043256,\n",
       "         1.00668921, 0.18579815],\n",
       "        [0.78308874, 0.45617359, 1.69300192, 0.93885477, 1.2902907 ,\n",
       "         0.90916299, 1.63748045],\n",
       "        [1.50202273, 0.92021538, 1.45058737, 1.97358072, 0.91294122,\n",
       "         1.36310305, 0.84783863],\n",
       "        [1.06416637, 0.2759373 , 1.82685315, 0.79881513, 1.54351744,\n",
       "         1.84715081, 0.26506204],\n",
       "        [0.78724634, 0.84743365, 1.90436976, 1.09224878, 0.5597109 ,\n",
       "         0.01876462, 0.67483844],\n",
       "        [1.38379878, 1.77011619, 0.0391868 , 1.88981753, 1.6122264 ,\n",
       "         1.63332266, 1.02809561],\n",
       "        [0.50266211, 1.09645071, 0.40641021, 1.59168935, 1.5274851 ,\n",
       "         0.31691821, 0.83763975],\n",
       "        [1.73392049, 1.90953694, 1.87946219, 1.67146756, 0.2062902 ,\n",
       "         0.32184614, 0.31810767],\n",
       "        [1.78039549, 1.20535338, 1.58122464, 1.84339779, 0.27474976,\n",
       "         1.98678558, 0.29430858],\n",
       "        [1.83860722, 0.56711532, 0.28834906, 0.91634139, 1.85836123,\n",
       "         0.42763   , 1.05173425]],\n",
       "\n",
       "       [[0.33734189, 0.27970127, 0.3949639 , 0.37731378, 1.05744522,\n",
       "         1.49799172, 1.60905617],\n",
       "        [1.76048775, 1.51976744, 0.42120529, 0.11564897, 0.22454168,\n",
       "         0.11101141, 0.71474351],\n",
       "        [0.96423835, 0.06928519, 1.76739245, 0.11731242, 0.87453081,\n",
       "         1.85108368, 0.49847406],\n",
       "        [0.66818431, 1.78816569, 0.49202632, 1.79905188, 1.80030379,\n",
       "         0.39571643, 1.76341153],\n",
       "        [0.20628516, 0.92508631, 0.91384329, 1.70132512, 1.57199308,\n",
       "         1.48538787, 0.50388187],\n",
       "        [0.41561257, 0.8817844 , 1.31552996, 0.15466124, 1.26324259,\n",
       "         1.38764807, 1.01229066],\n",
       "        [0.7333105 , 0.72430231, 0.22088384, 0.86538521, 0.67981536,\n",
       "         0.11425282, 1.65725067],\n",
       "        [0.78785704, 0.80796004, 0.06343813, 1.8631578 , 0.72660467,\n",
       "         0.20059419, 1.63023676],\n",
       "        [0.66903755, 0.7388017 , 1.91563818, 1.40583078, 1.07914305,\n",
       "         1.55247089, 0.26796099],\n",
       "        [0.37754281, 1.8425875 , 0.7710528 , 1.50758218, 0.9591144 ,\n",
       "         0.54103327, 1.37015118],\n",
       "        [0.40176742, 0.2198574 , 1.72625946, 1.59356178, 1.91180296,\n",
       "         0.59882877, 1.93247615],\n",
       "        [1.85776246, 0.9128937 , 1.39005881, 1.01627592, 0.15270616,\n",
       "         1.16281747, 1.82806122],\n",
       "        [0.50069536, 1.09365502, 0.96483187, 1.21591118, 0.79596696,\n",
       "         1.59054642, 1.79303949]]])>"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1+c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.05189186 0.57373296 0.85868265 0.79030928 0.85280103 0.66101582\n",
      "   0.63403636]\n",
      "  [0.8695273  0.68982408 0.08795359 0.13052773 0.85999763 0.83530149\n",
      "   0.28623198]\n",
      "  [0.04167173 0.76463422 0.02382539 0.00905678 0.22311752 0.33632955\n",
      "   0.09087339]\n",
      "  [0.3889636  0.57735477 0.49362672 0.84835808 0.89259879 0.21171404\n",
      "   0.069402  ]\n",
      "  [0.81660577 0.60803432 0.80427019 0.34063395 0.19900027 0.2748498\n",
      "   0.29842084]\n",
      "  [0.182249   0.5555543  0.08680516 0.28549876 0.78486663 0.93345935\n",
      "   0.72238526]\n",
      "  [0.35460273 0.76921833 0.64024754 0.77935438 0.08676843 0.13062564\n",
      "   0.19046093]\n",
      "  [0.45642587 0.00632118 0.67876556 0.19980604 0.27398114 0.48367395\n",
      "   0.97303948]\n",
      "  [0.38177813 0.66375088 0.80313084 0.6363751  0.13577629 0.49214293\n",
      "   0.02878798]\n",
      "  [0.4565323  0.48687081 0.18273431 0.30935462 0.94963889 0.89623636\n",
      "   0.24586661]\n",
      "  [0.04558644 0.77962314 0.38462485 0.62221887 0.21821605 0.99159983\n",
      "   0.69885648]\n",
      "  [0.17839067 0.2633969  0.77694347 0.38937209 0.32952864 0.78799096\n",
      "   0.07689172]\n",
      "  [0.6213315  0.68552333 0.58571614 0.20700671 0.82416321 0.452763\n",
      "   0.86721499]\n",
      "  [0.18709669 0.12861466 0.84622203 0.56373282 0.68667219 0.40152607\n",
      "   0.34751622]]\n",
      "\n",
      " [[0.05909793 0.70967765 0.41254473 0.4809297  0.13757362 0.01926973\n",
      "   0.06501545]\n",
      "  [0.84551234 0.1894887  0.23182803 0.36572018 0.68595559 0.4102058\n",
      "   0.54560868]\n",
      "  [0.81875557 0.44715245 0.68205005 0.09570933 0.30431941 0.24347533\n",
      "   0.7826625 ]\n",
      "  [0.86148849 0.34317327 0.15109122 0.63684197 0.20917234 0.30997593\n",
      "   0.84098104]\n",
      "  [0.70776683 0.08946629 0.66445367 0.54414382 0.57950457 0.85067833\n",
      "   0.51779426]\n",
      "  [0.29641374 0.65826737 0.2255196  0.707686   0.91629899 0.91020409\n",
      "   0.91037476]\n",
      "  [0.11623965 0.26774041 0.84086615 0.64364907 0.82619064 0.68003647\n",
      "   0.33267598]\n",
      "  [0.4783834  0.64230471 0.63274554 0.31124725 0.3148329  0.338212\n",
      "   0.48558664]\n",
      "  [0.53796655 0.3586055  0.08923783 0.39124027 0.41654916 0.04328198\n",
      "   0.37054694]\n",
      "  [0.47313921 0.77687655 0.33794856 0.92156874 0.73693178 0.13044529\n",
      "   0.65332219]\n",
      "  [0.11838695 0.0285772  0.13346138 0.48011507 0.83263317 0.77438376\n",
      "   0.06952065]\n",
      "  [0.67486453 0.76582697 0.91310985 0.16248355 0.01914911 0.89793993\n",
      "   0.91716924]\n",
      "  [0.2816877  0.06641354 0.53042322 0.50532986 0.60074305 0.28768232\n",
      "   0.06437681]\n",
      "  [0.10739674 0.92239416 0.22394745 0.09832651 0.58809637 0.82200774\n",
      "   0.17032894]]], shape=(2, 14, 7), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[[0.82837128 0.9669736  0.18628684 0.99693635 0.98462081 0.84002105\n",
      "   0.64245942]]\n",
      "\n",
      " [[0.67052435 0.28882688 0.16733855 0.26697624 0.56799119 0.09375318\n",
      "   0.98751145]]], shape=(2, 1, 7), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "b2=np.random.random((2,14,7))\n",
    "\n",
    "b2=tf.constant(b2)\n",
    "\n",
    "\n",
    "\n",
    "c2=np.random.random((2,1,7))\n",
    "\n",
    "c2=tf.constant(c2)\n",
    "print(b2)\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1149, shape=(2, 14, 7), dtype=float64, numpy=\n",
       "array([[[0.88026314, 1.54070656, 1.0449695 , 1.78724563, 1.83742184,\n",
       "         1.50103686, 1.27649577],\n",
       "        [1.69789858, 1.65679768, 0.27424044, 1.12746408, 1.84461844,\n",
       "         1.67532253, 0.9286914 ],\n",
       "        [0.87004301, 1.73160781, 0.21011224, 1.00599313, 1.20773833,\n",
       "         1.1763506 , 0.7333328 ],\n",
       "        [1.21733488, 1.54432837, 0.67991356, 1.84529443, 1.8772196 ,\n",
       "         1.05173508, 0.71186142],\n",
       "        [1.64497704, 1.57500791, 0.99055704, 1.3375703 , 1.18362108,\n",
       "         1.11487085, 0.94088025],\n",
       "        [1.01062028, 1.5225279 , 0.273092  , 1.28243511, 1.76948744,\n",
       "         1.7734804 , 1.36484468],\n",
       "        [1.18297401, 1.73619193, 0.82653439, 1.77629073, 1.07138924,\n",
       "         0.97064669, 0.83292035],\n",
       "        [1.28479715, 0.97329478, 0.8650524 , 1.19674239, 1.25860195,\n",
       "         1.32369499, 1.6154989 ],\n",
       "        [1.21014941, 1.63072448, 0.98941768, 1.63331145, 1.1203971 ,\n",
       "         1.33216398, 0.6712474 ],\n",
       "        [1.28490358, 1.45384441, 0.36902115, 1.30629097, 1.9342597 ,\n",
       "         1.73625741, 0.88832603],\n",
       "        [0.87395772, 1.74659674, 0.5709117 , 1.61915522, 1.20283686,\n",
       "         1.83162088, 1.34131589],\n",
       "        [1.00676195, 1.2303705 , 0.96323031, 1.38630844, 1.31414945,\n",
       "         1.62801201, 0.71935113],\n",
       "        [1.44970278, 1.65249693, 0.77200299, 1.20394306, 1.80878402,\n",
       "         1.29278405, 1.5096744 ],\n",
       "        [1.01546797, 1.09558826, 1.03250887, 1.56066917, 1.671293  ,\n",
       "         1.24154711, 0.98997564]],\n",
       "\n",
       "       [[0.72962228, 0.99850453, 0.57988328, 0.74790594, 0.70556481,\n",
       "         0.11302291, 1.0525269 ],\n",
       "        [1.51603669, 0.47831558, 0.39916658, 0.63269642, 1.25394678,\n",
       "         0.50395899, 1.53312013],\n",
       "        [1.48927992, 0.73597933, 0.8493886 , 0.36268557, 0.8723106 ,\n",
       "         0.33722852, 1.77017395],\n",
       "        [1.53201284, 0.63200015, 0.31842977, 0.90381821, 0.77716353,\n",
       "         0.40372912, 1.82849248],\n",
       "        [1.37829118, 0.37829317, 0.83179221, 0.81112005, 1.14749576,\n",
       "         0.94443151, 1.50530571],\n",
       "        [0.96693809, 0.94709424, 0.39285815, 0.97466224, 1.48429019,\n",
       "         1.00395728, 1.89788621],\n",
       "        [0.78676399, 0.55656729, 1.0082047 , 0.91062531, 1.39418183,\n",
       "         0.77378965, 1.32018743],\n",
       "        [1.14890775, 0.93113158, 0.80008408, 0.57822349, 0.88282409,\n",
       "         0.43196518, 1.47309809],\n",
       "        [1.2084909 , 0.64743238, 0.25657638, 0.65821651, 0.98454035,\n",
       "         0.13703516, 1.35805838],\n",
       "        [1.14366356, 1.06570343, 0.50528711, 1.18854498, 1.30492297,\n",
       "         0.22419848, 1.64083363],\n",
       "        [0.7889113 , 0.31740408, 0.30079992, 0.74709131, 1.40062436,\n",
       "         0.86813694, 1.0570321 ],\n",
       "        [1.34538888, 1.05465385, 1.0804484 , 0.42945979, 0.5871403 ,\n",
       "         0.99169311, 1.90468069],\n",
       "        [0.95221205, 0.35524042, 0.69776177, 0.7723061 , 1.16873424,\n",
       "         0.3814355 , 1.05188825],\n",
       "        [0.77792109, 1.21122104, 0.39128599, 0.36530275, 1.15608756,\n",
       "         0.91576093, 1.15784039]]])>"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2*c2\n",
    "b2+c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    " def mask_logits(inputs, mask, mask_value=-1e30):\n",
    "        '''mask矩阵的值为1时就是原来的值，为零时对应padding部分的值'''\n",
    "        mask = tf.cast(mask, tf.double)\n",
    "        #mask=1时就是input,mask=0时，inputs+mask_value是一个非常大的负数此时取softmax值几乎为0\n",
    "        return inputs + mask_value * (1 - mask)\n",
    "q_maxlen=20\n",
    "c_maxlen=40\n",
    "x_cont=tf.constant(np.random.random((2,40,128)))\n",
    "x_ques=tf.constant(np.random.random((2,20,128)))\n",
    "c_mask=tf.constant(np.random.randint(0,2,(2,40)))\n",
    "q_mask=tf.constant(np.random.randint(0,2,(2,20)))\n",
    "W0=tf.constant(np.random.random((128,1)))\n",
    "W1=tf.constant(np.random.random((128,1)))\n",
    "W2=tf.constant(np.random.random((1,1,128)))\n",
    "# bias=tf.constant(1,shape=[1])\n",
    "# get similarity matrix S\n",
    "##K.dot(x_cont, self.W0)维度变化： [batch_size,time_step,dim] *[dim,1] =[batch_size,time_step,1]\n",
    "subres0 = K.tile(K.dot(x_cont, W0), [1, 1, q_maxlen])\n",
    "subres1 = K.tile(K.permute_dimensions(K.dot(x_ques, W1), pattern=(0, 2, 1)), [1, c_maxlen, 1])\n",
    "subres2 = K.batch_dot(x_cont * W2, K.permute_dimensions(x_ques, pattern=(0, 2, 1)))\n",
    "S = subres0 + subres1 + subres2\n",
    "# S += bias\n",
    "q_mask = tf.expand_dims(q_mask, 1)\n",
    "#默认是对最后一维度，即axis=-1\n",
    "S_ = tf.nn.softmax(mask_logits(S, q_mask))\n",
    "c_mask = tf.expand_dims(c_mask, 2)\n",
    "S_T = K.permute_dimensions(tf.nn.softmax(mask_logits(S, c_mask), axis=1), (0, 2, 1))\n",
    "c2q = tf.matmul(S_, x_ques)\n",
    "q2c = tf.matmul(tf.matmul(S_, S_T), x_cont)\n",
    "result = K.concatenate([x_cont, c2q, x_cont * c2q, x_cont * q2c], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S=(2, 40, 20)\n",
      "S_=(2, 40, 20)\n",
      "S_T=(2, 20, 40)\n",
      "q2c =(2, 40, 128)\n",
      "x_cont=(2, 40, 128)\n",
      ",x_ques=(2, 20, 128)\n",
      " c2q=(2, 40, 128)\n",
      ", x_cont * c2q=(2, 40, 128)\n",
      ", x_cont * q2c=(2, 40, 128)\n",
      "\n",
      "result=(2, 40, 512)\n"
     ]
    }
   ],
   "source": [
    "print('S={}'.format(S.shape))\n",
    "print('S_={}'.format(S_.shape))\n",
    "print('S_T={}'.format(S_T.shape))\n",
    "\n",
    "print('q2c ={}'.format(q2c.shape))\n",
    "print('x_cont={}\\n,x_ques={}\\n c2q={}\\n, x_cont * c2q={}\\n, x_cont * q2c={}\\n'.format(x_cont.shape,x_ques.shape, c2q.shape, (x_cont * c2q).shape, (x_cont * q2c).shape))\n",
    "print('result={}'.format(result.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1022 14:27:36.210679 140655136515840 base_layer.py:1814] Layer conv1d_130 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 40, 128])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import  Conv1D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "# from tensorflow.keras.models import VarianceScaling\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "\n",
    "config = {\n",
    "    'word_dim': 300,\n",
    "    'char_dim': 64,\n",
    "    'cont_limit': 40,\n",
    "    'ques_limit': 20,\n",
    "    'char_limit': 16,\n",
    "    'ans_limit': 30,\n",
    "    'char_input_size': 1233,\n",
    "    'filters': 128,\n",
    "    'num_head': 8,\n",
    "    'dropout': 0.5,\n",
    "    'batch_size': 2,\n",
    "    'epoch': 25,\n",
    "    'ema_decay': 0.9999,\n",
    "    'learning_rate': 1e-3,\n",
    "    'path': 'QA001',\n",
    "    'use_cove': True\n",
    "}\n",
    "# parameters\n",
    "word_dim = config['word_dim']\n",
    "char_dim = config['char_dim']\n",
    "cont_limit = config['cont_limit']\n",
    "char_limit = config['char_limit']\n",
    "ans_limit = config['ans_limit']\n",
    "filters = config['filters']\n",
    "num_head = config['num_head']\n",
    "dropout = config['dropout']\n",
    "\n",
    "# from tensorflow.keras.models import *\n",
    "regularizer = l2(3e-7)\n",
    "\n",
    "init=VarianceScaling(scale=1.0, mode='fan_avg', distribution='uniform')\n",
    "init_relu = VarianceScaling(scale=2.0, mode='fan_in', distribution='normal')\n",
    "\n",
    "filters=128\n",
    "#strides=1\n",
    "x = Conv1D(filters=filters, kernel_size=1, strides=1,\n",
    "               kernel_initializer=init,\n",
    "               kernel_regularizer=regularizer,\n",
    "               activation='linear')(result)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.regularizers import *\n",
    "from tensorflow.keras.models import *\n",
    "\n",
    "from layers.context2query_attention import context2query_attention\n",
    "from layers.multihead_attention import Attention as MultiHeadAttention\n",
    "from layers.position_embedding import Position_Embedding as PositionEmbedding\n",
    "from layers.layer_norm import LayerNormalization\n",
    "from layers.layer_dropout import LayerDropout\n",
    "from layers.QAoutputBlock import QAoutputBlock\n",
    "from layers.BatchSlice import BatchSlice\n",
    "from layers.DepthwiseConv1D import DepthwiseConv1D\n",
    "from layers.LabelPadding import LabelPadding\n",
    "from tensorflow.python.keras.initializers import VarianceScaling\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mask_logits(inputs, mask, mask_value=-1e30):\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    return inputs + mask_value * (1 - mask)\n",
    "\n",
    "\n",
    "def highway(highway_layers, x, num_layers=2, dropout=0.0):\n",
    "    # reduce dim\n",
    "    x = highway_layers[0](x)\n",
    "    for i in range(num_layers):\n",
    "        T = highway_layers[i * 2 + 1](x)\n",
    "        H = highway_layers[i * 2 + 2](x)\n",
    "        H = Dropout(dropout)(H)\n",
    "        x = Lambda(lambda v: v[0] * v[1] + v[2] * (1 - v[1]))([H, T, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(conv_layers, x, num_conv=4, dropout=0.0, l=1., L=1.):\n",
    "    for i in range(num_conv):\n",
    "        residual = x\n",
    "        x = LayerNormalization()(x)\n",
    "        if i % 2 == 0:\n",
    "            x = Dropout(dropout)(x)\n",
    "        x = conv_layers[i](x)\n",
    "        x = LayerDropout(dropout * (l / L))([x, residual])\n",
    "    return x\n",
    "\n",
    "\n",
    "def attention_block(attention_layer, x, seq_mask, dropout=0.0, l=1., L=1.):\n",
    "    residual = x\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x1 = attention_layer[0](x)\n",
    "    x2 = attention_layer[1](x)\n",
    "    x = attention_layer[2]([x1, x2, seq_mask])\n",
    "    x = LayerDropout(dropout * (l / L))([x, residual])\n",
    "    return x\n",
    "\n",
    "\n",
    "def feed_forward_block(FeedForward_layers, x, dropout=0.0, l=1., L=1.):\n",
    "    residual = x\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = FeedForward_layers[0](x)\n",
    "    x = FeedForward_layers[1](x)\n",
    "    x = LayerDropout(dropout * (l / L))([x, residual])\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_Encoder_Layer\n",
    "# shared layers\n",
    "Encoder_DepthwiseConv2 = []\n",
    "Encoder_SelfAttention2 = []\n",
    "Encoder_FeedForward2 = []\n",
    "for i in range(7):\n",
    "    DepthwiseConv_share_2_temp = []\n",
    "    for i in range(2):\n",
    "        DepthwiseConv_share_2_temp.append(DepthwiseConv1D(5, filters))\n",
    "\n",
    "    Encoder_DepthwiseConv2.append(DepthwiseConv_share_2_temp)\n",
    "    Encoder_SelfAttention2.append([Conv1D(2 * filters, 1,\n",
    "                                          kernel_initializer=init,\n",
    "                                          kernel_regularizer=regularizer),\n",
    "                                   Conv1D(filters, 1,\n",
    "                                          kernel_initializer=init,\n",
    "                                          kernel_regularizer=regularizer),\n",
    "                                   MultiHeadAttention(filters, num_head, dropout=dropout, bias=False)])\n",
    "    Encoder_FeedForward2.append([Conv1D(filters, 1,\n",
    "                                        kernel_initializer=init,\n",
    "                                        kernel_regularizer=regularizer,\n",
    "                                        activation='relu'),\n",
    "                                 Conv1D(filters, 1,\n",
    "                                        kernel_initializer=init,\n",
    "                                        kernel_regularizer=regularizer,\n",
    "                                        activation='linear')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 40, 128)\n"
     ]
    }
   ],
   "source": [
    "##context_query_attention经过conv1d之后的值就是x\n",
    "outputs = [x]\n",
    "print(np.array(outputs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2, 40, 128)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in range(3):\n",
    "    x = outputs[-1]\n",
    "    for j in range(7):\n",
    "        x = PositionEmbedding()(x)\n",
    "        x = conv_block(Encoder_DepthwiseConv2[j], x, 2, dropout, l=j, L=7)\n",
    "#         print('x_shape conv',x.shape)\n",
    "#         x = attention_block(Encoder_SelfAttention2[j], x, c_mask, dropout, l=j, L=7)\n",
    "#         print('x_shape attention_block ',x.shape)\n",
    "        #前向传播出错了\n",
    "#         \n",
    "#         x = feed_forward_block(Encoder_FeedForward2[j], x, dropout, l=j, L=7)\n",
    "    outputs.append(x)\n",
    "print(np.array(outputs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x_start (2, 40, 256)\n",
      "1 x_start (2, 40, 1)\n",
      "2 x_start (2, 40)\n",
      "3 x_start (2, 40)\n",
      "x_shape (2, 40)\n"
     ]
    }
   ],
   "source": [
    "# Output_Layer\n",
    "x_start = Concatenate()([outputs[1], outputs[2]])\n",
    "print('0 x_start',x_start.shape)\n",
    "\n",
    "x_start = Conv1D(1, 1,\n",
    "                 kernel_initializer=init,\n",
    "                 kernel_regularizer=regularizer,\n",
    "                 activation='linear')(x_start)\n",
    "print('1 x_start',x_start.shape)\n",
    "\n",
    "x_start = Lambda(lambda x: tf.squeeze(x, axis=-1))(x_start)\n",
    "print('2 x_start',x_start.shape)\n",
    "##自己操作加上维度一致不出错\n",
    "# c_mask=tf.squeeze(c_mask, axis=-1)\n",
    "x_start = Lambda(lambda x: mask_logits(x[0], x[1]))([x_start, c_mask])\n",
    "print('3 x_start',x_start.shape)\n",
    "#x_start (2, 40)\n",
    "#做softmax相当于对40个位置进行分类\n",
    "x_start = Lambda(lambda x: K.softmax(x), name='start')(x_start)  # [bs, len]\n",
    "print('x_shape',x_start.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1198562, shape=(2, 40), dtype=float32, numpy=\n",
       "array([[0.0000000e+00, 0.0000000e+00, 3.6136807e-22, 0.0000000e+00,\n",
       "        1.4012985e-45, 0.0000000e+00, 4.9866411e-39, 0.0000000e+00,\n",
       "        3.9796876e-43, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        2.8025969e-45, 2.5563937e-35, 3.2045104e-31, 2.7057841e-33,\n",
       "        0.0000000e+00, 0.0000000e+00, 3.1916667e-37, 0.0000000e+00,\n",
       "        1.1799806e-23, 3.2117676e-16, 7.6809420e-10, 3.5423029e-05,\n",
       "        4.3051383e-01, 5.6943381e-01, 1.6915690e-05, 0.0000000e+00,\n",
       "        1.9391721e-12, 0.0000000e+00, 1.4369720e-09, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 7.1526917e-30, 4.2606157e-32,\n",
       "        1.7473498e-28, 6.3043306e-24, 0.0000000e+00, 1.1524154e-27],\n",
       "       [9.2996703e-04, 4.1794188e-08, 4.4424920e-22, 0.0000000e+00,\n",
       "        1.4012985e-45, 8.8281803e-44, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.0874076e-42, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.5414283e-44, 0.0000000e+00, 7.8925691e-31, 6.1787047e-33,\n",
       "        0.0000000e+00, 1.3040568e-39, 0.0000000e+00, 1.6513027e-30,\n",
       "        7.9475281e-23, 0.0000000e+00, 1.2475999e-09, 0.0000000e+00,\n",
       "        0.0000000e+00, 9.9907011e-01, 0.0000000e+00, 5.5715293e-10,\n",
       "        0.0000000e+00, 0.0000000e+00, 2.2997526e-09, 0.0000000e+00,\n",
       "        4.9989488e-13, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 1.6235516e-22, 3.7990000e-27]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.         0.00092997], shape=(2,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1198771, shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_start[:,0])\n",
    "\n",
    "sum(x_start[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_shape  (2, 40)\n"
     ]
    }
   ],
   "source": [
    "x_end = Concatenate()([outputs[1], outputs[3]])\n",
    "x_end = Conv1D(1, 1,\n",
    "               kernel_initializer=init,\n",
    "               kernel_regularizer=regularizer,\n",
    "               activation='linear')(x_end)\n",
    "x_end = Lambda(lambda x: tf.squeeze(x, axis=-1))(x_end)\n",
    "x_end = Lambda(lambda x: mask_logits(x[0], x[1]))([x_end, c_mask])\n",
    "x_end = Lambda(lambda x: K.softmax(x), name='end')(x_end)  # [bs, len]\n",
    "print('x_shape ',x_end.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25, 25])"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(x_start, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39, 39])"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(x_end, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_start_fin=[[25.]\n",
      " [25.]],\n",
      " x_end_fin=[[39.]\n",
      " [39.]]\n",
      "x_start=(2, 40),x_end=(2, 40)\n"
     ]
    }
   ],
   "source": [
    "##找到最大概率值对应的索引位置\n",
    "x_start_fin, x_end_fin = QAoutputBlock(ans_limit, name='qa_output')([x_start, x_end])\n",
    "print('x_start_fin={},\\n x_end_fin={}'.format(x_start_fin, x_end_fin))\n",
    "# if use model.fit, the output shape must be padded to the max length\n",
    "x_start = LabelPadding(cont_limit, name='start_pos')(x_start)\n",
    "x_end = LabelPadding(cont_limit, name='end_pos')(x_end)\n",
    "print('x_start={},x_end={}'.format(x_start.shape,x_end.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1198331, shape=(2, 1), dtype=float32, numpy=\n",
       "array([[14.],\n",
       "       [14.]], dtype=float32)>"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_start_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "##模拟context中的答案，开始的位置对应1,其它位置全是0\n",
    "start_label = np.random.randint(0, 2, (3, 50))\n",
    "##模拟context中的答案，答案结束的位置对应1,其它位置全是0\n",
    "end_label = np.random.randint(0, 2, (3,50))\n",
    "\n",
    "#找到答案开始位置对应的索引\n",
    "start_label_fin = np.argmax(start_label, axis=-1)\n",
    "#找到答案结束位置对应的索引\n",
    "end_label_fin = np.argmax(end_label, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 1, 0, 0],\n",
       "       [1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "        1, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(start_label.shape)\n",
    "start_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 4, 0])"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(start_label_fin.shape)\n",
    "start_label_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 3]\n",
      " [5 6]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "import numpy as np  \n",
    "'''\n",
    "作用：从列表、数组、张量等对象中抽取一部分数据\n",
    "\n",
    "begin和size是两个多维列表，他们共同决定了要抽取的数据的开始和结束位置\n",
    "\n",
    "begin表示从inputs的哪几个维度上的哪个元素开始抽取 \n",
    "size表示在inputs的各个维度上抽取的元素个数\n",
    "\n",
    "若begin[]或size[]中出现-1,表示抽取对应维度上的所有元素\n",
    "\n",
    "'''\n",
    "x=[\n",
    "    [1,2,3],\n",
    "    [4,5,6]\n",
    "]  \n",
    "\n",
    "begin = [0,1]  # 从x[0,1],即元素2开始抽取\n",
    "size = [2,2]   # 从x[0,1]开始，对x的第一个维度（行）抽取2个元素，在对x的第二个维度（列）抽取2个元素\n",
    "print(tf.slice(x,begin,size) ) # 输出[[2 5]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
